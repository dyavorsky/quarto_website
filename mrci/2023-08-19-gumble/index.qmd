---
title: "Type 1 Extreme Value Distribution"
description: "description"
author:
  - name: Dan Yavorsky
    url: danyavorsky.com
    orcid: 0000-0003-4095-6405
number-sections: true
date: 2023-08-19
draft: true
---

Get the book from Routledge ([here](https://www.routledge.com/Statistical-Theory-A-Concise-Introduction/Abramovich-Ritov/p/book/9781032007458)) or Amazon ([here](https://www.amazon.com/Statistical-Theory-Concise-Introduction-Chapman/dp/1032007451)) 

This is a truly excellent book explaining the underlying ideas, mathematics, and principles of major statistical concepts. Its organization is suburb, and the authors' commentary on _why_ a theorem is so useful and _how_ the presented ideas fit together and/or contrast is invaluable (and, quite frankly, better than I have seen anywhere else).

# Introduction

Suppose the observed data is the sample $\mathbf{y} = \{y_1, \ldots, y_n\}$ of size $n$. We will model $y$ as a realization of an $n$-dimensional random vector $\mathbf{Y} = \{Y_1, \ldots, Y_n\}$ with a joint distribution $f_\mathbf{Y}(\mathbf{y})$. The true distribution of the data is rarely completely known; nevertheless, it can often be reasonable to assume that it belongs to some family of distributions $\mathcal{F}$. We will assume that $\mathcal{F}$ is a _parametric_ family, that is, that we know the type of distribution $f_\mathbf{Y}(\mathbf{y})$ up to some unknown parameter(s) $\theta \in \Theta$, where $\Theta$ is a parameter space. Typically, we will consider the case where $y_1, \ldots, y_n$ are the results of _independent_ identical experiments. In this case, $Y_1, \ldots, Y_n$ can be treated as independent, identically distributed random variables with the common distribution $f_\theta(y)$ from a parametric family of distributions $\mathcal{F}_\theta$, $\theta \in \Theta$. 

Define the _likelihood function_ $L(\theta; \mathbf{y}) = P_\theta(\mathbf{y})$ --- the probability to observe the given data $\mathbf{y}$ for any possible value of $\theta \in \Theta$. First assume that $\mathbf{Y}$ is discrete. The value $L(\theta; \mathbf{y})$ can be viewed as a measure of likeliness of $\theta$ to the observed data $\mathbf{y}$. If $L(\theta_1; \mathbf{y}) > L(\theta_2; \mathbf{y})$ for a given $\mathbf{y}$, we can say that the value $\theta_1$ for $\theta$ is more suited to the data than $\theta_2$. For a continuous random variable $\mathbf{y}$, the likelihood ratio $L(\theta_1; \mathbf{y})/L(\theta_2; \mathbf{y})$ shows the strength of the evidence in favor of $\theta = \theta_1$ vs $\theta = \theta_2$.

A _statistic_ $T(\mathbf{Y})$ is any real or vector-valued function that can be computed using the data alone. A statistic $T(\mathbf{Y})$ is _sufficient_ for an unknown parameter $\theta$ if the conditional distribution of all the data $\mathbf{Y}$ given $T(\mathbf{Y})$ does not depend on the $\theta$. In other words, given $T(\mathbf{Y})$ no other information on $\theta$ can be extracted from $\mathbf{y}$. This definition allows one to check whether a given statistic $T(\mathbf{Y})$ is sufficient for $\theta$, but it does not provide one with a constructive way to find it. 

However, the _Fisher-Neyman Factorization Theorem_ says that a statistic $T(\mathbf{Y})$ is sufficient for $\theta$ iff for all $\theta \in \Theta$, $L(\theta, \mathbf{y}) = g(T(\mathbf{y}), \theta) \cdot h(\mathbf{y})$, where the function $g(\cdot)$ depends on $\theta$ and the statistic $T(\mathbf{Y})$, while $h(\mathbf{y})$ does not depend on $\theta$. In particular, if the likelihood $L(\theta; \mathbf{y})$ depends on data only through $T(\mathbf{Y})$, then $T(\mathbf{Y})$ is a sufficient statistic for $\theta$ and $h(\mathbf{y}) = 1$.

A sufficient statistic is not unique. For example, the entire sample $\mathbf{Y} = \{Y_1, \ldots, Y_n\}$ is always a (trivial) sufficient statistic. We may seek a minimal sufficient statistic implying the maximal reduction of the data.  A statistic $T(\mathbf{Y})$ is called a minimal sufficient statistic if it is a function of any other sufficient statistic.

Another important property of a statistic is _completeness_. Let $Y_1, \ldots, Y_n \sim f_\theta(y)$, where $\theta \in \Theta$. A statistic $T(\mathbf{Y})$ is complete if no statistic $g(\mathbf{T})$ exists (except $g(\mathbf{T})=0$) such that $E_\theta g(\mathbf{T}) = 0$ for all $\theta \in \Theta$. In other words, if $E_\theta g(\mathbf{T}) = 0$ for all $\theta \in \Theta$, then necessarily $g(\mathbf{T})=0$. To verify completeness for a general distribution can be a nontrivial mathematical problem, but thankfully it is much simpler for the exponential family of distributions that includes many of the "common" distributions. Completeness is a useful in determining minimal sufficiency because if a sufficient statistic $T(\mathbf{Y})$ is complete, then it is also minimal sufficient. (Note, however, that a minimal sufficient statistic may not necessarily be complete.) 

A (generally multivariate) family of distributions ${f_\theta(\mathbf{y}): \theta \in \Theta}$ is said to be an (one parameter) _exponential_ family if: (1) $\Theta$ is an open interval, (2) the support of the distribution $f_\theta$ does not depend on $\theta$, and (3) $f_\theta(\mathbf{y}) = exp\{c(\theta)T(\mathbf{y}) + d(\theta) + S(\mathbf{y})\}$ where $c(\cdot)$, $T(\cdot)$, $d(\cdot)$, and $S(\cdot)$ are known functions; $c(\theta)$ is usually called the _natural parameter_ of the distribution. We say that $f_\theta$ where $\theta = (\theta_1, \ldots \theta_p)$ belongs to a $k$-parameter exponential family by changing (3) such that $f_\theta(\mathbf{y}) = exp\{ \sum_{j=1}^k c_j(\theta)T_j(\mathbf{y}) + d(\theta) + S(\mathbf{y})\}$. The function $c(\theta) = \{c_1(\theta), \ldots, c_k(\theta)\}$ are the natural parameters of the distribution. (Note that the dimensionality $p$ of the original parameter $\theta$ is not necessarily the same as the dimensionality $k$ of the natural parameter $c(\theta)$.)

Consider a random sample  $Y_1, \ldots, Y_n$, where $Y_i \sim f_\theta(y)$ and $f_\theta$ belongs to a $k$-parameter exponential family of distributions, then (1) the joint distribution of $\mathbf{Y} = (Y_1, \ldots, Y_n)$ also belongs to the $k$-parameter exponential family, (2) $T_\mathbf{Y} = (\sum_{i=1}^n T_1(Y_i), \ldots, \sum_{i=1}^n T_k(Y_i))$ is the sufficient statistic for $c(\theta) = (c_1(\theta), \ldots, c_k(\theta))$ (and, therefore, for $\theta$), and (3) if some regularity conditions hold, then $T_\mathbf{Y}$ is complete and therefore minimal sufficient (if the latter exists).

# Point Estimation

Estimation of the unknown parameters of distributions from the data is one of the key issues in statistics. A (point) _estimator_ $\hat{\theta} = \hat{\theta}(\mathbf{Y})$ of an unknown parameter $\theta$ is any statistic used for estimating $\theta$. The value of $\hat{\theta}(\mathbf{y})$ evaluated for a given sample is called an _estimate_. This is a general, somewhat trivial definition that does not say anything about the goodness of estimation; one would evidently be interested in "good" estimators.

_Maximum likelihood estimation_ is the most used method of estimation of parameters in parametric models. As we've discussed, $L(\theta; \mathbf{y})$ is the measure of likeliness of a parameter's values $\theta$ for the observed data $\mathbf{y}$. It is only natural then to seek the "most likely" value of $\theta$. The MLE $\hat{\theta}$ of $\theta$ is $\hat{\theta} = \arg\max_{\theta\in\Theta} L(\theta; \mathbf{y})$ --- the value of $\theta$ that maximizes the likelihood. Often we are interested in a function of a parameter $\xi = g(\theta)$. When $g(\cdot)$ is 1:1, the MLE of $xi$ is the function $g(\cdot)$ applied to the MLE of $\theta$: $\hat{\xi} = g(\hat{\theta})$, the proof of which is just a reparameterization of the likelihood in terms of $\xi$ instead of $\theta$. Although the conception of the MLE was motivated by an intuitively clear underlying idea, the justification for its use is much deeper. It is a really "good" method of estimation (to be discussed later on the topic of asymptotics).

Another popular method of estimation is the _Method of Moments_. Its main idea is based on expressing the population moments of the distribution of data in terms of its unknown parameter(s) and equating them to their corresponding sample moments. MMEs have some known problems: consider a sample of size 4 from a uniform distribution $U(0, \theta)$ with the observed sample 0.2, 0.6, 2, and 0.4. The MME is 1.6, which does not make much sense given the observed value of 2 in the sample. For these and related reasons, the MMEs are less used than the MLE counterparts. However, MMEs are usually simpler to compute and can be used, for example, as reasonable initial values in numerical iterative procedures for MLEs. On the other hand, the Method of Moments does not require knowledge of the entire distribution of the data (up to the unknown parameters) but only its moments and thus may be less sensitive to possible misspecification of a model.

The _method of least squares_ play a key role in regression and analysis of variance. In a typical regression setup, we are given $n$ observations $(\mathbf{x}_i, y_i)$, $i=1, \ldots, n$ over $m$ explanatory variables $\mathbf{x} = (x_1, \ldots, x_m)$ and the response variable $\mathbf{Y}$. We assume that $y_i = g_\theta(\mathbf{x}_i) + \varepsilon_i$, $i = 1, \ldots, n$ where the response function $g_\theta(\cdot): \mathbb{R}^m \rightarrow \mathbb{R}$ has a known parametric form and depends on $p \le n$ unknown parameters $\theta = (\theta_1, \ldots, \theta_p)$. The LSE looks for a $\hat{\theta}$ that yields the best fit $g_\hat{\theta}(\mathbf{x})$ to the observed $\mathbf{y}$ w.r.t.\ the Euclidean distance: $\hat{\theta} = \arg\min_\theta \sum_{i=1}^n (y_i - g_\theta(\mathbf{x}_i))^2$. For linear regression, the solution is available in closed form. For non-linear regression, however, it can generally be only found numerically.

More generally than the three above procedures, one can consider any function $\rho(\theta, y)$ as a measure of the goodness-of-fit and look for an estimator that maximizes or minimizes $\sum_{i=1}^n \rho(\theta, y_i)$ w.r.t.\ $\theta$. Such estimators are called _M-estimators_. It runs out that various well-known estimators can be viewed as M-estimators for a particular $\rho(\theta, y)$ including $\bar{Y}$ for the sample mean as well as MLE, LSE, and a generalized version of MME not yet discussed. As we'll see when discussing asymptotics, M-estimators share many important asymptotic properties.

A natural question is how to compare between various estimators. First, we should define a measure of goodness-of-estimation. Recall that any estimator $\hat{\theta} = \hat{\theta}(Y_1, \ldots, Y_n)$ is a function of a random sample and therefore is a random variable itself with a certain distribution, expectation, variance, etc. A somewhat naive attempt to measure the goodness-of-estimation of $\hat{\theta}$ would be to consider the error $|\hat{\theta}-\theta|$. However, $\theta$ is unknown and, as we have mentioned, an estimator $\hat{\theta}$ is a random variable and hence the value $|\hat{\theta}-\theta|$ will vary from sample to sample. It may be "small" for some of the samples, while "large" for others and therefore cannot be used as a proper criterion for goodness-of-estimation of an estimator $\hat{\theta}$. A more reasonable measure would then be an average distance over all possible samples, that is, the mean absolute error $E|\hat{\theta}-\theta|$, where the expectation is taken w.r.t.\ the joint distribution of $\mathbf{Y} = (Y_1, \ldots, Y_n)$. It indeed can be used as a measure of goodness-of-estimation but usually, mostly due to convenience of differentiation, the conventional measure is the _mean squared error_ (MSE) given by $MSE(\hat{\theta}, \theta) = E(\hat{\theta}-\theta)^2$. 

The MSE can be decomposed into two components: $MSE(\hat{\theta}, \theta) = Var(\hat{\theta}) + b^2(\hat{\theta},\theta)$. The first is the stochastic error (variance) and the second is a systematic or deterministic error (bias). Having defined the goodness-of-estimation measure by MSE, one can compare different estimators and choose the one with the smallest MSE. However, since the $MSE(\hat{\theta}, \theta)$ typically depends on the unknown $\theta$, it is a common situation where no estimator is uniformly superior for all $\theta \in \Theta$.

Ideally, a good estimator with a small MSE should have both low variance and low bias. However, it might be hard to have both. One of the common approaches is to first control the bias component of the overall MSE and to consider unbiased estimators. There is no general rule or algorithm for deriving an unbiased estimator. In fact, unbiasedness is a property of an estimator rather than a method of estimation. One usually checks an MLE or any other estimator for bias. Sometimes one can then modify the original estimator to "correct" its bias. Note that unlike ML estimation, unbiasedness is not invariant under nonlinear transformation of the original parameter: if $\hat{\theta}$ is an unbiased estimator of $\theta$, $g(\hat{\theta})$ is generally a biased estimator for $g(\theta)$.

What does unbiasedness of an estimator $\hat{\theta}$ actually mean? Suppose we were observing not a single sample but all possible samples of size $n$ from a sample space and were calculating the estimates $\hat{\theta}_j$ for each one of them. The unbiasedness means that the average value of $\hat{\theta}$ over the entire sample space is $\theta$, but it does not guarantee yet that $\hat{\theta}_j \approx \theta$ for each particular sample. The dispersion of $\hat{\theta}_j$'s around their average value $\theta$ might be large and, since in reality we have only a single sample, its particular value of $\hat{\theta}$ might be quite away from $\theta$. To ensure with high confidence that $\hat{\theta}_j \approx \theta$ for any sample we need in addition for the variance $Var(\hat{\theta})$ to be small. 

An estimator $\hat{\theta}$ is called a _uniformly minimum variance unbiased estimator_ (UMVUE) of $\theta$ if $\hat{\theta}$ is unbiased and for any other unbiased estimator $\tilde{\theta}$ of $\theta$, $Var(\hat{\theta}) \le Var(\tilde{\theta})$. If the UMVUE exists, it is necessarily unique. Recall that there is no general algorithm to obtain unbiased estimators in general and a UMVUE in particular. However, there exists a lower bound for a variance of an unbiased estimator, which can be used as a benchmark for evaluating its goodness. 

Define the _Fisher Information Number_ $I(\theta) = E((\ln f_\theta(\mathbf{y}))'_\theta)^2$. The derivative of the log density is sometimes called the _Score Function_. Thus, the Fisher Information Number is the expected square of the Score. The _Cramer-Rao Lower Bound Theorem_ states that if $T$ is an unbiased estimator for $g(\theta)$, where $g(\cdot)$ is differentiable, then $Var(T) \ge (g'(\theta))^2 / I(\theta)$ or more simply, when $T$ is an unbiased estimator for $\theta$, $Var(T) \ge 1 / I(\theta)$. We are especially interested in the case where $Y_1, \ldots, Y_n$ is a random sample from a distribution $f_\theta(y)$. In that case, $I(\theta) = nI^*(\theta)$ where $I*(\theta) = E((\ln f_\theta(y))'_\theta)^2$ is the Fisher Information Number of $f_\theta(y)$, and, therefore, for any unbiased estimator $T$ of $g(\theta)$, we have that $Var(T) \ge (g'_\theta(\theta))^2 / nI^*(\theta)$. There is another, usually more convenient formula for calculating the Fisher Information Number $I(\theta)$ other than its direct definition: $I(\theta) = -E(\ln f_\theta(\mathbf{Y}))''_\theta$.

It is important to emphasize that the CRLB theorem is one direction only: if the variance of an unbiased estimator does not achieve the Cramer-Rao lower bound, one still cannot claim that it is not an UMVUE. Nevertheless, it can be used as a benchmark for measuring the goodness of an unbiased estimator. One special result related to the exponential family distribution: the Cramer-Rao lower bound is achieved only for distributions from the exponential family.

The Cramer-Rao lower bound allows one only to evaluate the goodness of a proposed unbiased estimator but does not provide any constructive way to derive it. In fact, as we have argued, there is no such general rule at all. However, if one manages to obtain any initial (even crude) unbiased estimator, it may be possible to improve it. The _Rao-Blackwell Theorem_ shows that if there is an unbiased estimator that is not a function of a sufficient statistic $W$, one can construct another unbiased estimator based on $W$ with an MSE not larger than the original one: let $T$ be an unbiased estimator of $\theta$ and $W$ be a sufficient statistic for $\theta$, and define $T_1 = E(T|W)$, then $T_1$ is an unbiased estimator of $\theta$ and $Var(T_1) \le Var(T)$.  Thus, in terms of MSE, only unbiased estimators based on a sufficient statistic are of interest. This demonstrates again a strong sense of the notion of sufficiency.

Does Rao-Blackwellization necessarily yield an UMVUE? Generally not. To guarantee UMVUE an additional requirement of completeness on a sufficient statistic $W$ is needed. The _Lehmann-Scheffe Theorem_ formalizes this: if $T$ is an unbiased estimator of $\theta$ and $W$ is a complete sufficient statistic for $\theta$, then $T_1 = E(T|W)$ is the unique UMVUE of $\theta$. Even without the Lehmann-Scheffe theorem, it can be shown under mild conditions that if the distribution of the data belongs to the exponential family and an unbiased estimator is a function of the corresponding sufficient statistic, it is an UMVUE. Note that despite its elegance, the application of the Rao-Blackwell Theorem in more complicated cases is quite limited. The two main obstacles are in finding an initial unbiased estimator $T$ and calculating the conditional expectation $E(T|W)$

# Confidence Intervals, Bounds, and Regions



# Hypothesis Testing

# Asymptotic Analysis

# Bayesian Inference

# Elements of Statistical Decision Theory

# Linear Models

# Nonparametric Estimation


