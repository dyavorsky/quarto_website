{
  "hash": "51a362508b3529a1ee0723626e11d295",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Gumble Distribution in Discrete Choice Models\"\ndescription: \"Derivation of Choice Probabilities\"\nnumber-sections: true\ndraft: false\n\n---\n\n\n\nThis short article seeks to describe how the Gumbel (or Type-1 Generalized Extreme Value) distribution's use in probabilistic discrete choice models leads to a closed-form expression for choice probabilities.\n\nA standard framework for a probabilistic discrete choice model (DCM) reflecting a random utility theory (RUT) in a cross-sectional setting is as follows: \n\n - The utility that a decision maker obtains from alternative $j$ is $U_j$, for $j = 1, \\ldots, J$. \n - The decision maker chooses the alternative that provides the greatest utility: choose alternative $k$ if and only if $U_k > U_j \\hspace{1ex} \\forall j \\ne k$. \n - Utility is decomposed to reflect the researcher's inability to understand or observe all factors that impact choices: $U_j = V_j + \\varepsilon_j$ where the joint density of the random vector $\\mathbf{\\varepsilon} = (\\varepsilon_1, \\ldots, \\varepsilon_J)$ for the decision maker is $f_\\mathbf{\\varepsilon}(\\mathbf{\\varepsilon})$. \n\nThe probability that the decision maker chooses alternative $k$ is:\n\n\\begin{align*}\n    P_{k} \n    &= \\text{Prob}\\left( U_k > U_j \\; \\forall j \\ne k \\right)  &\\\\\n    &= \\text{Prob}\\left( V_k + \\varepsilon_k > V_j + \\varepsilon_j \\; \\forall j \\ne k \\right)  &\\\\\n    &= \\text{Prob}\\left( \\varepsilon_j < \\varepsilon_k + V_k - V_j \\; \\forall j \\ne k \\right)  &\\\\\n\\end{align*}\n\nA common simplifying assumption about the joint distribution of vector $\\mathbf{\\varepsilon}$ is that its components are independent and identically distributed as Gumbel random variables. Specifically, the cumulative distribution function for each $\\varepsilon_{j}$ is:\n$$ \n\\text{Prob}(\\varepsilon_j \\leq x) = \\exp(-\\exp(-x)) = e^{-e^{-x}} \n$$\nThe Gumbel density and distribution are plotted below (in black) alongside the Normal (in dashed red) for comparison.\n\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n\n\nBecause each $\\varepsilon_j$ is assumed to be independently distributed, the probability that the decision maker chooses alternative $k$ for a given value of $\\varepsilon_k$ can be written as the product of the $J-1$ conditional distributions:\n\n\\begin{align*}\n    P_k | \\varepsilon_k\n    &= \\prod_{j \\ne k} \\exp(-\\exp(-\\varepsilon_k)) &\\\\\n    &= \\exp \\left( -\\sum_{j \\ne k} \\exp \\left( - ( \\varepsilon_k + V_k - V_j) \\right) \\right) &\\\\\n    &= \\exp \\left( - \\exp(\\varepsilon_k) \\times \\sum_{j \\ne k} \\exp \\left( V_j - V_k) \\right) \\right) \n\\end{align*}\n\n\nBut of course $\\varepsilon_k$ is not given, so to obtain the desired probability $P_k$, we must integrate over all possible values of $\\varepsilon_k$, weighted by its marginal density $f_{\\varepsilon_k}(\\varepsilon_k) = \\exp(-\\varepsilon_k) \\times \\exp \\left( -\\exp(\\varepsilon_k) \\right)$:\n$$ \nP_k = \\int_{-\\infty}^{\\infty} \\exp(-\\varepsilon_k) \\times \\exp \\left( - \\exp(\\varepsilon_k) \\times \\sum_{j=1}^J \\exp \\left( V_j - V_k) \\right) \\right) \\, d \\varepsilon_k \n$$\n\nFor notational simplicity, denote $a = \\sum_{j=1}^J \\exp ( V_j - V_k)$ and define the transformation of variables $z = \\exp(-\\varepsilon_k)$. Then $dz = -\\exp(-\\varepsilon_k) \\, d \\varepsilon_k = -z \\, d \\varepsilon_k$ and the integral becomes:\n$$\nP_k = \\int_\\infty^0 z \\times \\exp ( - z \\times a ) / (-z) \\, dz = \\int_0^\\infty \\exp (-z \\times a ) \\, dz\n$$\n\nEvaluating the integral yields:\n$$\nP_k = - \\left[ \\frac{1}{a} \\times \\left( 0-1 \\right) \\right] = \\frac{1}{a} = \\frac{1}{\\sum_{j=1}^J \\exp(V_j-V_k)} = \\frac{\\exp(V_k)}{\\sum_{j=1}^J \\exp(V_j)}\n$$\n\nThis is the familiar multinomial logit (MNL) choice probability.\n\nSection 3.10 on page 74 of Kenneth Train's book _Discrete Choice Methods with Simulation_ provides a related derivation.  If you are reading this post but have not yet read Train's [book](https://eml.berkeley.edu/books/train1201.pdf), you should go read it now.\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}