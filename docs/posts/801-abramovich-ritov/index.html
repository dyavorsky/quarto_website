<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="by Felix Abramovich and Ya’acov Ritov">

<title>Dan Yavorsky - Quotes and Notes from Statistical Theory: A Concise Introduction (2ed)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../..//images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../custom.scss">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Dan Yavorsky</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cv/index.html"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching/index.html"> 
<span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../pubs/index.html"> 
<span class="menu-text">Pubs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts/index.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="http://dcms-r.danyavorsky.com" target="_blank"> 
<span class="menu-text">DCMS-R</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../links/index.html"> 
<span class="menu-text">Links</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Quotes and Notes from <em>Statistical Theory: A Concise Introduction</em> (2ed)</h1>
                  <div>
        <div class="description">
          by Felix Abramovich and Ya’acov Ritov
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#point-estimation" id="toc-point-estimation" class="nav-link" data-scroll-target="#point-estimation"><span class="header-section-number">2</span> Point Estimation</a></li>
  <li><a href="#confidence-intervals-bounds-and-regions" id="toc-confidence-intervals-bounds-and-regions" class="nav-link" data-scroll-target="#confidence-intervals-bounds-and-regions"><span class="header-section-number">3</span> Confidence Intervals, Bounds, and Regions</a></li>
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing"><span class="header-section-number">4</span> Hypothesis Testing</a></li>
  <li><a href="#asymptotic-analysis" id="toc-asymptotic-analysis" class="nav-link" data-scroll-target="#asymptotic-analysis"><span class="header-section-number">5</span> Asymptotic Analysis</a></li>
  <li><a href="#bayesian-inference" id="toc-bayesian-inference" class="nav-link" data-scroll-target="#bayesian-inference"><span class="header-section-number">6</span> Bayesian Inference</a></li>
  <li><a href="#elements-of-statistical-decision-theory" id="toc-elements-of-statistical-decision-theory" class="nav-link" data-scroll-target="#elements-of-statistical-decision-theory"><span class="header-section-number">7</span> Elements of Statistical Decision Theory</a></li>
  <li><a href="#linear-models" id="toc-linear-models" class="nav-link" data-scroll-target="#linear-models"><span class="header-section-number">8</span> Linear Models</a></li>
  <li><a href="#nonparametric-estimation" id="toc-nonparametric-estimation" class="nav-link" data-scroll-target="#nonparametric-estimation"><span class="header-section-number">9</span> Nonparametric Estimation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Get the book from Routledge (<a href="https://www.routledge.com/Statistical-Theory-A-Concise-Introduction/Abramovich-Ritov/p/book/9781032007458">here</a>) or Amazon (<a href="https://www.amazon.com/Statistical-Theory-Concise-Introduction-Chapman/dp/1032007451">here</a>)</p>
<p>This is a truly excellent book explaining the underlying ideas, mathematics, and principles of major statistical concepts. Its organization is suburb, and the authors’ commentary on <em>why</em> a theorem is so useful and <em>how</em> the presented ideas fit together and/or contrast is invaluable (and, quite frankly, better than I have seen anywhere else).</p>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Suppose the observed data is the <strong>sample</strong> <span class="math inline">\(\mathbf{y} = \{y_1, \ldots, y_n\}\)</span> of size <span class="math inline">\(n\)</span>. We will model <span class="math inline">\(\mathbf{y}\)</span> as a realization of an <span class="math inline">\(n\)</span>-dimensional <strong>random vector</strong> <span class="math inline">\(\mathbf{Y} = \{Y_1, \ldots, Y_n\}\)</span> with a <strong>joint distribution</strong> <span class="math inline">\(f_\mathbf{Y}(\mathbf{y})\)</span>. The true distribution of the data is rarely completely known; nevertheless, it can often be reasonable to assume that it belongs to some family of distributions <span class="math inline">\(\mathcal{F}\)</span>. We will assume that <span class="math inline">\(\mathcal{F}\)</span> is a <strong>parametric</strong> family, that is, that we know the type of distribution <span class="math inline">\(f_\mathbf{Y}(\mathbf{y})\)</span> up to some unknown <strong>parameter(s)</strong> <span class="math inline">\(\theta \in \Theta\)</span>, where <span class="math inline">\(\Theta\)</span> is a parameter space. Typically, we will consider the case where <span class="math inline">\(y_1, \ldots, y_n\)</span> are the results of <strong>independent</strong> identical experiments. In this case, <span class="math inline">\(Y_1, \ldots, Y_n\)</span> can be treated as independent, identically distributed random variables with the common distribution <span class="math inline">\(f_\theta(y)\)</span> from a parametric family of distributions <span class="math inline">\(\mathcal{F}_\theta\)</span>, <span class="math inline">\(\theta \in \Theta\)</span>.</p>
<p>Define the <strong>likelihood function</strong> <span class="math inline">\(L(\theta; \mathbf{y}) = P_\theta(\mathbf{y})\)</span> — the probability to observe the given data <span class="math inline">\(\mathbf{y}\)</span> for any possible value of <span class="math inline">\(\theta \in \Theta\)</span>. First assume that <span class="math inline">\(\mathbf{Y}\)</span> is discrete. The value <span class="math inline">\(L(\theta; \mathbf{y})\)</span> can be viewed as a measure of likeliness of <span class="math inline">\(\theta\)</span> to the observed data <span class="math inline">\(\mathbf{y}\)</span>. If <span class="math inline">\(L(\theta_1; \mathbf{y}) &gt; L(\theta_2; \mathbf{y})\)</span> for a given <span class="math inline">\(\mathbf{y}\)</span>, we can say that the value <span class="math inline">\(\theta_1\)</span> for <span class="math inline">\(\theta\)</span> is more suited to the data than <span class="math inline">\(\theta_2\)</span>. For a continuous random variable <span class="math inline">\(\mathbf{y}\)</span>, the likelihood ratio <span class="math inline">\(L(\theta_1; \mathbf{y})/L(\theta_2; \mathbf{y})\)</span> shows the strength of the evidence in favor of <span class="math inline">\(\theta = \theta_1\)</span> vs <span class="math inline">\(\theta = \theta_2\)</span>.</p>
<p>A <strong>statistic</strong> <span class="math inline">\(T(\mathbf{Y})\)</span> is any real or vector-valued function that can be computed using the data alone. A statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is <strong>sufficient</strong> for an unknown parameter <span class="math inline">\(\theta\)</span> if the conditional distribution of all the data <span class="math inline">\(\mathbf{Y}\)</span> given <span class="math inline">\(T(\mathbf{Y})\)</span> does not depend on the <span class="math inline">\(\theta\)</span>. In other words, given <span class="math inline">\(T(\mathbf{Y})\)</span> no other information on <span class="math inline">\(\theta\)</span> can be extracted from <span class="math inline">\(\mathbf{y}\)</span>. This definition allows one to check whether a given statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is sufficient for <span class="math inline">\(\theta\)</span>, but it does not provide one with a constructive way to find it.</p>
<p>However, the <strong>Fisher-Neyman Factorization Theorem</strong> says that a statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is sufficient for <span class="math inline">\(\theta\)</span> iff for all <span class="math inline">\(\theta \in \Theta\)</span> that <span class="math inline">\(L(\theta, \mathbf{y}) = g(T(\mathbf{y}), \theta) \cdot h(\mathbf{y})\)</span>, where the function <span class="math inline">\(g(\cdot)\)</span> depends on <span class="math inline">\(\theta\)</span> and the statistic <span class="math inline">\(T(\mathbf{Y})\)</span>, while <span class="math inline">\(h(\mathbf{y})\)</span> does not depend on <span class="math inline">\(\theta\)</span>. In particular, if the likelihood <span class="math inline">\(L(\theta; \mathbf{y})\)</span> depends on data only through <span class="math inline">\(T(\mathbf{Y})\)</span>, then <span class="math inline">\(T(\mathbf{Y})\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(h(\mathbf{y}) = 1\)</span>.</p>
<p>A sufficient statistic is not unique. For example, the entire sample <span class="math inline">\(\mathbf{Y} = \{Y_1, \ldots, Y_n\}\)</span> is always a (trivial) sufficient statistic. We may seek a <strong>minimal sufficient statistic</strong> implying the maximal reduction of the data. A statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is called a minimal sufficient statistic if it is a function of any other sufficient statistic.</p>
<p>Another important property of a statistic is <strong>completeness</strong>. Let <span class="math inline">\(Y_1, \ldots, Y_n \sim f_\theta(y)\)</span>, where <span class="math inline">\(\theta \in \Theta\)</span>. A statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is complete if no statistic <span class="math inline">\(g(\mathbf{T})\)</span> exists (except <span class="math inline">\(g(\mathbf{T})=0\)</span>) such that <span class="math inline">\(E_\theta g(\mathbf{T}) = 0\)</span> for all <span class="math inline">\(\theta \in \Theta\)</span>. In other words, if <span class="math inline">\(E_\theta g(\mathbf{T}) = 0\)</span> for all <span class="math inline">\(\theta \in \Theta\)</span>, then necessarily <span class="math inline">\(g(\mathbf{T})=0\)</span>. To verify completeness for a general distribution can be a nontrivial mathematical problem, but thankfully it is much simpler for the exponential family of distributions that includes many of the “common” distributions. Completeness is a useful in determining minimal sufficiency because if a sufficient statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is complete, then it is also minimal sufficient. (Note, however, that a minimal sufficient statistic may not necessarily be complete.)</p>
<p>A (generally multivariate) family of distributions <span class="math inline">\({f_\theta(\mathbf{y}): \theta \in \Theta}\)</span> is said to be an (one parameter) <strong>exponential family</strong> if: (1) <span class="math inline">\(\Theta\)</span> is an open interval, (2) the support of the distribution <span class="math inline">\(f_\theta\)</span> does not depend on <span class="math inline">\(\theta\)</span>, and (3) <span class="math inline">\(f_\theta(\mathbf{y}) = exp\{c(\theta)T(\mathbf{y}) + d(\theta) + S(\mathbf{y})\}\)</span> where <span class="math inline">\(c(\cdot)\)</span>, <span class="math inline">\(T(\cdot)\)</span>, <span class="math inline">\(d(\cdot)\)</span>, and <span class="math inline">\(S(\cdot)\)</span> are known functions; <span class="math inline">\(c(\theta)\)</span> is usually called the <strong>natural parameter</strong> of the distribution. We say that <span class="math inline">\(f_\theta\)</span> where <span class="math inline">\(\theta = (\theta_1, \ldots \theta_p)\)</span> belongs to a <span class="math inline">\(k\)</span>-parameter exponential family by changing (3) such that <span class="math inline">\(f_\theta(\mathbf{y}) = exp\{ \sum_{j=1}^k c_j(\theta)T_j(\mathbf{y}) + d(\theta) + S(\mathbf{y})\}\)</span>. The function <span class="math inline">\(c(\theta) = \{c_1(\theta), \ldots, c_k(\theta)\}\)</span> are the natural parameters of the distribution. (Note that the dimensionality <span class="math inline">\(p\)</span> of the original parameter <span class="math inline">\(\theta\)</span> is not necessarily the same as the dimensionality <span class="math inline">\(k\)</span> of the natural parameter <span class="math inline">\(c(\theta)\)</span>.)</p>
<p>Consider a random sample <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, where <span class="math inline">\(Y_i \sim f_\theta(y)\)</span> and <span class="math inline">\(f_\theta\)</span> belongs to a <span class="math inline">\(k\)</span>-parameter exponential family of distributions, then (1) the joint distribution of <span class="math inline">\(\mathbf{Y} = (Y_1, \ldots, Y_n)\)</span> also belongs to the <span class="math inline">\(k\)</span>-parameter exponential family, (2) <span class="math inline">\(T_\mathbf{Y} = (\sum_{i=1}^n T_1(Y_i), \ldots, \sum_{i=1}^n T_k(Y_i))\)</span> is the sufficient statistic for <span class="math inline">\(c(\theta) = (c_1(\theta), \ldots, c_k(\theta))\)</span> (and, therefore, for <span class="math inline">\(\theta\)</span>), and (3) if some regularity conditions hold, then <span class="math inline">\(T_\mathbf{Y}\)</span> is complete and therefore minimal sufficient (if the latter exists).</p>
</section>
<section id="point-estimation" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Point Estimation</h1>
<p>Estimation of the unknown parameters of distributions from the data is one of the key issues in statistics. A (point) <strong>estimator</strong> <span class="math inline">\(\hat{\theta} = \hat{\theta}(\mathbf{Y})\)</span> of an unknown parameter <span class="math inline">\(\theta\)</span> is any statistic used for estimating <span class="math inline">\(\theta\)</span>. The value of <span class="math inline">\(\hat{\theta}(\mathbf{y})\)</span> evaluated for a given sample is called an <strong>estimate</strong>. This is a general, somewhat trivial definition that does not say anything about the goodness of estimation; one would evidently be interested in “good” estimators.</p>
<p><strong>Maximum Likelihood Estimation</strong> is the most used method of estimation of parameters in parametric models. As we’ve discussed, <span class="math inline">\(L(\theta; \mathbf{y})\)</span> is the measure of likeliness of a parameter’s values <span class="math inline">\(\theta\)</span> for the observed data <span class="math inline">\(\mathbf{y}\)</span>. It is only natural then to seek the “most likely” value of <span class="math inline">\(\theta\)</span>. The MLE <span class="math inline">\(\hat{\theta}\)</span> of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = \arg\max_{\theta\in\Theta} L(\theta; \mathbf{y})\)</span> — the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood. Often we are interested in a function of a parameter <span class="math inline">\(\xi = g(\theta)\)</span>. When <span class="math inline">\(g(\cdot)\)</span> is 1:1, the MLE of <span class="math inline">\(\xi\)</span> is the function <span class="math inline">\(g(\cdot)\)</span> applied to the MLE of <span class="math inline">\(\theta\)</span>: <span class="math inline">\(\hat{\xi} = g(\hat{\theta})\)</span>, the proof of which is just a reparameterization of the likelihood in terms of <span class="math inline">\(\xi\)</span> instead of <span class="math inline">\(\theta\)</span>. Although the conception of the MLE was motivated by an intuitively clear underlying idea, the justification for its use is much deeper. It is a really “good” method of estimation (to be discussed later on the topic of asymptotics).</p>
<p>Another popular method of estimation is the <strong>Method of Moments</strong>. Its main idea is based on expressing the population moments of the distribution of data in terms of its unknown parameter(s) and equating them to their corresponding sample moments. MMEs have some known problems: consider a sample of size 4 from a uniform distribution <span class="math inline">\(U(0, \theta)\)</span> with the observed sample 0.2, 0.6, 2, and 0.4. The MME is 1.6, which does not make much sense given the observed value of 2 in the sample. For these and related reasons, the MMEs are less used than the MLE counterparts. However, MMEs are usually simpler to compute and can be used, for example, as reasonable initial values in numerical iterative procedures for MLEs. On the other hand, the Method of Moments does not require knowledge of the entire distribution of the data (up to the unknown parameters) but only its moments and thus may be less sensitive to possible misspecification of a model.</p>
<p>The <strong>Method of Least Squares</strong> play a key role in regression and analysis of variance. In a typical regression setup, we are given <span class="math inline">\(n\)</span> observations <span class="math inline">\((\mathbf{x}_i, y_i)\)</span>, <span class="math inline">\(i=1, \ldots, n\)</span> over <span class="math inline">\(m\)</span> explanatory variables <span class="math inline">\(\mathbf{x} = (x_1, \ldots, x_m)\)</span> and the response variable <span class="math inline">\(\mathbf{Y}\)</span>. We assume that <span class="math inline">\(y_i = g_\theta(\mathbf{x}_i) + \varepsilon_i\)</span>, <span class="math inline">\(i = 1, \ldots, n\)</span> where the response function <span class="math inline">\(g_\theta(\cdot): \mathbb{R}^m \rightarrow \mathbb{R}\)</span> has a known parametric form and depends on <span class="math inline">\(p \le n\)</span> unknown parameters <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span>. The LSE looks for a <span class="math inline">\(\hat{\theta}\)</span> that yields the best fit <span class="math inline">\(g_\hat{\theta}(\mathbf{x})\)</span> to the observed <span class="math inline">\(\mathbf{y}\)</span> w.r.t.&nbsp;the Euclidean distance: <span class="math inline">\(\hat{\theta} = \arg\min_\theta \sum_{i=1}^n (y_i - g_\theta(\mathbf{x}_i))^2\)</span>. For linear regression, the solution is available in closed form. For non-linear regression, however, it can generally be only found numerically.</p>
<p>More generally than the three above procedures, one can consider any function <span class="math inline">\(\rho(\theta, y)\)</span> as a measure of the goodness-of-fit and look for an estimator that maximizes or minimizes <span class="math inline">\(\sum_{i=1}^n \rho(\theta, y_i)\)</span> w.r.t.&nbsp;<span class="math inline">\(\theta\)</span>. Such estimators are called <strong>M-estimators</strong>. It runs out that various well-known estimators can be viewed as M-estimators for a particular <span class="math inline">\(\rho(\theta, y)\)</span> including <span class="math inline">\(\bar{Y}\)</span> for the sample mean as well as MLE, LSE, and a generalized version of MME not yet discussed. As we’ll see when discussing asymptotics, M-estimators share many important asymptotic properties.</p>
<p>A natural question is how to compare between various estimators. First, we should define a measure of goodness-of-estimation. Recall that any estimator <span class="math inline">\(\hat{\theta} = \hat{\theta}(Y_1, \ldots, Y_n)\)</span> is a function of a random sample and therefore is a random variable itself with a certain distribution, expectation, variance, etc. A somewhat naive attempt to measure the goodness-of-estimation of <span class="math inline">\(\hat{\theta}\)</span> would be to consider the error <span class="math inline">\(|\hat{\theta}-\theta|\)</span>. However, <span class="math inline">\(\theta\)</span> is unknown and, as we have mentioned, an estimator <span class="math inline">\(\hat{\theta}\)</span> is a random variable and hence the value <span class="math inline">\(|\hat{\theta}-\theta|\)</span> will vary from sample to sample. It may be “small” for some of the samples, while “large” for others and therefore cannot be used as a proper criterion for goodness-of-estimation of an estimator <span class="math inline">\(\hat{\theta}\)</span>. A more reasonable measure would then be an average distance over all possible samples, that is, the mean absolute error <span class="math inline">\(E|\hat{\theta}-\theta|\)</span>, where the expectation is taken w.r.t.&nbsp;the joint distribution of <span class="math inline">\(\mathbf{Y} = (Y_1, \ldots, Y_n)\)</span>. It indeed can be used as a measure of goodness-of-estimation but usually, mostly due to convenience of differentiation, the conventional measure is the <strong>mean squared error</strong> (MSE) given by <span class="math inline">\(MSE(\hat{\theta}, \theta) = E(\hat{\theta}-\theta)^2\)</span>.</p>
<p>The MSE can be decomposed into two components: <span class="math inline">\(MSE(\hat{\theta}, \theta) = Var(\hat{\theta}) + b^2(\hat{\theta},\theta)\)</span>. The first is the stochastic error (variance) and the second is a systematic or deterministic error (bias). Having defined the goodness-of-estimation measure by MSE, one can compare different estimators and choose the one with the smallest MSE. However, since the <span class="math inline">\(MSE(\hat{\theta}, \theta)\)</span> typically depends on the unknown <span class="math inline">\(\theta\)</span>, it is a common situation where no estimator is uniformly superior for all <span class="math inline">\(\theta \in \Theta\)</span>.</p>
<p>Ideally, a good estimator with a small MSE should have both low variance and low bias. However, it might be hard to have both. One of the common approaches is to first control the bias component of the overall MSE and to consider unbiased estimators. There is no general rule or algorithm for deriving an unbiased estimator. In fact, unbiasedness is a property of an estimator rather than a method of estimation. One usually checks an MLE or any other estimator for bias. Sometimes one can then modify the original estimator to “correct” its bias. Note that unlike ML estimation, unbiasedness is not invariant under nonlinear transformation of the original parameter: if <span class="math inline">\(\hat{\theta}\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(g(\hat{\theta})\)</span> is generally a biased estimator for <span class="math inline">\(g(\theta)\)</span>.</p>
<p>What does unbiasedness of an estimator <span class="math inline">\(\hat{\theta}\)</span> actually mean? Suppose we were observing not a single sample but all possible samples of size <span class="math inline">\(n\)</span> from a sample space and were calculating the estimates <span class="math inline">\(\hat{\theta}_j\)</span> for each one of them. The unbiasedness means that the average value of <span class="math inline">\(\hat{\theta}\)</span> over the entire sample space is <span class="math inline">\(\theta\)</span>, but it does not guarantee yet that <span class="math inline">\(\hat{\theta}_j \approx \theta\)</span> for each particular sample. The dispersion of <span class="math inline">\(\hat{\theta}_j\)</span>’s around their average value <span class="math inline">\(\theta\)</span> might be large and, since in reality we have only a single sample, its particular value of <span class="math inline">\(\hat{\theta}\)</span> might be quite away from <span class="math inline">\(\theta\)</span>. To ensure with high confidence that <span class="math inline">\(\hat{\theta}_j \approx \theta\)</span> for any sample we need in addition for the variance <span class="math inline">\(Var(\hat{\theta})\)</span> to be small.</p>
<p>An estimator <span class="math inline">\(\hat{\theta}\)</span> is called a <strong>uniformly minimum variance unbiased estimator</strong> (UMVUE) of <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\hat{\theta}\)</span> is unbiased and for any other unbiased estimator <span class="math inline">\(\tilde{\theta}\)</span> of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(Var(\hat{\theta}) \le Var(\tilde{\theta})\)</span>. If the UMVUE exists, it is necessarily unique. Recall that there is no general algorithm to obtain unbiased estimators in general and a UMVUE in particular. However, there exists a lower bound for a variance of an unbiased estimator, which can be used as a benchmark for evaluating its goodness.</p>
<p>Define the <strong>Fisher Information Number</strong> <span class="math inline">\(I(\theta) = E((\ln f_\theta(\mathbf{y}))'_\theta)^2\)</span>. The derivative of the log density is sometimes called the <strong>Score Function</strong>. Thus, the Fisher Information Number is the expected square of the Score. The <strong>Cramer-Rao Lower Bound Theorem</strong> states that if <span class="math inline">\(T\)</span> is an unbiased estimator for <span class="math inline">\(g(\theta)\)</span>, where <span class="math inline">\(g(\cdot)\)</span> is differentiable, then <span class="math inline">\(Var(T) \ge (g'(\theta))^2 / I(\theta)\)</span> or more simply, when <span class="math inline">\(T\)</span> is an unbiased estimator for <span class="math inline">\(\theta\)</span>, <span class="math inline">\(Var(T) \ge 1 / I(\theta)\)</span>. We are especially interested in the case where <span class="math inline">\(Y_1, \ldots, Y_n\)</span> is a random sample from a distribution <span class="math inline">\(f_\theta(y)\)</span>. In that case, <span class="math inline">\(I(\theta) = nI^*(\theta)\)</span> where <span class="math inline">\(I^*(\theta) = E((\ln f_\theta(y))'_\theta)^2\)</span> is the Fisher Information Number of <span class="math inline">\(f_\theta(y)\)</span>, and, therefore, for any unbiased estimator <span class="math inline">\(T\)</span> of <span class="math inline">\(g(\theta)\)</span>, we have that <span class="math inline">\(Var(T) \ge (g'_\theta(\theta))^2 / nI^*(\theta)\)</span>. There is another, usually more convenient formula for calculating the Fisher Information Number <span class="math inline">\(I(\theta)\)</span> other than its direct definition: <span class="math inline">\(I(\theta) = -E(\ln f_\theta(\mathbf{Y}))''_\theta\)</span>.</p>
<p>It is important to emphasize that the CRLB theorem is one direction only: if the variance of an unbiased estimator does not achieve the Cramer-Rao lower bound, one still cannot claim that it is not an UMVUE. Nevertheless, it can be used as a benchmark for measuring the goodness of an unbiased estimator. One special result related to the exponential family distribution: the Cramer-Rao lower bound is achieved only for distributions from the exponential family.</p>
<p>The Cramer-Rao lower bound allows one only to evaluate the goodness of a proposed unbiased estimator but does not provide any constructive way to derive it. In fact, as we have argued, there is no such general rule at all. However, if one manages to obtain any initial (even crude) unbiased estimator, it may be possible to improve it. The <strong>Rao-Blackwell Theorem</strong> shows that if there is an unbiased estimator that is not a function of a sufficient statistic <span class="math inline">\(W\)</span>, one can construct another unbiased estimator based on <span class="math inline">\(W\)</span> with an MSE not larger than the original one: let <span class="math inline">\(T\)</span> be an unbiased estimator of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(W\)</span> be a sufficient statistic for <span class="math inline">\(\theta\)</span>, and define <span class="math inline">\(T_1 = E(T|W)\)</span>, then <span class="math inline">\(T_1\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(Var(T_1) \le Var(T)\)</span>. Thus, in terms of MSE, only unbiased estimators based on a sufficient statistic are of interest. This demonstrates again a strong sense of the notion of sufficiency.</p>
<p>Does Rao-Blackwellization necessarily yield an UMVUE? Generally not. To guarantee UMVUE an additional requirement of completeness on a sufficient statistic <span class="math inline">\(W\)</span> is needed. The <strong>Lehmann-Scheffe Theorem</strong> formalizes this: if <span class="math inline">\(T\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(W\)</span> is a complete sufficient statistic for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(T_1 = E(T|W)\)</span> is the unique UMVUE of <span class="math inline">\(\theta\)</span>. Even without the Lehmann-Scheffe theorem, it can be shown under mild conditions that if the distribution of the data belongs to the exponential family and an unbiased estimator is a function of the corresponding sufficient statistic, it is an UMVUE. Note that despite its elegance, the application of the Rao-Blackwell Theorem in more complicated cases is quite limited. The two main obstacles are in finding an initial unbiased estimator <span class="math inline">\(T\)</span> and calculating the conditional expectation <span class="math inline">\(E(T|W)\)</span></p>
</section>
<section id="confidence-intervals-bounds-and-regions" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Confidence Intervals, Bounds, and Regions</h1>
<p>When we estimate a parameter we essentially “guess” its value. This should be a “well educated guess,” hopefully the best of its kind (in whatever sense). However, statistical estimation is made with error. Presenting just the estimator, no matter how good it is, is usually not enough – one should give an idea about the estimation error. The words “estimation” and “estimate” (in contrast to “measure” and “value”) point to the fact that an estimate is an inexact appraisal of a value. Great efforts of statistics are invested in trying to quantify the estimation error and find ways to express it in a well-defined way.</p>
<p>Any estimator has error, that is, if <span class="math inline">\(\theta\)</span> is estimated by <span class="math inline">\(\hat{\theta}\)</span>, then <span class="math inline">\(\hat{\theta} - \theta\)</span> is usually different from <span class="math inline">\(0\)</span>. The standard measure of error is the mean squared error <span class="math inline">\(MSE = E(\hat{\theta} - \theta)^2\)</span>. When together with the value of the estimator we are given its MSE, we get a feeling of how precise the estimate is. It is more common to quote the standard error defined by <span class="math inline">\(SE(\hat{\theta}) = \sqrt{MSE(\hat{\theta}, \theta)} = \sqrt{E(\hat{\theta}-\theta)^2}\)</span>. Unlike the MSE, the SE is measured in the same units as the estimator.</p>
<p>However, the MSE expresses an average squared estimation error but tells nothing about its distribution, which generally might be complicated. One would be interested, in particular, in the probability <span class="math inline">\(P(|\hat{\theta}-\theta| &gt; c)\)</span> that the estimation error exceeds a certain accuracy level <span class="math inline">\(c &gt; 0\)</span>. <strong>Markov’s Inequality</strong> enables us to translate the MSE into an upper bound on this probability: <span class="math inline">\(P(|\hat{\theta}-\theta| \ge c) \le MSE/c^2\)</span>. However, this bound is usually very conservative and the actual probability may be much smaller. Typically, a quite close approximation of the error distribution is obtained by the Central Limit Theorem (discussed later).</p>
<p>Above, we suggested quoting the SE besides the estimator itself. However, standard statistical practice is different and it is not very intuitive. There are a few conceptual difficulties in being precise when talking about the error. Suppose we estimate <span class="math inline">\(\mu\)</span> with <span class="math inline">\(\bar{Y}\)</span> and we find <span class="math inline">\(\bar{Y}=10\)</span> and <span class="math inline">\(SE=0.2\)</span>. We are quite confident that <span class="math inline">\(\mu\)</span> is between <span class="math inline">\(9.6\)</span> and <span class="math inline">\(10.4\)</span>. However, this statement makes no probabilistic sense. The true <span class="math inline">\(\mu\)</span> is either in the interval <span class="math inline">\((9.6,10.4)\)</span> or it’s not. The unknown <span class="math inline">\(\mu\)</span> is not a random variable – it has a single fixed value, whether or not it is known to the statistician.</p>
<p>The “trick” that has been devised is to move from a probabilistic statement about the unknown (but with a fixed value) parameter to a probabilistic statement about the method. We say something like: “The interval <span class="math inline">\((9.6,10.4)\)</span> for the value <span class="math inline">\(\mu\)</span> was constructed by a method with is 95% successful.” Since the method is usually successful, we have confidence in its output.</p>
<p>Let <span class="math inline">\(Y_1, \ldots, Y_n \sim f_\theta(y)\)</span>, <span class="math inline">\(\theta \in \Theta\)</span>. A <span class="math inline">\((1-\alpha)100\%\)</span> <strong>Confidence Interval</strong> for <span class="math inline">\(\theta\)</span> is the pair of scalar-valued statistics <span class="math inline">\(L=L(Y_1, \ldots, Y_n)\)</span> and <span class="math inline">\(U=U(Y_1, \ldots, Y_n)\)</span> such that <span class="math inline">\(P(L \le \theta \le U) \ge 1-\alpha\)</span> for all <span class="math inline">\(\theta \in \Theta\)</span> with the inequality as closs to an equality as possible.</p>
<p>What is the right value of <span class="math inline">\(\alpha\)</span>? Nothing in the statistical theory dictates a particular choice. But the standard values are <span class="math inline">\(0.10\)</span>, <span class="math inline">\(0.05\)</span>, and <span class="math inline">\(0.01\)</span> with <span class="math inline">\(0.05\)</span> being most common.</p>
<p>A <strong>pivot</strong> is a function <span class="math inline">\(\psi(Y_1, \ldots, Y_n; \theta)\)</span> of the data and the parameters, whose distribution does not depend on unknown parameters. Note, the pivot is not a statistic and cnanot be calculated form the data, exactly because it depends on the unknown parameters. On the other hand, since its <em>distribution</em> does not depend on the unknown parameters, we can find an interval <span class="math inline">\(A_\alpha\)</span> such that <span class="math inline">\(P(\psi(Y_1, \ldots, Y_n; \theta) \in A_\alpha) = 1 - \alpha\)</span>. Then we can invert the inclusion and define the interval (or region in general) <span class="math inline">\(C_\alpha = \{\theta : \psi(Y_1, \ldots, Y_n; \theta) \in A_\alpha \}\)</span>. The set <span class="math inline">\(C_\alpha\)</span> is then a <span class="math inline">\((1-\alpha)100\%\)</span> confidence set. Note that <span class="math inline">\(C_\alpha\)</span> is a random set because it depends on the random sample.</p>
<p>Does a pivot always exist? For a random sample <span class="math inline">\(Y_1, \ldots, Y_n\)</span> from any continuous distribution with a cdf <span class="math inline">\(F_\theta(y)\)</span>, the value <span class="math inline">\(-\sum_{i=1}^n \ln F_\theta(Y_i) \sim \frac{1}{2} \chi^2_{2n}\)</span> and, therefore, is a pivot. However, in the general case, it might be difficult (if possible) to invert the corresponding confidence interval for this pivot to a confidence interval for the original parameter of interest <span class="math inline">\(\theta\)</span>. More convenient pivots are easy to find when the distribution belongs to a scale-location family.</p>
<p>Is a confidence interval for <span class="math inline">\(\theta\)</span> necessarily unique? Definitely not! Different choices of pivots lead to different forms of confidence intervals. Moreover, even for a given pivot, one can typically construct an infinite set of confidence intervals at the same confidence level. What is the “best” choice for a confidence interval? A conventional, somewhat <em>ad hoc</em> approach is based on error symmetry: set <span class="math inline">\(P(L&gt;\theta) = P(U&lt;\theta) = \alpha/2\)</span>. A more appealing approach would be to seek a confidence interval of a minimal expected length; however, in general, this leads to a nonlinear minimization problem that might not have a solution in closed form.</p>
<p>A parameter of interest may be not the original parameter <span class="math inline">\(\theta\)</span> but its function <span class="math inline">\(g(\theta)\)</span>. Let <span class="math inline">\((L,U)\)</span> be a <span class="math inline">\((1-\alpha)100\%\)</span> confidence interval for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(g(\cdot)\)</span> a strictly increasing function. Then <span class="math inline">\((g(L),g(U))\)</span> is a <span class="math inline">\((1-\alpha)100\%\)</span> confidence interval for <span class="math inline">\(g(\theta)\)</span>.</p>
<p>The normal confidence intervals are undoubtedly the most important. We do not claim that most data sets are sampled from a normal distribution – this is definitely far from being true. However, what really matters is whether the distribution of an estimator <span class="math inline">\(\hat{\theta} = \hat{\theta}(Y_1, \ldots, Y_n)\)</span> is close to normal rather than the distribution of <span class="math inline">\(Y_1, \ldots, Y_n\)</span> themselves. When discussing asymptotics later, we argue that many estimators based on large or even medium sized samples are indeed approximately normal and, therefore, the normal confidence intervals can (at least approximately) be used.</p>
</section>
<section id="hypothesis-testing" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Hypothesis Testing</h1>
<p>To be added</p>
</section>
<section id="asymptotic-analysis" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Asymptotic Analysis</h1>
<p>To be added</p>
</section>
<section id="bayesian-inference" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Bayesian Inference</h1>
<p>To be added</p>
</section>
<section id="elements-of-statistical-decision-theory" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Elements of Statistical Decision Theory</h1>
<p>To be added</p>
</section>
<section id="linear-models" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Linear Models</h1>
<p>To be added</p>
</section>
<section id="nonparametric-estimation" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Nonparametric Estimation</h1>
<p>To be added</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.danyavorsky\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>