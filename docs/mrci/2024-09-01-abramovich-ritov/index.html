<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.330">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dan Yavorsky">
<meta name="dcterms.date" content="2023-08-13">
<meta name="description" content="description">

<title>Dan Yavorsky - Quotes and Notes from Statistical Theory: A Concise Introduction (2ed) by Felix Abramovich and Ya’acov Ritov</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../..//images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Dan Yavorsky</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../cv/index.html" rel="" target="">
 <span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../teaching.html" rel="" target="">
 <span class="menu-text">Teaching</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publications.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../mrci.html" rel="" target="">
 <span class="menu-text">MR &amp; CI</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../dcms-r.danyavorsky.com" rel="" target="">
 <span class="menu-text">R-DCMS</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../lit.html" rel="" target="">
 <span class="menu-text">To Read</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Quotes and Notes from <em>Statistical Theory: A Concise Introduction</em> (2ed) by Felix Abramovich and Ya’acov Ritov</h1>
                  <div>
        <div class="description">
          description
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="danyavorsky.com">Dan Yavorsky</a> <a href="https://orcid.org/0000-0003-4095-6405" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 13, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#point-estimation" id="toc-point-estimation" class="nav-link" data-scroll-target="#point-estimation"><span class="header-section-number">2</span> Point Estimation</a></li>
  <li><a href="#confidence-intervals-bounds-and-regions" id="toc-confidence-intervals-bounds-and-regions" class="nav-link" data-scroll-target="#confidence-intervals-bounds-and-regions"><span class="header-section-number">3</span> Confidence Intervals, Bounds, and Regions</a></li>
  <li><a href="#hypothesis-testing" id="toc-hypothesis-testing" class="nav-link" data-scroll-target="#hypothesis-testing"><span class="header-section-number">4</span> Hypothesis Testing</a></li>
  <li><a href="#asymptotic-analysis" id="toc-asymptotic-analysis" class="nav-link" data-scroll-target="#asymptotic-analysis"><span class="header-section-number">5</span> Asymptotic Analysis</a></li>
  <li><a href="#bayesian-inference" id="toc-bayesian-inference" class="nav-link" data-scroll-target="#bayesian-inference"><span class="header-section-number">6</span> Bayesian Inference</a></li>
  <li><a href="#elements-of-statistical-decision-theory" id="toc-elements-of-statistical-decision-theory" class="nav-link" data-scroll-target="#elements-of-statistical-decision-theory"><span class="header-section-number">7</span> Elements of Statistical Decision Theory</a></li>
  <li><a href="#linear-models" id="toc-linear-models" class="nav-link" data-scroll-target="#linear-models"><span class="header-section-number">8</span> Linear Models</a></li>
  <li><a href="#nonparametric-estimation" id="toc-nonparametric-estimation" class="nav-link" data-scroll-target="#nonparametric-estimation"><span class="header-section-number">9</span> Nonparametric Estimation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Get the book from Routledge (<a href="https://www.routledge.com/Statistical-Theory-A-Concise-Introduction/Abramovich-Ritov/p/book/9781032007458">here</a>) or Amazon (<a href="https://www.amazon.com/Statistical-Theory-Concise-Introduction-Chapman/dp/1032007451">here</a>)</p>
<p>This is a truly excellent book explaining the underlying ideas, mathematics, and principles of major statistical concepts. Its organization is suburb, and the authors’ commentary on <em>why</em> a theorem is so useful and <em>how</em> the presented ideas fit together and/or contrast is invaluable (and, quite frankly, better than I have seen anywhere else).</p>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Suppose the observed data is the sample <span class="math inline">\(\mathbf{y} = \{y_1, \ldots, y_n\}\)</span> of size <span class="math inline">\(n\)</span>. We will model <span class="math inline">\(y\)</span> as a realization of an <span class="math inline">\(n\)</span>-dimensional random vector <span class="math inline">\(\mathbf{Y} = \{Y_1, \ldots, Y_n\}\)</span> with a joint distribution <span class="math inline">\(f_\mathbf{Y}(\mathbf{y})\)</span>. The true distribution of the data is rarely completely known; nevertheless, it can often be reasonable to assume that it belongs to some family of distributions <span class="math inline">\(\mathcal{F}\)</span>. We will assume that <span class="math inline">\(\mathcal{F}\)</span> is a <em>parametric</em> family, that is, that we know the type of distribution <span class="math inline">\(f_\mathbf{Y}(\mathbf{y})\)</span> up to some unknown parameter(s) <span class="math inline">\(\theta \in \Theta\)</span>, where <span class="math inline">\(\Theta\)</span> is a parameter space. Typically, we will consider the case where <span class="math inline">\(y_1, \ldots, y_n\)</span> are the results of <em>independent</em> identical experiments. In this case, <span class="math inline">\(Y_1, \ldots, Y_n\)</span> can be treated as independent, identically distributed random variables with the common distribution <span class="math inline">\(f_\theta(y)\)</span> from a parametric family of distributions <span class="math inline">\(\mathcal{F}_\theta\)</span>, <span class="math inline">\(\theta \in \Theta\)</span>.</p>
<p>Define the <em>likelihood function</em> <span class="math inline">\(L(\theta; \mathbf{y}) = P_\theta(\mathbf{y})\)</span> — the probability to observe the given data <span class="math inline">\(\mathbf{y}\)</span> for any possible value of <span class="math inline">\(\theta \in \Theta\)</span>. First assume that <span class="math inline">\(\mathbf{Y}\)</span> is discrete. The value <span class="math inline">\(L(\theta; \mathbf{y})\)</span> can be viewed as a measure of likeliness of <span class="math inline">\(\theta\)</span> to the observed data <span class="math inline">\(\mathbf{y}\)</span>. If <span class="math inline">\(L(\theta_1; \mathbf{y}) &gt; L(\theta_2; \mathbf{y})\)</span> for a given <span class="math inline">\(\mathbf{y}\)</span>, we can say that the value <span class="math inline">\(\theta_1\)</span> for <span class="math inline">\(\theta\)</span> is more suited to the data than <span class="math inline">\(\theta_2\)</span>. For a continuous random variable <span class="math inline">\(\mathbf{y}\)</span>, the likelihood ratio <span class="math inline">\(L(\theta_1; \mathbf{y})/L(\theta_2; \mathbf{y})\)</span> shows the strength of the evidence in favor of <span class="math inline">\(\theta = \theta_1\)</span> vs <span class="math inline">\(\theta = \theta_2\)</span>.</p>
<p>A <em>statistic</em> <span class="math inline">\(T(\mathbf{Y})\)</span> is any real or vector-valued function that can be computed using the data alone. A statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is <em>sufficient</em> for an unknown parameter <span class="math inline">\(\theta\)</span> if the conditional distribution of all the data <span class="math inline">\(\mathbf{Y}\)</span> given <span class="math inline">\(T(\mathbf{Y})\)</span> does not depend on the <span class="math inline">\(\theta\)</span>. In other words, given <span class="math inline">\(T(\mathbf{Y})\)</span> no other information on <span class="math inline">\(\theta\)</span> can be extracted from <span class="math inline">\(\mathbf{y}\)</span>. This definition allows one to check whether a given statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is sufficient for <span class="math inline">\(\theta\)</span>, but it does not provide one with a constructive way to find it.</p>
<p>However, the <em>Fisher-Neyman Factorization Theorem</em> says that a statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is sufficient for <span class="math inline">\(\theta\)</span> iff for all <span class="math inline">\(\theta \in \Theta\)</span>, <span class="math inline">\(L(\theta, \mathbf{y}) = g(T(\mathbf{y}), \theta) \cdot h(\mathbf{y})\)</span>, where the function <span class="math inline">\(g(\cdot)\)</span> depends on <span class="math inline">\(\theta\)</span> and the statistic <span class="math inline">\(T(\mathbf{Y})\)</span>, while <span class="math inline">\(h(\mathbf{y})\)</span> does not depend on <span class="math inline">\(\theta\)</span>. In particular, if the likelihood <span class="math inline">\(L(\theta; \mathbf{y})\)</span> depends on data only through <span class="math inline">\(T(\mathbf{Y})\)</span>, then <span class="math inline">\(T(\mathbf{Y})\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(h(\mathbf{y}) = 1\)</span>.</p>
<p>A sufficient statistic is not unique. For example, the entire sample <span class="math inline">\(\mathbf{Y} = \{Y_1, \ldots, Y_n\}\)</span> is always a (trivial) sufficient statistic. We may seek a minimal sufficient statistic implying the maximal reduction of the data. A statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is called a minimal sufficient statistic if it is a function of any other sufficient statistic.</p>
<p>Another important property of a statistic is <em>completeness</em>. Let <span class="math inline">\(Y_1, \ldots, Y_n \sim f_\theta(y)\)</span>, where <span class="math inline">\(\theta \in \Theta\)</span>. A statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is complete if no statistic <span class="math inline">\(g(\mathbf{T})\)</span> exists (except <span class="math inline">\(g(\mathbf{T})=0\)</span>) such that <span class="math inline">\(E_\theta g(\mathbf{T}) = 0\)</span> for all <span class="math inline">\(\theta \in \Theta\)</span>. In other words, if <span class="math inline">\(E_\theta g(\mathbf{T}) = 0\)</span> for all <span class="math inline">\(\theta \in \Theta\)</span>, then necessarily <span class="math inline">\(g(\mathbf{T})=0\)</span>. To verify completeness for a general distribution can be a nontrivial mathematical problem, but thankfully it is much simpler for the exponential family of distributions that includes many of the “common” distributions. Completeness is a useful in determining minimal sufficiency because if a sufficient statistic <span class="math inline">\(T(\mathbf{Y})\)</span> is complete, then it is also minimal sufficient. (Note, however, that a minimal sufficient statistic may not necessarily be complete.)</p>
<p>A (generally multivariate) family of distributions <span class="math inline">\({f_\theta(\mathbf{y}): \theta \in \Theta}\)</span> is said to be an (one parameter) <em>exponential</em> family if: (1) <span class="math inline">\(\Theta\)</span> is an open interval, (2) the support of the distribution <span class="math inline">\(f_\theta\)</span> does not depend on <span class="math inline">\(\theta\)</span>, and (3) <span class="math inline">\(f_\theta(\mathbf{y}) = exp\{c(\theta)T(\mathbf{y}) + d(\theta) + S(\mathbf{y})\}\)</span> where <span class="math inline">\(c(\cdot)\)</span>, <span class="math inline">\(T(\cdot)\)</span>, <span class="math inline">\(d(\cdot)\)</span>, and <span class="math inline">\(S(\cdot)\)</span> are known functions; <span class="math inline">\(c(\theta)\)</span> is usually called the <em>natural parameter</em> of the distribution. We say that <span class="math inline">\(f_\theta\)</span> where <span class="math inline">\(\theta = (\theta_1, \ldots \theta_p)\)</span> belongs to a <span class="math inline">\(k\)</span>-parameter exponential family by changing (3) such that <span class="math inline">\(f_\theta(\mathbf{y}) = exp\{ \sum_{j=1}^k c_j(\theta)T_j(\mathbf{y}) + d(\theta) + S(\mathbf{y})\}\)</span>. The function <span class="math inline">\(c(\theta) = \{c_1(\theta), \ldots, c_k(\theta)\}\)</span> are the natural parameters of the distribution. (Note that the dimensionality <span class="math inline">\(p\)</span> of the original parameter <span class="math inline">\(\theta\)</span> is not necessarily the same as the dimensionality <span class="math inline">\(k\)</span> of the natural parameter <span class="math inline">\(c(\theta)\)</span>.)</p>
<p>Consider a random sample <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, where <span class="math inline">\(Y_i \sim f_\theta(y)\)</span> and <span class="math inline">\(f_\theta\)</span> belongs to a <span class="math inline">\(k\)</span>-parameter exponential family of distributions, then (1) the joint distribution of <span class="math inline">\(\mathbf{Y} = (Y_1, \ldots, Y_n)\)</span> also belongs to the <span class="math inline">\(k\)</span>-parameter exponential family, (2) <span class="math inline">\(T_\mathbf{Y} = (\sum_{i=1}^n T_1(Y_i), \ldots, \sum_{i=1}^n T_k(Y_i))\)</span> is the sufficient statistic for <span class="math inline">\(c(\theta) = (c_1(\theta), \ldots, c_k(\theta))\)</span> (and, therefore, for <span class="math inline">\(\theta\)</span>), and (3) if some regularity conditions hold, then <span class="math inline">\(T_\mathbf{Y}\)</span> is complete and therefore minimal sufficient (if the latter exists).</p>
</section>
<section id="point-estimation" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Point Estimation</h1>
<p>Estimation of the unknown parameters of distributions from the data is one of the key issues in statistics. A (point) <em>estimator</em> <span class="math inline">\(\hat{\theta} = \hat{\theta}(\mathbf{Y})\)</span> of an unknown parameter <span class="math inline">\(\theta\)</span> is any statistic used for estimating <span class="math inline">\(\theta\)</span>. The value of <span class="math inline">\(\hat{\theta}(\mathbf{y})\)</span> evaluated for a given sample is called an <em>estimate</em>. This is a general, somewhat trivial definition that does not say anything about the goodness of estimation; one would evidently be interested in “good” estimators.</p>
<p><em>Maximum likelihood estimation</em> is the most used method of estimation of parameters in parametric models. As we’ve discussed, <span class="math inline">\(L(\theta; \mathbf{y})\)</span> is the measure of likeliness of a parameter’s values <span class="math inline">\(\theta\)</span> for the observed data <span class="math inline">\(\mathbf{y}\)</span>. It is only natural then to seek the “most likely” value of <span class="math inline">\(\theta\)</span>. The MLE <span class="math inline">\(\hat{\theta}\)</span> of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = \arg\max_{\theta\in\Theta} L(\theta; \mathbf{y})\)</span> — the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood. Often we are interested in a function of a parameter <span class="math inline">\(\xi = g(\theta)\)</span>. When <span class="math inline">\(g(\cdot)\)</span> is 1:1, the MLE of <span class="math inline">\(xi\)</span> is the function <span class="math inline">\(g(\cdot)\)</span> applied to the MLE of <span class="math inline">\(\theta\)</span>: <span class="math inline">\(\hat{\xi} = g(\hat{\theta})\)</span>, the proof of which is just a reparameterization of the likelihood in terms of <span class="math inline">\(\xi\)</span> instead of <span class="math inline">\(\theta\)</span>. Although the conception of the MLE was motivated by an intuitively clear underlying idea, the justification for its use is much deeper. It is a really “good” method of estimation (to be discussed later on the topic of asymptotics).</p>
<p>Another popular method of estimation is the <em>Method of Moments</em>. Its main idea is based on expressing the population moments of the distribution of data in terms of its unknown parameter(s) and equating them to their corresponding sample moments. MMEs have some known problems: consider a sample of size 4 from a uniform distribution <span class="math inline">\(U(0, \theta)\)</span> with the observed sample 0.2, 0.6, 2, and 0.4. The MME is 1.6, which does not make much sense given the observed value of 2 in the sample. For these and related reasons, the MMEs are less used than the MLE counterparts. However, MMEs are usually simpler to compute and can be used, for example, as reasonable initial values in numerical iterative procedures for MLEs. On the other hand, the Method of Moments does not require knowledge of the entire distribution of the data (up to the unknown parameters) but only its moments and thus may be less sensitive to possible misspecification of a model.</p>
<p>The <em>method of least squares</em> play a key role in regression and analysis of variance. In a typical regression setup, we are given <span class="math inline">\(n\)</span> observations <span class="math inline">\((\mathbf{x}_i, y_i)\)</span>, <span class="math inline">\(i=1, \ldots, n\)</span> over <span class="math inline">\(m\)</span> explanatory variables <span class="math inline">\(\mathbf{x} = (x_1, \ldots, x_m)\)</span> and the response variable <span class="math inline">\(\mathbf{Y}\)</span>. We assume that <span class="math inline">\(y_i = g_\theta(\mathbf{x}_i) + \varepsilon_i\)</span>, <span class="math inline">\(i = 1, \ldots, n\)</span> where the response function <span class="math inline">\(g_\theta(\cdot): \mathbb{R}^m \rightarrow \mathbb{R}\)</span> has a known parametric form and depends on <span class="math inline">\(p \le n\)</span> unknown parameters <span class="math inline">\(\theta = (\theta_1, \ldots, \theta_p)\)</span>. The LSE looks for a <span class="math inline">\(\hat{\theta}\)</span> that yields the best fit <span class="math inline">\(g_\hat{\theta}(\mathbf{x})\)</span> to the observed <span class="math inline">\(\mathbf{y}\)</span> w.r.t.&nbsp;the Euclidean distance: <span class="math inline">\(\hat{\theta} = \arg\min_\theta \sum_{i=1}^n (y_i - g_\theta(\mathbf{x}_i))^2\)</span>. For linear regression, the solution is available in closed form. For non-linear regression, however, it can generally be only found numerically.</p>
<p>More generally than the three above procedures, one can consider any function <span class="math inline">\(\rho(\theta, y)\)</span> as a measure of the goodness-of-fit and look for an estimator that maximizes or minimizes <span class="math inline">\(\sum_{i=1}^n \rho(\theta, y_i)\)</span> w.r.t.&nbsp;<span class="math inline">\(\theta\)</span>. Such estimators are called <em>M-estimators</em>. It runs out that various well-known estimators can be viewed as M-estimators for a particular <span class="math inline">\(\rho(\theta, y)\)</span> including <span class="math inline">\(\bar{Y}\)</span> for the sample mean as well as MLE, LSE, and a generalized version of MME not yet discussed. As we’ll see when discussing asymptotics, M-estimators share many important asymptotic properties.</p>
<p>A natural question is how to compare between various estimators. First, we should define a measure of goodness-of-estimation. Recall that any estimator <span class="math inline">\(\hat{\theta} = \hat{\theta}(Y_1, \ldots, Y_n)\)</span> is a function of a random sample and therefore is a random variable itself with a certain distribution, expectation, variance, etc. A somewhat naive attempt to measure the goodness-of-estimation of <span class="math inline">\(\hat{\theta}\)</span> would be to consider the error <span class="math inline">\(|\hat{\theta}-\theta|\)</span>. However, <span class="math inline">\(\theta\)</span> is unknown and, as we have mentioned, an estimator <span class="math inline">\(\hat{\theta}\)</span> is a random variable and hence the value <span class="math inline">\(|\hat{\theta}-\theta|\)</span> will vary from sample to sample. It may be “small” for some of the samples, while “large” for others and therefore cannot be used as a proper criterion for goodness-of-estimation of an estimator <span class="math inline">\(\hat{\theta}\)</span>. A more reasonable measure would then be an average distance over all possible samples, that is, the mean absolute error <span class="math inline">\(E|\hat{\theta}-\theta|\)</span>, where the expectation is taken w.r.t.&nbsp;the joint distribution of <span class="math inline">\(\mathbf{Y} = (Y_1, \ldots, Y_n)\)</span>. It indeed can be used as a measure of goodness-of-estimation but usually, mostly due to convenience of differentiation, the conventional measure is the <em>mean squared error</em> (MSE) given by <span class="math inline">\(MSE(\hat{\theta}, \theta) = E(\hat{\theta}-\theta)^2\)</span>.</p>
<p>The MSE can be decomposed into two components: <span class="math inline">\(MSE(\hat{\theta}, \theta) = Var(\hat{\theta}) + b^2(\hat{\theta},\theta)\)</span>. The first is the stochastic error (variance) and the second is a systematic or deterministic error (bias). Having defined the goodness-of-estimation measure by MSE, one can compare different estimators and choose the one with the smallest MSE. However, since the <span class="math inline">\(MSE(\hat{\theta}, \theta)\)</span> typically depends on the unknown <span class="math inline">\(\theta\)</span>, it is a common situation where no estimator is uniformly superior for all <span class="math inline">\(\theta \in \Theta\)</span>.</p>
<p>Ideally, a good estimator with a small MSE should have both low variance and low bias. However, it might be hard to have both. One of the common approaches is to first control the bias component of the overall MSE and to consider unbiased estimators. There is no general rule or algorithm for deriving an unbiased estimator. In fact, unbiasedness is a property of an estimator rather than a method of estimation. One usually checks an MLE or any other estimator for bias. Sometimes one can then modify the original estimator to “correct” its bias. Note that unlike ML estimation, unbiasedness is not invariant under nonlinear transformation of the original parameter: if <span class="math inline">\(\hat{\theta}\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(g(\hat{\theta})\)</span> is generally a biased estimator for <span class="math inline">\(g(\theta)\)</span>.</p>
<p>What does unbiasedness of an estimator <span class="math inline">\(\hat{\theta}\)</span> actually mean? Suppose we were observing not a single sample but all possible samples of size <span class="math inline">\(n\)</span> from a sample space and were calculating the estimates <span class="math inline">\(\hat{\theta}_j\)</span> for each one of them. The unbiasedness means that the average value of <span class="math inline">\(\hat{\theta}\)</span> over the entire sample space is <span class="math inline">\(\theta\)</span>, but it does not guarantee yet that <span class="math inline">\(\hat{\theta}_j \approx \theta\)</span> for each particular sample. The dispersion of <span class="math inline">\(\hat{\theta}_j\)</span>’s around their average value <span class="math inline">\(\theta\)</span> might be large and, since in reality we have only a single sample, its particular value of <span class="math inline">\(\hat{\theta}\)</span> might be quite away from <span class="math inline">\(\theta\)</span>. To ensure with high confidence that <span class="math inline">\(\hat{\theta}_j \approx \theta\)</span> for any sample we need in addition for the variance <span class="math inline">\(Var(\hat{\theta})\)</span> to be small.</p>
<p>An estimator <span class="math inline">\(\hat{\theta}\)</span> is called a <em>uniformly minimum variance unbiased estimator</em> (UMVUE) of <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\hat{\theta}\)</span> is unbiased and for any other unbiased estimator <span class="math inline">\(\tilde{\theta}\)</span> of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(Var(\hat{\theta}) \le Var(\tilde{\theta})\)</span>. If the UMVUE exists, it is necessarily unique. Recall that there is no general algorithm to obtain unbiased estimators in general and a UMVUE in particular. However, there exists a lower bound for a variance of an unbiased estimator, which can be used as a benchmark for evaluating its goodness.</p>
<p>Define the <em>Fisher Information Number</em> <span class="math inline">\(I(\theta) = E((\ln f_\theta(\mathbf{y}))'_\theta)^2\)</span>. The derivative of the log density is sometimes called the <em>Score Function</em>. Thus, the Fisher Information Number is the expected square of the Score. The <em>Cramer-Rao Lower Bound Theorem</em> states that if <span class="math inline">\(T\)</span> is an unbiased estimator for <span class="math inline">\(g(\theta)\)</span>, where <span class="math inline">\(g(\cdot)\)</span> is differentiable, then <span class="math inline">\(Var(T) \ge (g'(\theta))^2 / I(\theta)\)</span> or more simply, when <span class="math inline">\(T\)</span> is an unbiased estimator for <span class="math inline">\(\theta\)</span>, <span class="math inline">\(Var(T) \ge 1 / I(\theta)\)</span>. We are especially interested in the case where <span class="math inline">\(Y_1, \ldots, Y_n\)</span> is a random sample from a distribution <span class="math inline">\(f_\theta(y)\)</span>. In that case, <span class="math inline">\(I(\theta) = nI^*(\theta)\)</span> where <span class="math inline">\(I*(\theta) = E((\ln f_\theta(y))'_\theta)^2\)</span> is the Fisher Information Number of <span class="math inline">\(f_\theta(y)\)</span>, and, therefore, for any unbiased estimator <span class="math inline">\(T\)</span> of <span class="math inline">\(g(\theta)\)</span>, we have that <span class="math inline">\(Var(T) \ge (g'_\theta(\theta))^2 / nI^*(\theta)\)</span>. There is another, usually more convenient formula for calculating the Fisher Information Number <span class="math inline">\(I(\theta)\)</span> other than its direct definition: <span class="math inline">\(I(\theta) = -E(\ln f_\theta(\mathbf{Y}))''_\theta\)</span>.</p>
<p>It is important to emphasize that the CRLB theorem is one direction only: if the variance of an unbiased estimator does not achieve the Cramer-Rao lower bound, one still cannot claim that it is not an UMVUE. Nevertheless, it can be used as a benchmark for measuring the goodness of an unbiased estimator. One special result related to the exponential family distribution: the Cramer-Rao lower bound is achieved only for distributions from the exponential family.</p>
<p>The Cramer-Rao lower bound allows one only to evaluate the goodness of a proposed unbiased estimator but does not provide any constructive way to derive it. In fact, as we have argued, there is no such general rule at all. However, if one manages to obtain any initial (even crude) unbiased estimator, it may be possible to improve it. The <em>Rao-Blackwell Theorem</em> shows that if there is an unbiased estimator that is not a function of a sufficient statistic <span class="math inline">\(W\)</span>, one can construct another unbiased estimator based on <span class="math inline">\(W\)</span> with an MSE not larger than the original one: let <span class="math inline">\(T\)</span> be an unbiased estimator of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(W\)</span> be a sufficient statistic for <span class="math inline">\(\theta\)</span>, and define <span class="math inline">\(T_1 = E(T|W)\)</span>, then <span class="math inline">\(T_1\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(Var(T_1) \le Var(T)\)</span>. Thus, in terms of MSE, only unbiased estimators based on a sufficient statistic are of interest. This demonstrates again a strong sense of the notion of sufficiency.</p>
<p>Does Rao-Blackwellization necessarily yield an UMVUE? Generally not. To guarantee UMVUE an additional requirement of completeness on a sufficient statistic <span class="math inline">\(W\)</span> is needed. The <em>Lehmann-Scheffe Theorem</em> formalizes this: if <span class="math inline">\(T\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(W\)</span> is a complete sufficient statistic for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(T_1 = E(T|W)\)</span> is the unique UMVUE of <span class="math inline">\(\theta\)</span>. Even without the Lehmann-Scheffe theorem, it can be shown under mild conditions that if the distribution of the data belongs to the exponential family and an unbiased estimator is a function of the corresponding sufficient statistic, it is an UMVUE. Note that despite its elegance, the application of the Rao-Blackwell Theorem in more complicated cases is quite limited. The two main obstacles are in finding an initial unbiased estimator <span class="math inline">\(T\)</span> and calculating the conditional expectation <span class="math inline">\(E(T|W)\)</span></p>
</section>
<section id="confidence-intervals-bounds-and-regions" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Confidence Intervals, Bounds, and Regions</h1>
</section>
<section id="hypothesis-testing" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Hypothesis Testing</h1>
</section>
<section id="asymptotic-analysis" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Asymptotic Analysis</h1>
</section>
<section id="bayesian-inference" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Bayesian Inference</h1>
</section>
<section id="elements-of-statistical-decision-theory" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Elements of Statistical Decision Theory</h1>
</section>
<section id="linear-models" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Linear Models</h1>
</section>
<section id="nonparametric-estimation" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Nonparametric Estimation</h1>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>