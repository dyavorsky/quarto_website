[
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Econometrics \n            \n            \n                MFE 402 | \n                UCLA\n            \n            A broad introduction to fundamental econometric models, methods of estimation, and approaches to statistical inference; implementation in R\n            \n                Syllabus\n                Slides\n                Evals 2023.1\n                Evals 2023.2\n                Evals 2022.1\n                Evals 2022.2\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Marketing Strategy and Policy \n            \n            \n                EMBA 411 | \n                UCLA\n            \n            A solid foundation for strategic marketing thinking; an introduction to a series of frameworks and tools that may be used to solve marketing problems\n            \n                Syllabus\n                Evals 2022.1\n                Evals 2022.2\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Marketing Analytics \n            \n            \n                MGTA 495 | \n                UCSD\n            \n            A survey of analytically-based consumer insights methods that inform marketing strategy\n            \n                Syllabus\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#graduate-courses",
    "href": "teaching/index.html#graduate-courses",
    "title": "Teaching",
    "section": "",
    "text": "Econometrics \n            \n            \n                MFE 402 | \n                UCLA\n            \n            A broad introduction to fundamental econometric models, methods of estimation, and approaches to statistical inference; implementation in R\n            \n                Syllabus\n                Slides\n                Evals 2023.1\n                Evals 2023.2\n                Evals 2022.1\n                Evals 2022.2\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Marketing Strategy and Policy \n            \n            \n                EMBA 411 | \n                UCLA\n            \n            A solid foundation for strategic marketing thinking; an introduction to a series of frameworks and tools that may be used to solve marketing problems\n            \n                Syllabus\n                Evals 2022.1\n                Evals 2022.2\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Marketing Analytics \n            \n            \n                MGTA 495 | \n                UCSD\n            \n            A survey of analytically-based consumer insights methods that inform marketing strategy\n            \n                Syllabus\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#undergraduate-courses",
    "href": "teaching/index.html#undergraduate-courses",
    "title": "Teaching",
    "section": "Undergraduate Courses",
    "text": "Undergraduate Courses\n\n\n\n    \n        \n            \n        \n        \n            \n                Customer Analytics \n            \n            \n                MGT 100 | \n                UCSD\n            \n            Combine customer data, economic theory, and statistical modeling to improve business decision-making related to segmentation, demand estimation, pricing, branding, and customer aquisition and retention; implementation in R\n            \n                Syllabus\n                Evals 2024\n                Evals 2023.1\n                Evals 2023.2\n                Evals 2022\n                KW Version\n            \n        \n    \n    \n        \n            \n        \n        \n            \n                Business Analytics \n            \n            \n                MGT 153 | \n                UCSD\n            \n            Develope proficiency with popular software to execute common analytic techniques; Excel, Tableau, SQL, R, Python\n            \n                Syllabus\n                Evals 2023\n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Content for You or Future Me",
    "section": "",
    "text": "How I Like Things on My Computers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentals of Version Control\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of Choice Probabilities\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html#posts",
    "href": "posts/index.html#posts",
    "title": "Content for You or Future Me",
    "section": "",
    "text": "How I Like Things on My Computers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFundamentals of Version Control\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of Choice Probabilities\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html#summaries",
    "href": "posts/index.html#summaries",
    "title": "Content for You or Future Me",
    "section": "Summaries",
    "text": "Summaries\n\n\n\n\n\n\n\n\n\n\nQuotes and Notes from Statistical Theory: A Concise Introduction (2ed)\n\n\nby Felix Abramovich and Ya’acov Ritov\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/101-gumble/index.html",
    "href": "posts/101-gumble/index.html",
    "title": "Gumble Distribution in Discrete Choice Models",
    "section": "",
    "text": "This short article seeks to describe the Gumbel (or Type-1 Generalized Extreme Value) distribution and how it’s use in probabilistic discrete choice models leads to a closed-form expression for choice probabilities.\nA standard framework for a probabilistic discrete choice model (DCM) reflecting a random utility theory (RUT) is as follows:\n\nThe utility that decision maker \\(i\\) obtains from alternative \\(j\\) is \\(U_{ij}\\), for \\(j = 1, \\ldots, J\\).\nThe decision maker chooses the alternative that provides the greatest utility: choose alternative \\(k\\) if and only if \\(U_{ik} &gt; U_{ij} \\hspace{1ex} \\forall j \\ne k\\).\nUtility is decomposed to reflect the researcher’s inability to understand or observe all factors that impact choices: \\(U_{ij} = V_{ij} + \\varepsilon_{ij}\\) where the joint density of the random vector \\(\\mathbf{\\varepsilon}_i = (\\varepsilon_{i1}, \\ldots, \\varepsilon_{iJ})\\) for decision maker \\(i\\) is \\(f_{\\mathbf{\\varepsilon}_i}(\\mathbf{\\varepsilon}_i)\\).\n\nThe probability that decision maker \\(i\\) chooses alternative \\(k\\) is:\n\\[\\begin{align*}\n    P_{ik}\n    &= \\text{Prob}\\left( U_{ik} &gt; U_{ij} \\; \\forall j \\ne k \\right)  &\\\\\n    &= \\text{Prob}\\left( V_{ik} + \\varepsilon_{ik} &gt; V_{ij} + \\varepsilon_{ij} \\; \\forall j \\ne k \\right)  &\\\\\n    &= \\text{Prob}\\left( \\varepsilon_{ij} &lt; \\varepsilon_{ik} + V_{ik} - V_{ij} \\; \\forall j \\ne k \\right)  &\\\\\n    &= \\int_{\\varepsilon_i} I\\left( \\varepsilon_{ij} &lt; \\varepsilon_{ik} + V_{ik} - V_{ij} \\; \\forall j \\ne k \\right) \\, f_{\\mathbf{\\varepsilon}_i}(\\mathbf{\\varepsilon}_i) \\, d \\mathbf{\\varepsilon}_i &\n\\end{align*}\\]\nwhere \\(I(\\cdot)\\) is the indicator function equaling one when the expression in parentheses is true and zero otherwise.\nA standard assumption about the joint distribution of vector \\(\\mathbf{\\varepsilon}_i\\) is that the components are independent and identically distributed as Gumbel random variables. Specifically, the cumulative distribution function for each \\(\\varepsilon_{ij}\\) is: \\[\n\\text{Prob}(\\varepsilon_{ij} \\leq x) = \\exp(-\\exp(-x)) = e^{-e^{-x}}\n\\] The Gumbel density and distribution are plotted below (in black) alongside the Normal (in dashed red) for comparison.\n\n\n\n\n\n\n\n\n\n\nBecause each \\(\\varepsilon_{ij}\\) is assumed to be independently distributed, the probability of decision maker \\(i\\) choosing alternative \\(k\\) for a given value of \\(\\varepsilon_{ik}\\) can be written as the product of the \\(J-1\\) conditional distributions:\n\\[\\begin{align*}\n    P_{ik} | \\varepsilon_{ik}\n    &= \\prod_{j \\ne k} \\exp(-\\exp(-\\varepsilon_{ik})) &\\\\\n    &= \\exp \\left( -\\sum_{j \\ne k} \\exp \\left( - ( \\varepsilon_{ik} + V_{ik} - V_{ij}) \\right) \\right) &\\\\\n    &= \\exp \\left( - \\exp(\\varepsilon_{ik}) \\times \\sum_{j \\ne k} \\exp \\left( V_{ij} - V_{ik}) \\right) \\right)\n\\end{align*}\\]\nBut of course \\(\\varepsilon_{ik}\\) is not given, so to obtain the desired probability \\(P_{ik}\\), we must integrate over all possible values of \\(\\varepsilon_{ik}\\), weighted by its marginal density \\(f_{\\varepsilon_{ik}}(\\varepsilon_{ik}) = \\exp(-\\varepsilon_{ik}) \\times \\exp ( -\\exp(\\varepsilon_{ik})\\): \\[\nP_{ik} = \\int_{-\\infty}^{\\infty} \\exp(-\\varepsilon_{ik}) \\times \\exp \\left( - \\exp(\\varepsilon_{ik}) \\times \\sum_{j=1}^J \\exp \\left( V_{ij} - V_{ik}) \\right) \\right) \\, d \\varepsilon_{ik}\n\\]\nFor notational simplicity, denote \\(a = \\sum_{j=1}^J \\exp ( V_{ij} - V_{ik})\\) and define the transformation of variables \\(z = \\exp(-\\varepsilon_{ik})\\). Then \\(dz = -\\exp(-\\varepsilon_{ik}) \\, d \\varepsilon_{ik} = -z \\, d \\varepsilon_{ik}\\) and the integral becomes: \\[\nP_{ik} = \\int_{\\infty}^{0} z \\times \\exp ( - z \\times a ) / (-z) \\, dz = \\int_{0}^{\\infty} \\exp (-z \\times a ) \\, dz\n\\]\nEvaluating the integral yields: \\[\nP_{ik} = - \\left[ \\frac{1}{a} \\times \\left( 0 - 1 \\right) \\right] = \\frac{1}{a} = \\frac{1}{\\sum_{j=1}^J \\exp ( V_{ij} - V_{ik})} = \\frac{\\exp ( V_{ik} )}{\\sum_{j = 1}^{J} \\exp ( V_{ij} )}\n\\]\nThis is the standard multinomial logit (MNL) choice probability."
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#get-quarto",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#get-quarto",
    "title": "Create a Website with Quarto",
    "section": "Get Quarto",
    "text": "Get Quarto\nQuarto comes bundled with RStudio (v2022.07.1 and later). So if you are using RStudio, you should already have Quarto installed. You can check your version of RStudio by going to the RStudio menu item Help &gt; About RStudio\nIf you are using an older version of RStudio, you can download and install Quarto:\n\nhttps://quarto.org/docs/get-started\n\nYou can confirm that Quarto is installed by either:\n\nIn the terminal, running quarto --version and checking that a version number is returned\nIn RStudio, creating a new Quarto document by going to File &gt; New File &gt; Quarto Document and rendering that document via Ctrl+Shift+K or by clicking the “Render” button (it has a blue arrow and is located just above the editor pane). This will create an HTML file in the same directory as the Quarto document, and open that file in RStudio’s Viewer tab or a new browser tab.\n\n\n\n\n(Alt+Click to zoom in)"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-1-create-website-directory-with-template",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-1-create-website-directory-with-template",
    "title": "Create a Website with Quarto",
    "section": "Step 1: Create Website Directory with Template",
    "text": "Step 1: Create Website Directory with Template\n\nCreate a new project in RStudio by going to File &gt; New Project or by using the dropdown at the top-right of RStudio\nChoose “New Directory” and then “Quarto Website”\nChoose a name for the directory that will hold the website files (I use quarto_website in this tutorial) and where you’ll store that directory on your computer\nUncheck the Git, renv, and Visual Editor options – you can enable those later if you wish\nClick “Create Project”"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-2a-inspect-the-template-directory",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-2a-inspect-the-template-directory",
    "title": "Create a Website with Quarto",
    "section": "Step 2a: Inspect the Template Directory",
    "text": "Step 2a: Inspect the Template Directory\nThe template directory contains:\n\na quarto.yml file, which is the configuration file for the website\ntwo files that will generate the content on two tabs of the website: index.qmd and about.qmd\na styles.scss file, which is a stylesheet to customize html elements of the website\nand likely also your .Rproj RStudio project file and a related .Rproj.user directory"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-2b-inspect-the-template-files",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-2b-inspect-the-template-files",
    "title": "Create a Website with Quarto",
    "section": "Step 2b: Inspect the Template Files",
    "text": "Step 2b: Inspect the Template Files\nThe _quarto.yml config file controls the website options\n\nadd line 3 in the image below: output-dir: docs to change the directory from the defualt _site to the GitHub-friendly docs so we don’t have to do this later\n\nThe index.qmd quarto file contains the content of the landing page for the website\n\n\n\n_quarto.yml\n\n\n\n\n\n\n\n\nindex.qmd"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-3-render-the-template-website",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-3-render-the-template-website",
    "title": "Create a Website with Quarto",
    "section": "Step 3: Render the Template Website",
    "text": "Step 3: Render the Template Website\nOther than the setting the output directory to be docs, let’s not worry about content and configuration just yet, and instead let’s ensure that the template renders correctly\nRender the website by clicking the “Render Website” button in RStudio’s Build tab\nYou should see the rendered HTML in either the Viewer tab or a new browser tab. The template has two pages (Home and About) and you can navigate to them via the links at the top (possibly hidden in hamburger menu, depending on your screen size)."
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-4a-customize-the-landing-page",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-4a-customize-the-landing-page",
    "title": "Create a Website with Quarto",
    "section": "Step 4a: Customize the Landing Page",
    "text": "Step 4a: Customize the Landing Page\nWe’ll change the landing page to be an “about you” page.\nQuarto contains an about option that uses a built-in template to layout the content of that section of the page (more info here)\n\nadd image: files/profile_pic.png and we’ll create that directory and add the image file soon\nspecify the about structure and select a template (jolla, trestles, solana, marquee, or broadside)\nadd links using the bootstrap5 icon, href, and possible text options\nadd some descriptive information about yourself"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-4b-update-navigation-to-the-landing-page",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-4b-update-navigation-to-the-landing-page",
    "title": "Create a Website with Quarto",
    "section": "Step 4b: Update Navigation to the Landing Page",
    "text": "Step 4b: Update Navigation to the Landing Page\nIn the _quarto.yml file, update the text on line 9, which controls the text displayed in the navigation bar for the landing page\nChange it from “Home” to “About”"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-4c-add-your-profile-pic",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-4c-add-your-profile-pic",
    "title": "Create a Website with Quarto",
    "section": "Step 4c: Add Your Profile Pic",
    "text": "Step 4c: Add Your Profile Pic\nCreate the subdirectory Files\nAdd your profile pic to the Files directory with the filename profile_pic.png (if you use a different filename, be sure to update the reference to it in the index.qmd header)"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-5a-change-the-templates-about-page-to-a-resume-page",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-5a-change-the-templates-about-page-to-a-resume-page",
    "title": "Create a Website with Quarto",
    "section": "Step 5a: Change the Template’s About Page to a Resume Page",
    "text": "Step 5a: Change the Template’s About Page to a Resume Page\nChange the filename from about.qmd to resume.qmd\nEdit the title to say “Resume”, “Curriculum Vitae”, or something similar\nAdd the line {{&lt; pdf files/resume.pdf &gt;}} possibly with the height and width options shown in the image below"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-5b-update-the-config-file",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-5b-update-the-config-file",
    "title": "Create a Website with Quarto",
    "section": "Step 5b: Update the Config File",
    "text": "Step 5b: Update the Config File\nUpdate the _quarto.yml config file to reflect the changed filename (resume.qmd instead of about.qmd)"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-5c-add-your-resume-pdf",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-5c-add-your-resume-pdf",
    "title": "Create a Website with Quarto",
    "section": "Step 5c: Add Your Resume PDF",
    "text": "Step 5c: Add Your Resume PDF\nAdd your resume to the files subdirectory\nCall the file resume.pdf (if you use a different filename, update the reference to it in the resume.qmd file)"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-6-render-your-website",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-6-render-your-website",
    "title": "Create a Website with Quarto",
    "section": "Step 6: Render Your Website",
    "text": "Step 6: Render Your Website\nOn the Build tab in RStudio, click the “Render Website” button"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-7a-add-a-blog-to-showcase-your-projects",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-7a-add-a-blog-to-showcase-your-projects",
    "title": "Create a Website with Quarto",
    "section": "Step 7a: Add a “Blog” to Showcase Your Projects",
    "text": "Step 7a: Add a “Blog” to Showcase Your Projects\nCreate a projects.qmd file in the quarto_website directory\nUse the special listing option to indicate a “listing” page with one thumbnail and link to each blog post\nIndicate that the contents of the blog will reside in a subdirectory named projects"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-7b-add-a-link-to-the-blog-in-navigation-bar",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-7b-add-a-link-to-the-blog-in-navigation-bar",
    "title": "Create a Website with Quarto",
    "section": "Step 7b: Add a Link to the “Blog” in Navigation Bar",
    "text": "Step 7b: Add a Link to the “Blog” in Navigation Bar\nIn the _quarto.yml config file, add a link to the “blog” via\n\n- href: projects.qmd to specify the file\ntext: Projects to specify the text in the Navbar"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-7c-create-the-blog-sub-directory-structure",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-7c-create-the-blog-sub-directory-structure",
    "title": "Create a Website with Quarto",
    "section": "Step 7c: Create the “Blog” Sub-directory Structure",
    "text": "Step 7c: Create the “Blog” Sub-directory Structure\nIn the quarto_website directory, create the sub-directory projects\nWithin the projects sub-directory, create sub-directories for your first two blog posts, which I’ve named project1 and project2\nIn each blog post directory, add an index.qmd file (they can be empty for now)"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-7d-add-content-to-first-blog-post",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-7d-add-content-to-first-blog-post",
    "title": "Create a Website with Quarto",
    "section": "Step 7d: Add Content to First Blog Post",
    "text": "Step 7d: Add Content to First Blog Post\nIn the quarto_website/projects/project1/index.qmd file, add some content using markdown syntax"
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-7e-render-your-website-and-view-your-blog-post",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#step-7e-render-your-website-and-view-your-blog-post",
    "title": "Create a Website with Quarto",
    "section": "Step 7e: Render Your Website and View Your Blog Post",
    "text": "Step 7e: Render Your Website and View Your Blog Post\nRender the website and view the listing page for the blog as well as your first blog post."
  },
  {
    "objectID": "posts/020-quarto-websites/slides/windows_rstudio/index.html#congrats---youve-got-a-website",
    "href": "posts/020-quarto-websites/slides/windows_rstudio/index.html#congrats---youve-got-a-website",
    "title": "Create a Website with Quarto",
    "section": "Congrats - You’ve Got A Website!",
    "text": "Congrats - You’ve Got A Website!\nTo learn more, definitely check out the quarto.org website on making websites: https://quarto.org/docs/websites/\nIf you found anything confusing in my tutorial, here are other excellent tutorials on developing Quarto websites and blogs that may help you troubleshoot:\n\n\n\nAuthor\nLink\n\n\n\n\nUCSB Med\nhttps://ucsb-meds.github.io/creating-quarto-websites/\n\n\nMarvin Schmitt\nhttps://www.marvinschmitt.com/blog/website-tutorial-quarto/\n\n\nAlbert Rapp\nhttps://albert-rapp.de/posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html\n\n\nSam Csik\nhttps://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/\n\n\n\nWhen in doubt, visit the GitHub page of someone else’s Quarto website to see how they did it!\n\nIn particular Andrew Heiss and Marvin Schmitt have fully-featured Quarto-based websites\nThe website by Emil Hvitfeldt, Meghan Hall, Mine Dogucu, and Thom Mock offer inspiration too\n\n\n\nwww.danyavorsky.com"
  },
  {
    "objectID": "posts/010-git-basics/index.html",
    "href": "posts/010-git-basics/index.html",
    "title": "Get Going with Git",
    "section": "",
    "text": "Git is a free and open source distributed version control system.\nThis post serves as a set of notes to quickly refer to. For detailed explanations, see these excellent free resources:\n\nPro Git by Scott Chacon and Ben Straub\nHappy Git and GitHub for the UserR by Jenny Bryan"
  },
  {
    "objectID": "posts/010-git-basics/index.html#whats-git",
    "href": "posts/010-git-basics/index.html#whats-git",
    "title": "Get Going with Git",
    "section": "",
    "text": "Git is a free and open source distributed version control system.\nThis post serves as a set of notes to quickly refer to. For detailed explanations, see these excellent free resources:\n\nPro Git by Scott Chacon and Ben Straub\nHappy Git and GitHub for the UserR by Jenny Bryan"
  },
  {
    "objectID": "posts/010-git-basics/index.html#getting-going",
    "href": "posts/010-git-basics/index.html#getting-going",
    "title": "Get Going with Git",
    "section": "Getting Going",
    "text": "Getting Going\n\nCheck to see if you have Git installed already:\n\nOn Windows, open the command prompt and type git --version or search for the application Git Bash.\n\nIf you see git version 2.43.0.windows.1 or something similar, you’re good to go.\n\nIf you see git is not recognized as an internal or external command, you need to install Git.\n\nOn Mac, open the terminal and type which git or git --version.\n\nIf you see something like /usr/local/bin/git or git version 2.39.3 (Apple Git-145), you’re good to go.\n\nIf you see command not found, you need to install Git.\n\n\n\n\nTo download or update Git\n\ngo to https://git-scm.com/downloads and install like any other software\nto update on Mac with Homebrew: brew upgrade git\nto update on Windows: git update git-for-windows\n\n\n\nTell Git who you are\n\nat the terminal (mac) or command prompt or git bash (Windows)\n\ntype git config --global user.name \"your name\" (can be your full name or github username)\ntype git config --global user.email \"your email\" (should be the email you use with GitHub)\ncheck the result with git config --list\n\n\n\n\nRegister a free GitHub account\n\npick a username that is short, timeless, professional, and (preferably) all lowercase\n\n\n\nLink your GitHub account and your local Git\n\nGit Credential Manager is the best way to store your GitHub credentials\n\non Windows, GCM is included with Git for Windows\non Mac, you can install GCM via Homebrew with brew install --cask git-credential-manager\n\nCreate a new repository on GitHub\nClone the repository to your local machine by running git clone URL where URL is the https web url of the repository you just created\nFollow the prompts to authenticate with your GitHub credentials"
  },
  {
    "objectID": "posts/010-git-basics/index.html#git-commands",
    "href": "posts/010-git-basics/index.html#git-commands",
    "title": "Get Going with Git",
    "section": "Git Commands",
    "text": "Git Commands\n\nThe basics\nTo copy down a GitHub repository to your local machine\n\ngit clone https://github.com/your-username/your-repo.git to copy down a repository from GitHub to your local machine\ngit remote --verbose to check the remote was cloned successfully\n\nTo add a remote to an existing local repository\n\ngit remote add origin https://github.com/your-username/your-repo.git to add a remote to an existing local repository with nickname origin\ngit remote --verbose to check the remote was added successfully\ngit remote show origin to see more details about the remote\n\nMost common workflow commands\n\ngit add newdoc.txt to stage a file for a commit\ngit commit -m \"a commit message\" to commit all staged files\ngit push origin main to push local commits (on branch main) up to the remote repository (origin)\ngit push -u origin main to push local commits up to the remote repository and have local main track remote main\ngit pull origin main to pull down the latest changes (on branch main) from the remote repository (origin)\n\nChecking status\n\ngit status to check on the state of your repository\ngit log --oneline to see a list of commits\ngit diff to see the changes you’ve made since the last commit\n\nBranching\n\ngit branch [branch-name] to create a new branch\ngit checkout [branch-name] to switch to the branch\ngit checkout -b [branch-name] to create a new branch and switch to it in one command\ngit checkout main to switch back to the main branch\ngit merge [branch-name] (when on main) to merge the branch into the main branch\n\n\n\nIntermediate\nComing Soon"
  },
  {
    "objectID": "links/index.html#r-friends",
    "href": "links/index.html#r-friends",
    "title": "An Incomplete Collection",
    "section": "R & Friends",
    "text": "R & Friends\n\nR Programming\n\nBig Book of R compiled by Oscar Baruffa\nR for Data Science by Hadley Wickham and Garrett Grolemund\nAdvanced R by Hadley Wickham\nAdavned R Solutions by Malte Grosser and Henning Bumann\nggplot2 by Hadley Wickham\nR Packages by Hadley Wickham and Jenny Bryan\n\n\n\nGit\n\nPro Git by Scott Chacon and Ben Straub\nHappy Git and GitHub for the UserR by Jenny Bryan\nGit in Practice by Mike McQuaid\n\n\n\nVim\n\nOpenVim by Henrik Huttunen\nMastering the Vim Language recorded presentation by Chris Toomey\nVim as Your Editor a 6-part playist by ThePrimeagen"
  },
  {
    "objectID": "links/index.html#less-academic",
    "href": "links/index.html#less-academic",
    "title": "An Incomplete Collection",
    "section": "Less Academic",
    "text": "Less Academic\n\nOn Life\n\nThis is Water by David Foster Wallace\nA Mathematician’s Lament by Paul Lockhart\nTeaching Smart People How to Learn by Chris Argyris\n\n\n\nOn Writing\n\nHow to Send and Reply to email by Matt Might\nPublication, Publication by Gary King\n\n\n\nOn Getting a PhD (in Econ or Marketing)\n\nKen Wilbur: Reasons to Get (or Not) a PhD and Choosing a field, Subfield, and PhD Program\nSusan Athey: Professional Advice\nJesse Shapiro: Notes on Applying for a PhD in Economics"
  },
  {
    "objectID": "links/index.html#more-academic",
    "href": "links/index.html#more-academic",
    "title": "An Incomplete Collection",
    "section": "More Academic",
    "text": "More Academic\n\nDemand Estimation\n\nBerry (1994) Estimating Discrete Choice Models of Product Differentiation [link]\nNevo (2000) A Practitioner’s Guide to Estimation of Random-Coefcients Logit Models of Demand [link]\nChintagunta & Nair (2011) Discrete Choice Models of Consumer Demand in Marketing [link]\nBerry and Haile (2021) Foundations of Demand Estimation [link]\nGandhi & Nevo (2021) Empirical Models of Demand and Supply in Differentiated Products Industries [link]\nUrsu, Seiler, & Honka (Draft) The Sequential Search Model: A Framework for Empirical Research [link]\n\n\n\nCausal Inference\n\nRossi (2014) Even the Rich Can Make Themselves Poor: A Critical Examination of IV Methods in Marketing Applications [link]\nAngrist (2022) Empirical Strategies in Economics: Illuminating the Path From Cause to Effect [link]\nHeckman and Pinto (Draft) Causality and Econometrics [link]"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dan Yavorsky",
    "section": "",
    "text": "SVP, Analytics | GBK Collective\nLecturer, Econometrics | UCLA Anderson\nLecturer, Customer Analytics | UCSD Rady\n\n\n\n\n\n\nGBK Collective | 2022 - Present\nBain & Co | 2020 - 2021\nCornerstone Research | 2006 - 2014\n\n\n\n\n\n\nPhD Quant. Marketing | UCLA Anderson 2020\nMBA Management | UCLA Anderson 2014\nCFA Charterholder (inactive) | CFA Institute 2012\nBA Economics & Mathematics | CMC 2006"
  },
  {
    "objectID": "index.html#current",
    "href": "index.html#current",
    "title": "Dan Yavorsky",
    "section": "",
    "text": "SVP, Analytics | GBK Collective\nLecturer, Econometrics | UCLA Anderson\nLecturer, Customer Analytics | UCSD Rady"
  },
  {
    "objectID": "index.html#work-history",
    "href": "index.html#work-history",
    "title": "Dan Yavorsky",
    "section": "",
    "text": "GBK Collective | 2022 - Present\nBain & Co | 2020 - 2021\nCornerstone Research | 2006 - 2014"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Dan Yavorsky",
    "section": "",
    "text": "PhD Quant. Marketing | UCLA Anderson 2020\nMBA Management | UCLA Anderson 2014\nCFA Charterholder (inactive) | CFA Institute 2012\nBA Economics & Mathematics | CMC 2006"
  },
  {
    "objectID": "posts/001-my-setup/index.html",
    "href": "posts/001-my-setup/index.html",
    "title": "My Computing Environment Preferences",
    "section": "",
    "text": "Terminal & Shell\n\nOn Mac\nThe Terminal is an application on your Mac. A Shell is a program that runs in the Terminal. It has a command-line interface (CLI) instead of the ubiquitous graphical-user interface (GUI) familiar to most computer users. To run commands in the Terminal via the Shell (or, equivalently, to execute programs on the command line) you type commands at the prompt and hit enter. I use iTerm2 terminal application with the default zsh shell on Mac, configured using Oh My Zsh.\nA nice installation guide is provided by Josean Martinez. It walks through installation of the homebrew package manager, the iTerm2 terminal application, Git version control software, the Oh My Zsh framework for managing the Zsh configuration, the PowerLevel10k Oh My Zsh theme, and Josean’s iTerm2 coolnight color scheme.\nMy PowerLevel10k theme preferences are the following:\n\n\nYes, install the Meslo Nerd Font\n\n\nClassic prompt style\n\n\nUnicode character set\n\n\nLight prompt color\n\n\n12-hour format for current time\n\n\nAngled prompt separator\n\n\nSharp prompt heads\n\n\nFlat prompt tails\n\n\n2-line prompt height\n\n\nSolid prompt connection\n\n\nNo prompt frame\n\n\nSparse prompt spacing\n\n\nMany icons\n\n\nConcise prompt flow\n\n\nEnable transient prompt\n\n\nVerbose instant prompt mode\n\n\nI also use the zsh-autosuggestions and zsh-syntax-highlighting plugins. A 3-step procedure downloads, installs, and enables most Zsh plug-ins:\n\ndownload the plug-in: git clone &lt;url&gt; $ZSH_CUSTOM/plugins/&lt;plugin-name&gt;\nadd the plugin name to the array in the ~/.zshrc file: plugins=(&lt;plugin-name&gt; &lt;plugin-name&gt; ...)\nreload the Zsh configuration file via source ~/.zshrc to enable the plugins\n\n\n\n\n\nRStudio\n\nVisual Look\nFor the visual look of RStudio, I use the RStudio theme Darkstudio and my own version of Atom’s One Dark editor theme, along with the Fira Code font. They can be installed as follows:\n\n# rstudio theme\ndevtools::install_github(\"https://github.com/rileytwo/darkstudio\")\n\npath_to_index &lt;- \"/Applications/RStudio.app/Contents/Resources/app/www/index.htm\"\ndarkstudio::activate(path_to_index) # on mac\ndarkstudio::activate() # on windows (open rstudio with \"run as admin\")\n\n# editor theme\nurl &lt;- \"https://github.com/dyavorsky/atom-inspired-rstudio-theme/blob/main/atom_inspired.rstheme\"\nrstudioapi::addTheme(url, apply = TRUE, force = TRUE)\n\n\n\nGlobal Options\nWhen installing RStudio on a new machine, ensure the following global options are set:\n\nUncheck Restore most recently opened project at startup\nUncheck Restore previously open source documents at startup\nUncheck Restore .RData into workspace at startup\nSet Save workspace to .RData on exit to “Never”\n\n\n\nMost-used shortcuts\nNavigation\n\nCtrl + 1 moves focus to the script editor\nCtrl + 2 moves focus to the console\nCtrl + Tab moves focus to the next script tab\nAlt + Shift + M moves focus to the terminal\n\nEditing\n\nAlt + Shift + Arrow starts multi-cursor editing\nAlt + Arrow moves the current line up or down\nAlt + Shift + Arrow duplicates the current line up or down\nCtrl + Space brings up RStudio’s path auto-completion menu (useful to avoid Co-pilot path auto-completion)\n\nRunning\n\nCtrl + Alt + B runs an R script from beginning to current line\nCtrl + Shift + K renders a Quarto document\n\n\n\n\n\nVS Code\n\nExtensions\nThemes:\n\nAtom One Dark by Mahmoud Ali\nMaterial by Equinusocio\nMaterial Icons by Equinusocio\n\nLanguages:\n\nR by REditorSupport\nQuarto by quarto.org\nPython by Microsoft\nJupyter by Microsoft\nJulia by julialang\nLaTeX Workshop by James Yu\n\nOther:\n\nGitHub Copilot by GitHub\nError Lens by Alexander"
  },
  {
    "objectID": "posts/801-abramovich-ritov/index.html",
    "href": "posts/801-abramovich-ritov/index.html",
    "title": "Quotes and Notes from Statistical Theory: A Concise Introduction (2ed)",
    "section": "",
    "text": "Get the book from Routledge (here) or Amazon (here)\nThis is a truly excellent book explaining the underlying ideas, mathematics, and principles of major statistical concepts. Its organization is suburb, and the authors’ commentary on why a theorem is so useful and how the presented ideas fit together and/or contrast is invaluable (and, quite frankly, better than I have seen anywhere else).\n\n1 Introduction\nSuppose the observed data is the sample \\(\\mathbf{y} = \\{y_1, \\ldots, y_n\\}\\) of size \\(n\\). We will model \\(\\mathbf{y}\\) as a realization of an \\(n\\)-dimensional random vector \\(\\mathbf{Y} = \\{Y_1, \\ldots, Y_n\\}\\) with a joint distribution \\(f_\\mathbf{Y}(\\mathbf{y})\\). The true distribution of the data is rarely completely known; nevertheless, it can often be reasonable to assume that it belongs to some family of distributions \\(\\mathcal{F}\\). We will assume that \\(\\mathcal{F}\\) is a parametric family, that is, that we know the type of distribution \\(f_\\mathbf{Y}(\\mathbf{y})\\) up to some unknown parameter(s) \\(\\theta \\in \\Theta\\), where \\(\\Theta\\) is a parameter space. Typically, we will consider the case where \\(y_1, \\ldots, y_n\\) are the results of independent identical experiments. In this case, \\(Y_1, \\ldots, Y_n\\) can be treated as independent, identically distributed random variables with the common distribution \\(f_\\theta(y)\\) from a parametric family of distributions \\(\\mathcal{F}_\\theta\\), \\(\\theta \\in \\Theta\\).\nDefine the likelihood function \\(L(\\theta; \\mathbf{y}) = P_\\theta(\\mathbf{y})\\) — the probability to observe the given data \\(\\mathbf{y}\\) for any possible value of \\(\\theta \\in \\Theta\\). First assume that \\(\\mathbf{Y}\\) is discrete. The value \\(L(\\theta; \\mathbf{y})\\) can be viewed as a measure of likeliness of \\(\\theta\\) to the observed data \\(\\mathbf{y}\\). If \\(L(\\theta_1; \\mathbf{y}) &gt; L(\\theta_2; \\mathbf{y})\\) for a given \\(\\mathbf{y}\\), we can say that the value \\(\\theta_1\\) for \\(\\theta\\) is more suited to the data than \\(\\theta_2\\). For a continuous random variable \\(\\mathbf{y}\\), the likelihood ratio \\(L(\\theta_1; \\mathbf{y})/L(\\theta_2; \\mathbf{y})\\) shows the strength of the evidence in favor of \\(\\theta = \\theta_1\\) vs \\(\\theta = \\theta_2\\).\nA statistic \\(T(\\mathbf{Y})\\) is any real or vector-valued function that can be computed using the data alone. A statistic \\(T(\\mathbf{Y})\\) is sufficient for an unknown parameter \\(\\theta\\) if the conditional distribution of all the data \\(\\mathbf{Y}\\) given \\(T(\\mathbf{Y})\\) does not depend on the \\(\\theta\\). In other words, given \\(T(\\mathbf{Y})\\) no other information on \\(\\theta\\) can be extracted from \\(\\mathbf{y}\\). This definition allows one to check whether a given statistic \\(T(\\mathbf{Y})\\) is sufficient for \\(\\theta\\), but it does not provide one with a constructive way to find it.\nHowever, the Fisher-Neyman Factorization Theorem says that a statistic \\(T(\\mathbf{Y})\\) is sufficient for \\(\\theta\\) iff for all \\(\\theta \\in \\Theta\\) that \\(L(\\theta, \\mathbf{y}) = g(T(\\mathbf{y}), \\theta) \\cdot h(\\mathbf{y})\\), where the function \\(g(\\cdot)\\) depends on \\(\\theta\\) and the statistic \\(T(\\mathbf{Y})\\), while \\(h(\\mathbf{y})\\) does not depend on \\(\\theta\\). In particular, if the likelihood \\(L(\\theta; \\mathbf{y})\\) depends on data only through \\(T(\\mathbf{Y})\\), then \\(T(\\mathbf{Y})\\) is a sufficient statistic for \\(\\theta\\) and \\(h(\\mathbf{y}) = 1\\).\nA sufficient statistic is not unique. For example, the entire sample \\(\\mathbf{Y} = \\{Y_1, \\ldots, Y_n\\}\\) is always a (trivial) sufficient statistic. We may seek a minimal sufficient statistic implying the maximal reduction of the data. A statistic \\(T(\\mathbf{Y})\\) is called a minimal sufficient statistic if it is a function of any other sufficient statistic.\nAnother important property of a statistic is completeness. Let \\(Y_1, \\ldots, Y_n \\sim f_\\theta(y)\\), where \\(\\theta \\in \\Theta\\). A statistic \\(T(\\mathbf{Y})\\) is complete if no statistic \\(g(\\mathbf{T})\\) exists (except \\(g(\\mathbf{T})=0\\)) such that \\(E_\\theta g(\\mathbf{T}) = 0\\) for all \\(\\theta \\in \\Theta\\). In other words, if \\(E_\\theta g(\\mathbf{T}) = 0\\) for all \\(\\theta \\in \\Theta\\), then necessarily \\(g(\\mathbf{T})=0\\). To verify completeness for a general distribution can be a nontrivial mathematical problem, but thankfully it is much simpler for the exponential family of distributions that includes many of the “common” distributions. Completeness is a useful in determining minimal sufficiency because if a sufficient statistic \\(T(\\mathbf{Y})\\) is complete, then it is also minimal sufficient. (Note, however, that a minimal sufficient statistic may not necessarily be complete.)\nA (generally multivariate) family of distributions \\({f_\\theta(\\mathbf{y}): \\theta \\in \\Theta}\\) is said to be an (one parameter) exponential family if: (1) \\(\\Theta\\) is an open interval, (2) the support of the distribution \\(f_\\theta\\) does not depend on \\(\\theta\\), and (3) \\(f_\\theta(\\mathbf{y}) = exp\\{c(\\theta)T(\\mathbf{y}) + d(\\theta) + S(\\mathbf{y})\\}\\) where \\(c(\\cdot)\\), \\(T(\\cdot)\\), \\(d(\\cdot)\\), and \\(S(\\cdot)\\) are known functions; \\(c(\\theta)\\) is usually called the natural parameter of the distribution. We say that \\(f_\\theta\\) where \\(\\theta = (\\theta_1, \\ldots \\theta_p)\\) belongs to a \\(k\\)-parameter exponential family by changing (3) such that \\(f_\\theta(\\mathbf{y}) = exp\\{ \\sum_{j=1}^k c_j(\\theta)T_j(\\mathbf{y}) + d(\\theta) + S(\\mathbf{y})\\}\\). The function \\(c(\\theta) = \\{c_1(\\theta), \\ldots, c_k(\\theta)\\}\\) are the natural parameters of the distribution. (Note that the dimensionality \\(p\\) of the original parameter \\(\\theta\\) is not necessarily the same as the dimensionality \\(k\\) of the natural parameter \\(c(\\theta)\\).)\nConsider a random sample \\(Y_1, \\ldots, Y_n\\), where \\(Y_i \\sim f_\\theta(y)\\) and \\(f_\\theta\\) belongs to a \\(k\\)-parameter exponential family of distributions, then (1) the joint distribution of \\(\\mathbf{Y} = (Y_1, \\ldots, Y_n)\\) also belongs to the \\(k\\)-parameter exponential family, (2) \\(T_\\mathbf{Y} = (\\sum_{i=1}^n T_1(Y_i), \\ldots, \\sum_{i=1}^n T_k(Y_i))\\) is the sufficient statistic for \\(c(\\theta) = (c_1(\\theta), \\ldots, c_k(\\theta))\\) (and, therefore, for \\(\\theta\\)), and (3) if some regularity conditions hold, then \\(T_\\mathbf{Y}\\) is complete and therefore minimal sufficient (if the latter exists).\n\n\n2 Point Estimation\nEstimation of the unknown parameters of distributions from the data is one of the key issues in statistics. A (point) estimator \\(\\hat{\\theta} = \\hat{\\theta}(\\mathbf{Y})\\) of an unknown parameter \\(\\theta\\) is any statistic used for estimating \\(\\theta\\). The value of \\(\\hat{\\theta}(\\mathbf{y})\\) evaluated for a given sample is called an estimate. This is a general, somewhat trivial definition that does not say anything about the goodness of estimation; one would evidently be interested in “good” estimators.\nMaximum Likelihood Estimation is the most used method of estimation of parameters in parametric models. As we’ve discussed, \\(L(\\theta; \\mathbf{y})\\) is the measure of likeliness of a parameter’s values \\(\\theta\\) for the observed data \\(\\mathbf{y}\\). It is only natural then to seek the “most likely” value of \\(\\theta\\). The MLE \\(\\hat{\\theta}\\) of \\(\\theta\\) is \\(\\hat{\\theta} = \\arg\\max_{\\theta\\in\\Theta} L(\\theta; \\mathbf{y})\\) — the value of \\(\\theta\\) that maximizes the likelihood. Often we are interested in a function of a parameter \\(\\xi = g(\\theta)\\). When \\(g(\\cdot)\\) is 1:1, the MLE of \\(\\xi\\) is the function \\(g(\\cdot)\\) applied to the MLE of \\(\\theta\\): \\(\\hat{\\xi} = g(\\hat{\\theta})\\), the proof of which is just a reparameterization of the likelihood in terms of \\(\\xi\\) instead of \\(\\theta\\). Although the conception of the MLE was motivated by an intuitively clear underlying idea, the justification for its use is much deeper. It is a really “good” method of estimation (to be discussed later on the topic of asymptotics).\nAnother popular method of estimation is the Method of Moments. Its main idea is based on expressing the population moments of the distribution of data in terms of its unknown parameter(s) and equating them to their corresponding sample moments. MMEs have some known problems: consider a sample of size 4 from a uniform distribution \\(U(0, \\theta)\\) with the observed sample 0.2, 0.6, 2, and 0.4. The MME is 1.6, which does not make much sense given the observed value of 2 in the sample. For these and related reasons, the MMEs are less used than the MLE counterparts. However, MMEs are usually simpler to compute and can be used, for example, as reasonable initial values in numerical iterative procedures for MLEs. On the other hand, the Method of Moments does not require knowledge of the entire distribution of the data (up to the unknown parameters) but only its moments and thus may be less sensitive to possible misspecification of a model.\nThe Method of Least Squares play a key role in regression and analysis of variance. In a typical regression setup, we are given \\(n\\) observations \\((\\mathbf{x}_i, y_i)\\), \\(i=1, \\ldots, n\\) over \\(m\\) explanatory variables \\(\\mathbf{x} = (x_1, \\ldots, x_m)\\) and the response variable \\(\\mathbf{Y}\\). We assume that \\(y_i = g_\\theta(\\mathbf{x}_i) + \\varepsilon_i\\), \\(i = 1, \\ldots, n\\) where the response function \\(g_\\theta(\\cdot): \\mathbb{R}^m \\rightarrow \\mathbb{R}\\) has a known parametric form and depends on \\(p \\le n\\) unknown parameters \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\). The LSE looks for a \\(\\hat{\\theta}\\) that yields the best fit \\(g_\\hat{\\theta}(\\mathbf{x})\\) to the observed \\(\\mathbf{y}\\) w.r.t. the Euclidean distance: \\(\\hat{\\theta} = \\arg\\min_\\theta \\sum_{i=1}^n (y_i - g_\\theta(\\mathbf{x}_i))^2\\). For linear regression, the solution is available in closed form. For non-linear regression, however, it can generally be only found numerically.\nMore generally than the three above procedures, one can consider any function \\(\\rho(\\theta, y)\\) as a measure of the goodness-of-fit and look for an estimator that maximizes or minimizes \\(\\sum_{i=1}^n \\rho(\\theta, y_i)\\) w.r.t. \\(\\theta\\). Such estimators are called M-estimators. It runs out that various well-known estimators can be viewed as M-estimators for a particular \\(\\rho(\\theta, y)\\) including \\(\\bar{Y}\\) for the sample mean as well as MLE, LSE, and a generalized version of MME not yet discussed. As we’ll see when discussing asymptotics, M-estimators share many important asymptotic properties.\nA natural question is how to compare between various estimators. First, we should define a measure of goodness-of-estimation. Recall that any estimator \\(\\hat{\\theta} = \\hat{\\theta}(Y_1, \\ldots, Y_n)\\) is a function of a random sample and therefore is a random variable itself with a certain distribution, expectation, variance, etc. A somewhat naive attempt to measure the goodness-of-estimation of \\(\\hat{\\theta}\\) would be to consider the error \\(|\\hat{\\theta}-\\theta|\\). However, \\(\\theta\\) is unknown and, as we have mentioned, an estimator \\(\\hat{\\theta}\\) is a random variable and hence the value \\(|\\hat{\\theta}-\\theta|\\) will vary from sample to sample. It may be “small” for some of the samples, while “large” for others and therefore cannot be used as a proper criterion for goodness-of-estimation of an estimator \\(\\hat{\\theta}\\). A more reasonable measure would then be an average distance over all possible samples, that is, the mean absolute error \\(E|\\hat{\\theta}-\\theta|\\), where the expectation is taken w.r.t. the joint distribution of \\(\\mathbf{Y} = (Y_1, \\ldots, Y_n)\\). It indeed can be used as a measure of goodness-of-estimation but usually, mostly due to convenience of differentiation, the conventional measure is the mean squared error (MSE) given by \\(MSE(\\hat{\\theta}, \\theta) = E(\\hat{\\theta}-\\theta)^2\\).\nThe MSE can be decomposed into two components: \\(MSE(\\hat{\\theta}, \\theta) = Var(\\hat{\\theta}) + b^2(\\hat{\\theta},\\theta)\\). The first is the stochastic error (variance) and the second is a systematic or deterministic error (bias). Having defined the goodness-of-estimation measure by MSE, one can compare different estimators and choose the one with the smallest MSE. However, since the \\(MSE(\\hat{\\theta}, \\theta)\\) typically depends on the unknown \\(\\theta\\), it is a common situation where no estimator is uniformly superior for all \\(\\theta \\in \\Theta\\).\nIdeally, a good estimator with a small MSE should have both low variance and low bias. However, it might be hard to have both. One of the common approaches is to first control the bias component of the overall MSE and to consider unbiased estimators. There is no general rule or algorithm for deriving an unbiased estimator. In fact, unbiasedness is a property of an estimator rather than a method of estimation. One usually checks an MLE or any other estimator for bias. Sometimes one can then modify the original estimator to “correct” its bias. Note that unlike ML estimation, unbiasedness is not invariant under nonlinear transformation of the original parameter: if \\(\\hat{\\theta}\\) is an unbiased estimator of \\(\\theta\\), \\(g(\\hat{\\theta})\\) is generally a biased estimator for \\(g(\\theta)\\).\nWhat does unbiasedness of an estimator \\(\\hat{\\theta}\\) actually mean? Suppose we were observing not a single sample but all possible samples of size \\(n\\) from a sample space and were calculating the estimates \\(\\hat{\\theta}_j\\) for each one of them. The unbiasedness means that the average value of \\(\\hat{\\theta}\\) over the entire sample space is \\(\\theta\\), but it does not guarantee yet that \\(\\hat{\\theta}_j \\approx \\theta\\) for each particular sample. The dispersion of \\(\\hat{\\theta}_j\\)’s around their average value \\(\\theta\\) might be large and, since in reality we have only a single sample, its particular value of \\(\\hat{\\theta}\\) might be quite away from \\(\\theta\\). To ensure with high confidence that \\(\\hat{\\theta}_j \\approx \\theta\\) for any sample we need in addition for the variance \\(Var(\\hat{\\theta})\\) to be small.\nAn estimator \\(\\hat{\\theta}\\) is called a uniformly minimum variance unbiased estimator (UMVUE) of \\(\\theta\\) if \\(\\hat{\\theta}\\) is unbiased and for any other unbiased estimator \\(\\tilde{\\theta}\\) of \\(\\theta\\), \\(Var(\\hat{\\theta}) \\le Var(\\tilde{\\theta})\\). If the UMVUE exists, it is necessarily unique. Recall that there is no general algorithm to obtain unbiased estimators in general and a UMVUE in particular. However, there exists a lower bound for a variance of an unbiased estimator, which can be used as a benchmark for evaluating its goodness.\nDefine the Fisher Information Number \\(I(\\theta) = E((\\ln f_\\theta(\\mathbf{y}))'_\\theta)^2\\). The derivative of the log density is sometimes called the Score Function. Thus, the Fisher Information Number is the expected square of the Score. The Cramer-Rao Lower Bound Theorem states that if \\(T\\) is an unbiased estimator for \\(g(\\theta)\\), where \\(g(\\cdot)\\) is differentiable, then \\(Var(T) \\ge (g'(\\theta))^2 / I(\\theta)\\) or more simply, when \\(T\\) is an unbiased estimator for \\(\\theta\\), \\(Var(T) \\ge 1 / I(\\theta)\\). We are especially interested in the case where \\(Y_1, \\ldots, Y_n\\) is a random sample from a distribution \\(f_\\theta(y)\\). In that case, \\(I(\\theta) = nI^*(\\theta)\\) where \\(I^*(\\theta) = E((\\ln f_\\theta(y))'_\\theta)^2\\) is the Fisher Information Number of \\(f_\\theta(y)\\), and, therefore, for any unbiased estimator \\(T\\) of \\(g(\\theta)\\), we have that \\(Var(T) \\ge (g'_\\theta(\\theta))^2 / nI^*(\\theta)\\). There is another, usually more convenient formula for calculating the Fisher Information Number \\(I(\\theta)\\) other than its direct definition: \\(I(\\theta) = -E(\\ln f_\\theta(\\mathbf{Y}))''_\\theta\\).\nIt is important to emphasize that the CRLB theorem is one direction only: if the variance of an unbiased estimator does not achieve the Cramer-Rao lower bound, one still cannot claim that it is not an UMVUE. Nevertheless, it can be used as a benchmark for measuring the goodness of an unbiased estimator. One special result related to the exponential family distribution: the Cramer-Rao lower bound is achieved only for distributions from the exponential family.\nThe Cramer-Rao lower bound allows one only to evaluate the goodness of a proposed unbiased estimator but does not provide any constructive way to derive it. In fact, as we have argued, there is no such general rule at all. However, if one manages to obtain any initial (even crude) unbiased estimator, it may be possible to improve it. The Rao-Blackwell Theorem shows that if there is an unbiased estimator that is not a function of a sufficient statistic \\(W\\), one can construct another unbiased estimator based on \\(W\\) with an MSE not larger than the original one: let \\(T\\) be an unbiased estimator of \\(\\theta\\) and \\(W\\) be a sufficient statistic for \\(\\theta\\), and define \\(T_1 = E(T|W)\\), then \\(T_1\\) is an unbiased estimator of \\(\\theta\\) and \\(Var(T_1) \\le Var(T)\\). Thus, in terms of MSE, only unbiased estimators based on a sufficient statistic are of interest. This demonstrates again a strong sense of the notion of sufficiency.\nDoes Rao-Blackwellization necessarily yield an UMVUE? Generally not. To guarantee UMVUE an additional requirement of completeness on a sufficient statistic \\(W\\) is needed. The Lehmann-Scheffe Theorem formalizes this: if \\(T\\) is an unbiased estimator of \\(\\theta\\) and \\(W\\) is a complete sufficient statistic for \\(\\theta\\), then \\(T_1 = E(T|W)\\) is the unique UMVUE of \\(\\theta\\). Even without the Lehmann-Scheffe theorem, it can be shown under mild conditions that if the distribution of the data belongs to the exponential family and an unbiased estimator is a function of the corresponding sufficient statistic, it is an UMVUE. Note that despite its elegance, the application of the Rao-Blackwell Theorem in more complicated cases is quite limited. The two main obstacles are in finding an initial unbiased estimator \\(T\\) and calculating the conditional expectation \\(E(T|W)\\)\n\n\n3 Confidence Intervals, Bounds, and Regions\nWhen we estimate a parameter we essentially “guess” its value. This should be a “well educated guess,” hopefully the best of its kind (in whatever sense). However, statistical estimation is made with error. Presenting just the estimator, no matter how good it is, is usually not enough – one should give an idea about the estimation error. The words “estimation” and “estimate” (in contrast to “measure” and “value”) point to the fact that an estimate is an inexact appraisal of a value. Great efforts of statistics are invested in trying to quantify the estimation error and find ways to express it in a well-defined way.\nAny estimator has error, that is, if \\(\\theta\\) is estimated by \\(\\hat{\\theta}\\), then \\(\\hat{\\theta} - \\theta\\) is usually different from \\(0\\). The standard measure of error is the mean squared error \\(MSE = E(\\hat{\\theta} - \\theta)^2\\). When together with the value of the estimator we are given its MSE, we get a feeling of how precise the estimate is. It is more common to quote the standard error defined by \\(SE(\\hat{\\theta}) = \\sqrt{MSE(\\hat{\\theta}, \\theta)} = \\sqrt{E(\\hat{\\theta}-\\theta)^2}\\). Unlike the MSE, the SE is measured in the same units as the estimator.\nHowever, the MSE expresses an average squared estimation error but tells nothing about its distribution, which generally might be complicated. One would be interested, in particular, in the probability \\(P(|\\hat{\\theta}-\\theta| &gt; c)\\) that the estimation error exceeds a certain accuracy level \\(c &gt; 0\\). Markov’s Inequality enables us to translate the MSE into an upper bound on this probability: \\(P(|\\hat{\\theta}-\\theta| \\ge c) \\le MSE/c^2\\). However, this bound is usually very conservative and the actual probability may be much smaller. Typically, a quite close approximation of the error distribution is obtained by the Central Limit Theorem (discussed later).\nAbove, we suggested quoting the SE besides the estimator itself. However, standard statistical practice is different and it is not very intuitive. There are a few conceptual difficulties in being precise when talking about the error. Suppose we estimate \\(\\mu\\) with \\(\\bar{Y}\\) and we find \\(\\bar{Y}=10\\) and \\(SE=0.2\\). We are quite confident that \\(\\mu\\) is between \\(9.6\\) and \\(10.4\\). However, this statement makes no probabilistic sense. The true \\(\\mu\\) is either in the interval \\((9.6,10.4)\\) or it’s not. The unknown \\(\\mu\\) is not a random variable – it has a single fixed value, whether or not it is known to the statistician.\nThe “trick” that has been devised is to move from a probabilistic statement about the unknown (but with a fixed value) parameter to a probabilistic statement about the method. We say something like: “The interval \\((9.6,10.4)\\) for the value \\(\\mu\\) was constructed by a method with is 95% successful.” Since the method is usually successful, we have confidence in its output.\nLet \\(Y_1, \\ldots, Y_n \\sim f_\\theta(y)\\), \\(\\theta \\in \\Theta\\). A \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\theta\\) is the pair of scalar-valued statistics \\(L=L(Y_1, \\ldots, Y_n)\\) and \\(U=U(Y_1, \\ldots, Y_n)\\) such that \\(P(L \\le \\theta \\le U) \\ge 1-\\alpha\\) for all \\(\\theta \\in \\Theta\\) with the inequality as closs to an equality as possible.\nWhat is the right value of \\(\\alpha\\)? Nothing in the statistical theory dictates a particular choice. But the standard values are \\(0.10\\), \\(0.05\\), and \\(0.01\\) with \\(0.05\\) being most common.\nA pivot is a function \\(\\psi(Y_1, \\ldots, Y_n; \\theta)\\) of the data and the parameters, whose distribution does not depend on unknown parameters. Note, the pivot is not a statistic and cnanot be calculated form the data, exactly because it depends on the unknown parameters. On the other hand, since its distribution does not depend on the unknown parameters, we can find an interval \\(A_\\alpha\\) such that \\(P(\\psi(Y_1, \\ldots, Y_n; \\theta) \\in A_\\alpha) = 1 - \\alpha\\). Then we can invert the inclusion and define the interval (or region in general) \\(C_\\alpha = \\{\\theta : \\psi(Y_1, \\ldots, Y_n; \\theta) \\in A_\\alpha \\}\\). The set \\(C_\\alpha\\) is then a \\((1-\\alpha)100\\%\\) confidence set. Note that \\(C_\\alpha\\) is a random set because it depends on the random sample.\nDoes a pivot always exist? For a random sample \\(Y_1, \\ldots, Y_n\\) from any continuous distribution with a cdf \\(F_\\theta(y)\\), the value \\(-\\sum_{i=1}^n \\ln F_\\theta(Y_i) \\sim \\frac{1}{2} \\chi^2_{2n}\\) and, therefore, is a pivot. However, in the general case, it might be difficult (if possible) to invert the corresponding confidence interval for this pivot to a confidence interval for the original parameter of interest \\(\\theta\\). More convenient pivots are easy to find when the distribution belongs to a scale-location family.\nIs a confidence interval for \\(\\theta\\) necessarily unique? Definitely not! Different choices of pivots lead to different forms of confidence intervals. Moreover, even for a given pivot, one can typically construct an infinite set of confidence intervals at the same confidence level. What is the “best” choice for a confidence interval? A conventional, somewhat ad hoc approach is based on error symmetry: set \\(P(L&gt;\\theta) = P(U&lt;\\theta) = \\alpha/2\\). A more appealing approach would be to seek a confidence interval of a minimal expected length; however, in general, this leads to a nonlinear minimization problem that might not have a solution in closed form.\nA parameter of interest may be not the original parameter \\(\\theta\\) but its function \\(g(\\theta)\\). Let \\((L,U)\\) be a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\theta\\) and \\(g(\\cdot)\\) a strictly increasing function. Then \\((g(L),g(U))\\) is a \\((1-\\alpha)100\\%\\) confidence interval for \\(g(\\theta)\\).\nThe normal confidence intervals are undoubtedly the most important. We do not claim that most data sets are sampled from a normal distribution – this is definitely far from being true. However, what really matters is whether the distribution of an estimator \\(\\hat{\\theta} = \\hat{\\theta}(Y_1, \\ldots, Y_n)\\) is close to normal rather than the distribution of \\(Y_1, \\ldots, Y_n\\) themselves. When discussing asymptotics later, we argue that many estimators based on large or even medium sized samples are indeed approximately normal and, therefore, the normal confidence intervals can (at least approximately) be used.\n\n\n4 Hypothesis Testing\nTo be added\n\n\n5 Asymptotic Analysis\nTo be added\n\n\n6 Bayesian Inference\nTo be added\n\n\n7 Elements of Statistical Decision Theory\nTo be added\n\n\n8 Linear Models\nTo be added\n\n\n9 Nonparametric Estimation\nTo be added"
  },
  {
    "objectID": "pubs/index.html#co-author",
    "href": "pubs/index.html#co-author",
    "title": "Dan Yavorsky",
    "section": "Co-Author",
    "text": "Co-Author\n\nConsumer Search in the U.S. Auto Industry: The Role of Dealership Visits\nDan Yavorsky, Elisabeth Honka, and Keith Chen\nQuantitative Marketing and Economics (2021)"
  },
  {
    "objectID": "pubs/index.html#contributor",
    "href": "pubs/index.html#contributor",
    "title": "Dan Yavorsky",
    "section": "Contributor",
    "text": "Contributor\n\nThe Sequential Search Model: A Framework for Empirical Research\nRaluca Ursu, Stephan Seiler, and Elisabeth Honka\nWorking Paper (2023)\n\n\nThe Value of Flexible Work: Evidence from Uber Drivers\nKeith Chen, Judy Chevalier, Peter Rossi, and Emily Oehlsen\nJournal of Political Economy (2019)\n\n\nggplot2: Elegant Graphics for Data Analysis\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen\nSpringer\n\n\nR Packages: Organize, Test, Document, and Share Your Code\nHadley Wickham and Jennifer Bryan\nSpringer"
  },
  {
    "objectID": "pubs/index.html#short-form",
    "href": "pubs/index.html#short-form",
    "title": "Dan Yavorsky",
    "section": "Short Form",
    "text": "Short Form\n\nGBK Employee Spotlight\nGBK Thought Leadership Blog\n\n\nSawtooth A&I Summit 2023\nGBK Thought Leadership Blog"
  }
]