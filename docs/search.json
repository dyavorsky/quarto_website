[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Resume",
    "section": "",
    "text": "Download Resume"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Econometrics \n                \n            \n            \n                MFE 402 | \n                UCLA\n                \n            \n            A broad introduction to fundamental econometric models, methods of estimation, and approaches to statistical inference. Implementation in R.\n\n            \n                \n                \n                Syllabus\n                \n                \n                \n                Slides\n                \n                \n                \n                Evals 2025\n                \n                \n                \n                Evals 2024.1\n                \n                \n                \n                Evals 2024.2\n                \n                \n                \n                Evals 2023.1\n                \n                \n                \n                Evals 2023.2\n                \n                \n                \n                Evals 2022.1\n                \n                \n                \n                Evals 2022.2\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                R Workshop \n                \n            \n            \n                MFE | \n                UCLA\n                \n            \n            A 3-session introduction to the R software and environment, emphasizing the fundamental data objects and the particularities of the language before branching into topics most relevant for data analysis and statistical computing.\n\n            \n                \n                \n                Website\n                \n                \n                \n                Code\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Marketing Strategy and Policy \n                \n            \n            \n                EMBA 411 | \n                UCLA\n                \n            \n            A solid foundation for strategic marketing thinking; an introduction to a series of frameworks and tools that may be used to solve marketing problems.\n\n            \n                \n                \n                Syllabus\n                \n                \n                \n                Evals 2022.1\n                \n                \n                \n                Evals 2022.2\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Marketing Analytics \n                \n            \n            \n                MGTA 495 | \n                UCSD\n                \n            \n            A survey of analytic methods -- and a review of the statistical methods required -- to analyze survey data for consumer insights and to inform marketing strategy: A/B testing, Best-Worst Scaling, Conjoint Analysis, Segmentation, Key Drivers Analysis.\n\n            \n                \n                \n                Syllabus\n                \n                \n                \n                Slides\n                \n                \n                \n                Evals 2025\n                \n                \n                \n                Evals 2024\n                \n                \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#graduate",
    "href": "teaching/index.html#graduate",
    "title": "Teaching",
    "section": "",
    "text": "Econometrics \n                \n            \n            \n                MFE 402 | \n                UCLA\n                \n            \n            A broad introduction to fundamental econometric models, methods of estimation, and approaches to statistical inference. Implementation in R.\n\n            \n                \n                \n                Syllabus\n                \n                \n                \n                Slides\n                \n                \n                \n                Evals 2025\n                \n                \n                \n                Evals 2024.1\n                \n                \n                \n                Evals 2024.2\n                \n                \n                \n                Evals 2023.1\n                \n                \n                \n                Evals 2023.2\n                \n                \n                \n                Evals 2022.1\n                \n                \n                \n                Evals 2022.2\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                R Workshop \n                \n            \n            \n                MFE | \n                UCLA\n                \n            \n            A 3-session introduction to the R software and environment, emphasizing the fundamental data objects and the particularities of the language before branching into topics most relevant for data analysis and statistical computing.\n\n            \n                \n                \n                Website\n                \n                \n                \n                Code\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Marketing Strategy and Policy \n                \n            \n            \n                EMBA 411 | \n                UCLA\n                \n            \n            A solid foundation for strategic marketing thinking; an introduction to a series of frameworks and tools that may be used to solve marketing problems.\n\n            \n                \n                \n                Syllabus\n                \n                \n                \n                Evals 2022.1\n                \n                \n                \n                Evals 2022.2\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Marketing Analytics \n                \n            \n            \n                MGTA 495 | \n                UCSD\n                \n            \n            A survey of analytic methods -- and a review of the statistical methods required -- to analyze survey data for consumer insights and to inform marketing strategy: A/B testing, Best-Worst Scaling, Conjoint Analysis, Segmentation, Key Drivers Analysis.\n\n            \n                \n                \n                Syllabus\n                \n                \n                \n                Slides\n                \n                \n                \n                Evals 2025\n                \n                \n                \n                Evals 2024\n                \n                \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#undergraduate",
    "href": "teaching/index.html#undergraduate",
    "title": "Teaching",
    "section": "Undergraduate",
    "text": "Undergraduate\n\n\n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Customer Analytics \n                \n            \n            \n                MGT 100 | \n                UCSD\n                \n            \n            Combine customer data, economic theory, and statistical modeling to improve business decision-making related to segmentation, demand estimation, pricing, branding, and customer aquisition and retention. Implementation in R. Co-created with Prof. Ken Wilbur.\n\n            \n                \n                \n                Syllabus\n                \n                \n                \n                Slides F24\n                \n                \n                \n                Recordings SS24\n                \n                \n                \n                Evals 2025.4\n                \n                \n                \n                Evals 2025.3\n                \n                \n                \n                Evals 2025.1\n                \n                \n                \n                Evals 2025.2\n                \n                \n                \n                Evals 2024.1\n                \n                \n                \n                Evals 2024.2\n                \n                \n                \n                Evals 2024.3\n                \n                \n                \n                Evals 2023.1\n                \n                \n                \n                Evals 2023.2\n                \n                \n                \n                Evals 2022\n                \n                \n                \n                KW Version\n                \n                \n            \n        \n    \n    \n    \n        \n            \n            \n            \n        \n        \n            \n                \n                Technology for Business Analytics \n                \n            \n            \n                MGT 153 | \n                UCSD\n                \n            \n            Develop proficiency with popular software to execute common analytic techniques and present graphical summaries of data: Excel, Tableau, SQL, R, Python.\n\n            \n                \n                \n                Syllabus\n                \n                \n                \n                Evals 2023\n                \n                \n            \n        \n    \n    \n\nNo matching items"
  },
  {
    "objectID": "links/stats.html",
    "href": "links/stats.html",
    "title": "Links",
    "section": "",
    "text": "Statistical Rethinking 2 with rstan and the tidyverse by Solomon Kurz\nTeaching Bayesian Statistics by TALMO\nA Collection of Econometrics Resources by Giuseepe Cavaliere\nApplied Machine Learning for Tabular Data by Max Kuhn and Kjell Johnson\nStatistical Tools for Causal Inference by Sylvain Chabe-Ferret\nCausal Inference: The Mixtape by Scott Cunningham\nApplied Causal Inference Powered by ML and AI by Chernozhukov, Hansen, Kallus, Spindler, and Syrgkanis\nCausal Machine Learning by Chernozhukov, Hansen, Kallus, Spindler, and Syrgkanis\nThe Data Science Handbooks: Advice and Insights from 25 Amazing Data Scientists by Shan, Wang, Chen, and Song\nTelling Stories with Data by Rohan Alexxander\n\n\n\n\n\nHierarchical Modeling by Michael Betancourt\nKeep Calm and Learn Multilevel Logistic Modeling by Nicolas Sommet and Davide Morselli\nMath-Stat Course Notes by Joshua Tebbs\nNotes by Adam N Smith\nEconometrics Notes by Tim Armstrong\nMachine Learning by Aric LaBarr\n\n\n\n\n\nCaret Package Practical Guide from machinelearningplus.com"
  },
  {
    "objectID": "links/stats.html#stats-and-related-fields",
    "href": "links/stats.html#stats-and-related-fields",
    "title": "Links",
    "section": "",
    "text": "Statistical Rethinking 2 with rstan and the tidyverse by Solomon Kurz\nTeaching Bayesian Statistics by TALMO\nA Collection of Econometrics Resources by Giuseepe Cavaliere\nApplied Machine Learning for Tabular Data by Max Kuhn and Kjell Johnson\nStatistical Tools for Causal Inference by Sylvain Chabe-Ferret\nCausal Inference: The Mixtape by Scott Cunningham\nApplied Causal Inference Powered by ML and AI by Chernozhukov, Hansen, Kallus, Spindler, and Syrgkanis\nCausal Machine Learning by Chernozhukov, Hansen, Kallus, Spindler, and Syrgkanis\nThe Data Science Handbooks: Advice and Insights from 25 Amazing Data Scientists by Shan, Wang, Chen, and Song\nTelling Stories with Data by Rohan Alexxander\n\n\n\n\n\nHierarchical Modeling by Michael Betancourt\nKeep Calm and Learn Multilevel Logistic Modeling by Nicolas Sommet and Davide Morselli\nMath-Stat Course Notes by Joshua Tebbs\nNotes by Adam N Smith\nEconometrics Notes by Tim Armstrong\nMachine Learning by Aric LaBarr\n\n\n\n\n\nCaret Package Practical Guide from machinelearningplus.com"
  },
  {
    "objectID": "links/blogs.html",
    "href": "links/blogs.html",
    "title": "Links",
    "section": "",
    "text": "Andrew Gelman\nRichard McElreath\nCosma Shalizi\n\n\n\n\n\nInstaCart\nNetflix\nAirbnb\nAmazon Science\n\n\n\n\n\nJoel Cadwell (inactive)\nChris Chapman’s QuantUX\nJohn Cook\nMarc and Jeff Dotson\nAndrew Heiss\nStephan Seiler’s Annual Best-of]"
  },
  {
    "objectID": "links/blogs.html#blogs",
    "href": "links/blogs.html#blogs",
    "title": "Links",
    "section": "",
    "text": "Andrew Gelman\nRichard McElreath\nCosma Shalizi\n\n\n\n\n\nInstaCart\nNetflix\nAirbnb\nAmazon Science\n\n\n\n\n\nJoel Cadwell (inactive)\nChris Chapman’s QuantUX\nJohn Cook\nMarc and Jeff Dotson\nAndrew Heiss\nStephan Seiler’s Annual Best-of]"
  },
  {
    "objectID": "links/advice.html",
    "href": "links/advice.html",
    "title": "Links",
    "section": "",
    "text": "This is Water by David Foster Wallace\nA Mathematician’s Lament by Paul Lockhart\nMaker’s Schedule, Manager’s Schedule by Paul Graham\nLessons from Amazon (alt link) by David Anderson\n\n\n\n\n\nHow to Send and Reply to email by Matt Might\nPublication, Publication by Gary King\n\n\n\n\n\nTeaching Smart People How to Learn by Chris Argyris\nWhy Books Don’t Work by Andy Matuschak\nEvergreen Notes by Andy Matuschak\n\n\n\n\n\nThe Slight Edge by Jeff Olsen video\n8 Steps to UnF Your Life by Joey Schweitzer\nWhy Most People Are Only Giving 70% by John Amaechi\n\n\n\n\nKen Wilbur: Reasons to Get (or Not) a PhD and Choosing a field, Subfield, and PhD Program\nSusan Athey: Professional Advice\nJesse Shapiro: Notes on Applying for a PhD in Economics\nAnatoli Colicev: The PhD Journey\nMartin Schwartz: The Importance of Stupidity in Scientific Research"
  },
  {
    "objectID": "links/advice.html#advice",
    "href": "links/advice.html#advice",
    "title": "Links",
    "section": "",
    "text": "This is Water by David Foster Wallace\nA Mathematician’s Lament by Paul Lockhart\nMaker’s Schedule, Manager’s Schedule by Paul Graham\nLessons from Amazon (alt link) by David Anderson\n\n\n\n\n\nHow to Send and Reply to email by Matt Might\nPublication, Publication by Gary King\n\n\n\n\n\nTeaching Smart People How to Learn by Chris Argyris\nWhy Books Don’t Work by Andy Matuschak\nEvergreen Notes by Andy Matuschak\n\n\n\n\n\nThe Slight Edge by Jeff Olsen video\n8 Steps to UnF Your Life by Joey Schweitzer\nWhy Most People Are Only Giving 70% by John Amaechi\n\n\n\n\nKen Wilbur: Reasons to Get (or Not) a PhD and Choosing a field, Subfield, and PhD Program\nSusan Athey: Professional Advice\nJesse Shapiro: Notes on Applying for a PhD in Economics\nAnatoli Colicev: The PhD Journey\nMartin Schwartz: The Importance of Stupidity in Scientific Research"
  },
  {
    "objectID": "posts/020-quarto-websites/index.html",
    "href": "posts/020-quarto-websites/index.html",
    "title": "Build a Quarto Website",
    "section": "",
    "text": "Updated August 2024\nQuarto is an open-source scientific and technical publishing system. It enables you to write plain text (.qmd) source files and render them into consumable (e.g., html, pdf) output files. This tutorial walks you through the steps to create a website with Quarto. This information is also available on the excellent Quarto website and in several other tutorials. A subsequent post (this one) demonstrates how to freely host your website on GitHub. Additionally, in December 2024, Posit posted a four-part video series detailing elements of Quarto websites on YouTube here.\nOne main purpose for creating a website with Quarto is the ability to add “blog” posts written as Quarto documents. These documents can contain code (e.g., R, Python, C++, Julia, other) which – as part of the rendering process – will run the code and embed the code’s output (e.g., numeric results or plots) in the rendered html output file. As a result, a publication-ready blog post can be written as one Quarto source file and no copy/paste is necessary to include code or the code’s resulting numeric or graphic output.\nSo, let’s go make that website."
  },
  {
    "objectID": "posts/020-quarto-websites/index.html#step-1-get-quarto",
    "href": "posts/020-quarto-websites/index.html#step-1-get-quarto",
    "title": "Build a Quarto Website",
    "section": "Step 1: Get Quarto",
    "text": "Step 1: Get Quarto\nQuarto comes bundled with RStudio (v2022.07.1 and later). Therefore, if you are using RStudio, you should already have Quarto installed. You can check your version of RStudio by going to RStudio’s Help menu and selecting “About RStudio.”\nIf you prefer VS Code, the command line, or another IDE, you can download and install Quarto from quarto.org. VS Code users should also install the Quarto VS Code Extension.\nYou can confirm that Quarto is installed by running quarto --version at the command line in the Terminal (on Mac or Linux) or in the Git-Bash terminal on Windows – note that this is not the R console. If a version number is returned, you’re good to go.\n\n\nTerminal\n\nquarto --version"
  },
  {
    "objectID": "posts/020-quarto-websites/index.html#step-2-create-a-website-project",
    "href": "posts/020-quarto-websites/index.html#step-2-create-a-website-project",
    "title": "Build a Quarto Website",
    "section": "Step 2: Create a Website Project",
    "text": "Step 2: Create a Website Project\nIn RStudio, create a new project by selecting “New Project” from the File menu or by using the dropdown at the top-right of RStudio. Choose “New Directory” and then “Quarto Website.” Enter a name for the directory that will hold the website files (I use quarto_website in this tutorial) and where you’ll store that directory on your computer. Unless you know what you’re doing, I recommend for now that you uncheck the Git, renv, and Visual Editor options –- you can enable those later if you wish. Finally, click “Create Project”.\n\n\n\n\n\n\nNoteScreenshots\n\n\n\n\n\n\nNew ProjectNew DirectoryQuarto WebsiteSelect Directory\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn VS Code, search the Command Pallet (Ctrl+Shift+PCtrl+Shift+P on Windows or Cmd+Shift+PCmd+Shift+P on Mac) for “Quarto: Create Project”. Select “Website Project.” Then select the directory that will hold the website files and enter a name for the project directory.\n\n\n\n\n\n\nNoteScreenshots\n\n\n\n\n\n\nNew ProjectWebsite ProjectSelect Directory\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you’re not using RStudio or VS Code, you can simply create the text files that are discussed next."
  },
  {
    "objectID": "posts/020-quarto-websites/index.html#step-3-inspect-the-template",
    "href": "posts/020-quarto-websites/index.html#step-3-inspect-the-template",
    "title": "Build a Quarto Website",
    "section": "Step 3: Inspect the Template",
    "text": "Step 3: Inspect the Template\nThe template directory contains two important files: a _quarto.yml configuration file, and the index.qmd Quarto file with the contents of the landing page.\nThere is also a placeholder for a second website page (the about.qmd Quarto file) and an empty styles.css file. Depending on your IDE, you may also have RStudio-related or VS Code-related files or folders.\n\n\n\n\n\n\nNoteScreenshots\n\n\n\n\n\n\nRStudioVS Code\n\n\n\n\n\n\n\n\n\n\n\n\nThe _quarto.yml configuration file specifies (1) that the rendered documents should combine to form a website, (2) contents of the navigation bar that enables users to move around your website, and (3) the theme and style used to visually enhance the website.\n\n\n_quarto.yml\n\n1project:\n  type: website\n\n2website:\n  title: \"quarto_website\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - href: about.qmd\n\n3format:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\n\n\n1\n\nIdentify the project as a website.\n\n2\n\nSpecify links in the Navigation Bar\n\n3\n\nControl the website’s visual theme/style\n\n\nThe index.qmd file contains a bare minimum template with the document properties controlled by the YAML header sandwiched between triple dashes at the top, followed by the document content below.\n\n\nindex.qmd\n\n1---\ntitle: \"quarto_website\"\n---\n\n2This is a Quarto website.\n\n\n1\n\nYAML header\n\n2\n\nDocument content"
  },
  {
    "objectID": "posts/020-quarto-websites/index.html#step-4-prepare-for-github",
    "href": "posts/020-quarto-websites/index.html#step-4-prepare-for-github",
    "title": "Build a Quarto Website",
    "section": "Step 4: Prepare for GitHub",
    "text": "Step 4: Prepare for GitHub\nLater, we will host our website on GitHub. It can be helpful to set two properties now.\nFirst, change the name of the output directory that will hold the rendered html files to “docs” by adding line 3 below to the _quarto.yml configuration file:\n\n\n_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs  # &lt;-- add this\n\nNext, add an empty text file titled “.nojekyll” in the project directory. For example, at the command line you could run:\n\n\nTerminal\n\ntouch .nojekyll"
  },
  {
    "objectID": "posts/020-quarto-websites/index.html#step-5-render-the-template",
    "href": "posts/020-quarto-websites/index.html#step-5-render-the-template",
    "title": "Build a Quarto Website",
    "section": "Step 5: Render the Template",
    "text": "Step 5: Render the Template\nBefore we make any customizations other than those in Step 4, let’s ensure we can render the “source” files that we have been working with into html output files.\nIn Rstudio, click the “Render Website” button on the Build tab. In VS Code, click the “preview” button at the top-right of any .qmd file. Or from the Terminal, use cd to navigate inside of the website directory and run quarto preview. The rendered website should appear in RStudio’s Viewer tab, a new VS Code tab titled “Quarto Preview,” or as a new tab in your default browser.\n\n\n\n\n\n\nNoteScreenshots\n\n\n\n\n\n\nRStudioVS Code\n\n\n\n\n\n\n\n\n\n\n\n\nThe rendered website should have a navigation bar at the top, with the website title and links controlled by the content in the _quarto.yml configuration file. The information on the landing page is populated from the content of the index.qmd file and similarly the information on the About page is populated from the content of the about.qmd file."
  },
  {
    "objectID": "posts/020-quarto-websites/index.html#step-6-customize-the-home-page",
    "href": "posts/020-quarto-websites/index.html#step-6-customize-the-home-page",
    "title": "Build a Quarto Website",
    "section": "Step 6: Customize the Home Page",
    "text": "Step 6: Customize the Home Page\nI have elected to have the landing page of the website be an “about me” page. So to avoid confusion, delete the existing “about.qmd” file and the corresponding navbar entry in the _quarto.yml configuration file:\nYou should also rename the website title, which shows up in the navbar and you can optionally change the navbar text for the landing page from “Home” to “About” to further reflect that the landing page will be an “about me” page.\n\n\n_quarto.yml\n\nwebsite: \n  title: \"quarto_website\" # &lt;-- change this\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home        # &lt;-- optionally change this to \"About\"\n      - href: about.qmd   # &lt;-- delete this line\n\nQuarto has two special page types – about pages and blog listing pages – and we’ll use both in this tutorial. To make the landing page of the website into an “about me” page, change index.qmd to the following:\n\n\nindex.qmd\n\n---\ntitle: \"Your Name\"\n1image: \"files/profile_pic.jpg\"\nabout:\n2  template: jolla\n3  links:\n    - icon: envelope\n      href: mailto:your_email@address.com\n    - icon: linkedin\n      href: https://www.linkedin.com/in/your_username\n---\n\n4Welcome to my website!\n\n\n1\n\nadd an image\n\n2\n\nuse an “about page” template\n\n3\n\ncreate buttons with Bootstrap5 icons\n\n4\n\nadd some information\n\n\n\nWe’ve specified that we would like to include an image from the profile_pic.jpg file in the files sub-directory. You’ll need to create that sub-directory inside of your website directory and add a file with a photo. To do this at the terminal, you can use mkdir to create a directory and cp to copy.\n\n\n\nTerminal\n\npwd # confirm you are in your 'quarto_website' directory\nmkdir files\ncp path/to/profile_pic.jpg files/profile_pic.jpg\n\n\nWe’ve also requested the jolla template. There are 5 built-in templates; a preview of each is available in the Quarto documentation.\nWe’ve also added buttons that link to our email and LinkedIn profile. You can add addition buttons decorated with any of the Bootstrap 5 icons.\nFinally, outside of the YAML document header, we started to add some text. Here, you can expand the content with additional information, images, or any of Quarto’s document authoring options.\n\nAt this point, re-render your website to check out your customized “about you” landing page.\n\n\n\n\n\n\nNoteScreenshot"
  },
  {
    "objectID": "posts/020-quarto-websites/index.html#step-7-add-a-resume",
    "href": "posts/020-quarto-websites/index.html#step-7-add-a-resume",
    "title": "Build a Quarto Website",
    "section": "Step 7: Add a Resume",
    "text": "Step 7: Add a Resume\nCreate a new Quarto document at the top-level of your website directory (where the about.qmd file used to live). We’ll call this one resume.qmd.\nAdd a PDF file of your resume to your files sub-directory.\n\n\nTerminal\n\ncp path/to/resume.pdf files/resume.pdf\n\nNext, at the command line in the Terminal (not the R console), install a handy Quarto extension for embedding PDFs in HTML files from user jmgirard by running the following command. Hit enter or otherwise indicate “y” when prompted twice, first about trusting the author and then about implementing changes.\n\n\nTerminal\n\nquarto add jmgirard/embedpdf\n\nNext, add content to the resume.qmd file we created a moment ago. This should include the embed syntax {{&lt; pdf path/to/resume.pdf &gt;}}. I have found the displayed height and width options to work well.\n\n\nresume.qmd\n\n---\ntitle: \"My Resume\"\n---\n\n{{&lt; pdf files/resume.pdf width=100% height=800 &gt;}}\n\nLastly in the step, add a link to the navbar so visitors to your website can navigate to the Resume page:\n\n\n_quarto.yml\n\nwebsite: \n  title: \"quarto_website\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: About\n      - href: resume.qmd  # &lt;-- add this\n        text: Resume      # &lt;-- add this\n\nRender yet again, possibly with Ctrl+Shift+KCtrl+Shift+K or Cmd+Shift+KCmd+Shift+K since memorizing these shortcuts is quite helpful given how often we re-render.\n\n\n\n\n\n\nNoteScreenshot"
  },
  {
    "objectID": "posts/020-quarto-websites/index.html#step-8-add-a-blog",
    "href": "posts/020-quarto-websites/index.html#step-8-add-a-blog",
    "title": "Build a Quarto Website",
    "section": "Step 8: Add a Blog",
    "text": "Step 8: Add a Blog\nCreate a Quarto file at the top-level of the website directory alongside index.qmd and resume.qmd. I’ll call mine projects.qmd. Create a sub-directory to hold the contents of the blog. I’ll call mine projects. In that sub-directory, make sub-sub-directories for each blog post. I call mine project1 and project2. Finally, in those sub-sub-directories, add index.qmd files.\n\n\nTerminal\n\ntouch projects.qmd\nmkdir projects\n\nmkdir projects/project1\nmkdir projects/project2\n\ntouch projects/project1/index.qmd\ntouch projects/project2/index.qmd\n\n\n\n\n\n\n\nNoteScreenshots\n\n\n\n\n\n\nRStudioVS Code\n\n\n\n\n\n\n\n\n\n\n\n\nIn projects.qmd, we want to specify that we’re making a blog. Quarto calls this a “listing” page. Fill out the Quarto file with at least the following:\n\n\nprojects.qmd\n\n---\ntitle: \"My Projects\"\nlisting:\n  contents: projects  # &lt;-- the sub-directory with blog content\n  type: grid          # &lt;-- my preferred layout type\n---\n\nAnd as always, add a link to the navbar so visitors to your website can navigate to your blog / project portfolio:\n\n\n_quarto.yml\n\nwebsite: \n  title: \"quarto_website\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: About\n      - href: resume.qmd\n        text: Resume\n      - href: projects.qmd  # &lt;-- add this\n        text: Projects      # &lt;-- add this\n\nYou can render now and have both projects / blog posts entirely empty. Or, just to demonstrate what happens when we add a bit of content, add the following to the project1 index.qmd file and then render.\n\n\nindex.qmd\n\n---\ntitle: \"Analysis of Cars\"\nauthor: Your Name\ndate: today\n---\n\n# Header\n\nLet's investigate the relationship between fuel efficiency (`mpg`) and engine displacement (`disp`) from the `mtcars` dataset. Those variables have a correlation of `r cor(mtcars$mpg, mtcars$disp) |&gt; format(digits=2)`.\n\n## Sub-Header\n\nHere is a plot:\n\n```{r}\n#| message: false \n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)\n```\n\nThe “listing” page for the blog shows links to the individual blog posts in a grid layout.\n\n\n\n\n\n\nNoteScreenshot\n\n\n\n\n\n\n\n\n\nThe posts themselves contain headers, text, code, and output from the code including a plot.\n\n\n\n\n\n\nNoteScreenshot"
  },
  {
    "objectID": "posts/020-quarto-websites/index.html#step-9-grab-a-beverage",
    "href": "posts/020-quarto-websites/index.html#step-9-grab-a-beverage",
    "title": "Build a Quarto Website",
    "section": "Step 9: Grab a Beverage",
    "text": "Step 9: Grab a Beverage\nIf you made it this far, you have a website skeleton. Now spend some time personalizing it and adding your projects, blog posts, or whatever else. Like many hobbies, working on a website can be fun and addictive. Check out the next section for additional inspiration.\nIf you’re ready to host your website on GitHub for the world to see, follow the steps here."
  },
  {
    "objectID": "posts/020-quarto-websites/index.html#additional-resources",
    "href": "posts/020-quarto-websites/index.html#additional-resources",
    "title": "Build a Quarto Website",
    "section": "Additional Resources",
    "text": "Additional Resources\nTo learn more, definitely check out quarto.org’s resources for making websites: https://quarto.org/docs/websites/\nPosit’s four-part YouTube tutorial on Quarto Websites is another valuable resource: part 1, part 2, part 3, part 4.\nAdditionally, there are several other excellent tutorials on developing Quarto websites and blogs:\n\n\n\nAuthor\nLink\n\n\n\n\nUCSB Med\nhttps://ucsb-meds.github.io/creating-quarto-websites/\n\n\nBeatriz Milz\nhttps://beamilz.com/posts/2022-06-05-creating-a-blog-with-quarto/en/\n\n\nMarvin Schmitt\nhttps://www.marvin-schmitt.com/blog/website-tutorial-quarto/\n\n\nAlbert Rapp\nhttps://albert-rapp.de/posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html\n\n\nSam Csik\nhttps://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/\n\n\nDeepsha Menghani\nhttps://www.youtube.com/watch?v=xtSFXtDf4cM\n\n\n\n\\[\n\\hspace{1em}\n\\]\nFinally, if you see something you like on someone else’s website, you can usually visit their GitHub page to see how they did it!\n\nIn particular Andrew Heiss and Marvin Schmitt have fully-featured Quarto-based websites.\nIn addition, the following websites (listed alphabetically by first name) offer inspiration too: Albert Rapp, Beatriz Milz, Chris Kenny, Ella Kaye, Emil Hvitfeldt, Meghan Hall, Mine Dogucu, Sam Csik, and Tom Mock.\nMine is here."
  },
  {
    "objectID": "posts/801-abramovich-ritov/index.html",
    "href": "posts/801-abramovich-ritov/index.html",
    "title": "Statistical Theory: A Concise Introduction (2ed)",
    "section": "",
    "text": "Get the book from Routledge (here) or Amazon (here)\nThis is a truly excellent book explaining the underlying ideas, mathematics, and principles of major statistical concepts. Its organization is suburb, and the authors’ commentary on why a theorem is so useful and how the presented ideas fit together and/or contrast is invaluable (and, quite frankly, better than I have seen anywhere else).\n\n1 Introduction\nSuppose the observed data is the sample \\(\\mathbf{y} = \\{y_1, \\ldots, y_n\\}\\) of size \\(n\\). We will model \\(\\mathbf{y}\\) as a realization of an \\(n\\)-dimensional random vector \\(\\mathbf{Y} = \\{Y_1, \\ldots, Y_n\\}\\) with a joint distribution \\(f_\\mathbf{Y}(\\mathbf{y})\\). The true distribution of the data is rarely completely known; nevertheless, it can often be reasonable to assume that it belongs to some family of distributions \\(\\mathcal{F}\\). We will assume that \\(\\mathcal{F}\\) is a parametric family, that is, that we know the type of distribution \\(f_\\mathbf{Y}(\\mathbf{y})\\) up to some unknown parameter(s) \\(\\theta \\in \\Theta\\), where \\(\\Theta\\) is a parameter space. Typically, we will consider the case where \\(y_1, \\ldots, y_n\\) are the results of independent identical experiments. In this case, \\(Y_1, \\ldots, Y_n\\) can be treated as independent, identically distributed random variables with the common distribution \\(f_\\theta(y)\\) from a parametric family of distributions \\(\\mathcal{F}_\\theta\\), \\(\\theta \\in \\Theta\\).\nDefine the likelihood function \\(L(\\theta; \\mathbf{y}) = P_\\theta(\\mathbf{y})\\) — the probability to observe the given data \\(\\mathbf{y}\\) for any possible value of \\(\\theta \\in \\Theta\\). First assume that \\(\\mathbf{Y}\\) is discrete. The value \\(L(\\theta; \\mathbf{y})\\) can be viewed as a measure of likeliness of \\(\\theta\\) to the observed data \\(\\mathbf{y}\\). If \\(L(\\theta_1; \\mathbf{y}) &gt; L(\\theta_2; \\mathbf{y})\\) for a given \\(\\mathbf{y}\\), we can say that the value \\(\\theta_1\\) for \\(\\theta\\) is more suited to the data than \\(\\theta_2\\). For a continuous random variable \\(\\mathbf{y}\\), the likelihood ratio \\(L(\\theta_1; \\mathbf{y})/L(\\theta_2; \\mathbf{y})\\) shows the strength of the evidence in favor of \\(\\theta = \\theta_1\\) vs \\(\\theta = \\theta_2\\).\nA statistic \\(T(\\mathbf{Y})\\) is any real or vector-valued function that can be computed using the data alone. A statistic \\(T(\\mathbf{Y})\\) is sufficient for an unknown parameter \\(\\theta\\) if the conditional distribution of all the data \\(\\mathbf{Y}\\) given \\(T(\\mathbf{Y})\\) does not depend on \\(\\theta\\). In other words, given \\(T(\\mathbf{Y})\\) no other information on \\(\\theta\\) can be extracted from \\(\\mathbf{y}\\). This definition allows one to check whether a given statistic \\(T(\\mathbf{Y})\\) is sufficient for \\(\\theta\\), but it does not provide one with a constructive way to find it.\nHowever, the Fisher-Neyman Factorization Theorem says that a statistic \\(T(\\mathbf{Y})\\) is sufficient for \\(\\theta\\) iff for all \\(\\theta \\in \\Theta\\) that \\(L(\\theta, \\mathbf{y}) = g(T(\\mathbf{y}), \\theta) \\cdot h(\\mathbf{y})\\), where the function \\(g(\\cdot)\\) depends on \\(\\theta\\) and the statistic \\(T(\\mathbf{Y})\\), while \\(h(\\mathbf{y})\\) does not depend on \\(\\theta\\). In particular, if the likelihood \\(L(\\theta; \\mathbf{y})\\) depends on data only through \\(T(\\mathbf{Y})\\), then \\(T(\\mathbf{Y})\\) is a sufficient statistic for \\(\\theta\\) and \\(h(\\mathbf{y}) = 1\\).\nA sufficient statistic is not unique. For example, the entire sample \\(\\mathbf{Y} = \\{Y_1, \\ldots, Y_n\\}\\) is always a (trivial) sufficient statistic. We may seek a minimal sufficient statistic implying the maximal reduction of the data. A statistic \\(T(\\mathbf{Y})\\) is called a minimal sufficient statistic if it is a function of any other sufficient statistic.\nAnother important property of a statistic is completeness. Let \\(Y_1, \\ldots, Y_n \\sim f_\\theta(y)\\), where \\(\\theta \\in \\Theta\\). A statistic \\(T(\\mathbf{Y})\\) is complete if no statistic \\(g(\\mathbf{T})\\) exists (except \\(g(\\mathbf{T})=0\\)) such that \\(E_\\theta g(\\mathbf{T}) = 0\\) for all \\(\\theta \\in \\Theta\\). In other words, if \\(E_\\theta g(\\mathbf{T}) = 0\\) for all \\(\\theta \\in \\Theta\\), then necessarily \\(g(\\mathbf{T})=0\\). To verify completeness for a general distribution can be a nontrivial mathematical problem, but thankfully it is much simpler for the exponential family of distributions that includes many of the “common” distributions. Completeness is a useful in determining minimal sufficiency because if a sufficient statistic \\(T(\\mathbf{Y})\\) is complete, then it is also minimal sufficient. (Note, however, that a minimal sufficient statistic may not necessarily be complete.)\nA (generally multivariate) family of distributions \\({f_\\theta(\\mathbf{y}): \\theta \\in \\Theta}\\) is said to be an (one parameter) exponential family if: (1) \\(\\Theta\\) is an open interval, (2) the support of the distribution \\(f_\\theta\\) does not depend on \\(\\theta\\), and (3) \\(f_\\theta(\\mathbf{y}) = exp\\{c(\\theta)T(\\mathbf{y}) + d(\\theta) + S(\\mathbf{y})\\}\\) where \\(c(\\cdot)\\), \\(T(\\cdot)\\), \\(d(\\cdot)\\), and \\(S(\\cdot)\\) are known functions; \\(c(\\theta)\\) is usually called the natural parameter of the distribution. We say that \\(f_\\theta\\) where \\(\\theta = (\\theta_1, \\ldots \\theta_p)\\) belongs to a \\(k\\)-parameter exponential family by changing (3) such that \\(f_\\theta(\\mathbf{y}) = exp\\{ \\sum_{j=1}^k c_j(\\theta)T_j(\\mathbf{y}) + d(\\theta) + S(\\mathbf{y})\\}\\). The function \\(c(\\theta) = \\{c_1(\\theta), \\ldots, c_k(\\theta)\\}\\) are the natural parameters of the distribution. (Note that the dimensionality \\(p\\) of the original parameter \\(\\theta\\) is not necessarily the same as the dimensionality \\(k\\) of the natural parameter \\(c(\\theta)\\).)\nConsider a random sample \\(Y_1, \\ldots, Y_n\\), where \\(Y_i \\sim f_\\theta(y)\\) and \\(f_\\theta\\) belongs to a \\(k\\)-parameter exponential family of distributions, then (1) the joint distribution of \\(\\mathbf{Y} = (Y_1, \\ldots, Y_n)\\) also belongs to the \\(k\\)-parameter exponential family, (2) \\(T_\\mathbf{Y} = (\\sum_{i=1}^n T_1(Y_i), \\ldots, \\sum_{i=1}^n T_k(Y_i))\\) is the sufficient statistic for \\(c(\\theta) = (c_1(\\theta), \\ldots, c_k(\\theta))\\) (and, therefore, for \\(\\theta\\)), and (3) if some regularity conditions hold, then \\(T_\\mathbf{Y}\\) is complete and therefore minimal sufficient (if the latter exists).\n\n\n2 Point Estimation\nEstimation of the unknown parameters of distributions from the data is one of the key issues in statistics. A (point) estimator \\(\\hat{\\theta} = \\hat{\\theta}(\\mathbf{Y})\\) of an unknown parameter \\(\\theta\\) is any statistic used for estimating \\(\\theta\\). The value of \\(\\hat{\\theta}(\\mathbf{y})\\) evaluated for a given sample is called an estimate. This is a general, somewhat trivial definition that does not say anything about the goodness of estimation; one would evidently be interested in “good” estimators.\nMaximum Likelihood Estimation is the most used method of estimation of parameters in parametric models. As we’ve discussed, \\(L(\\theta; \\mathbf{y})\\) is the measure of likeliness of a parameter’s values \\(\\theta\\) for the observed data \\(\\mathbf{y}\\). It is only natural then to seek the “most likely” value of \\(\\theta\\). The MLE \\(\\hat{\\theta}\\) of \\(\\theta\\) is \\(\\hat{\\theta} = \\arg\\max_{\\theta\\in\\Theta} L(\\theta; \\mathbf{y})\\) — the value of \\(\\theta\\) that maximizes the likelihood. Often we are interested in a function of a parameter \\(\\xi = g(\\theta)\\). When \\(g(\\cdot)\\) is 1:1, the MLE of \\(\\xi\\) is the function \\(g(\\cdot)\\) applied to the MLE of \\(\\theta\\): \\(\\hat{\\xi} = g(\\hat{\\theta})\\), the proof of which is just a reparameterization of the likelihood in terms of \\(\\xi\\) instead of \\(\\theta\\). Although the conception of the MLE was motivated by an intuitively clear underlying idea, the justification for its use is much deeper. It is a really “good” method of estimation (to be discussed later on the topic of asymptotics).\nAnother popular method of estimation is the Method of Moments. Its main idea is based on expressing the population moments of the distribution of data in terms of its unknown parameter(s) and equating them to their corresponding sample moments. MMEs have some known problems: consider a sample of size 4 from a uniform distribution \\(U(0, \\theta)\\) with the observed sample 0.2, 0.6, 2, and 0.4. The MME is 1.6, which does not make much sense given the observed value of 2 in the sample. For these and related reasons, the MMEs are less used than the MLE counterparts. However, MMEs are usually simpler to compute and can be used, for example, as reasonable initial values in numerical iterative procedures for MLEs. On the other hand, the Method of Moments does not require knowledge of the entire distribution of the data (up to the unknown parameters) but only its moments and thus may be less sensitive to possible misspecification of a model.\nThe Method of Least Squares play a key role in regression and analysis of variance. In a typical regression setup, we are given \\(n\\) observations \\((\\mathbf{x}_i, y_i)\\), \\(i=1, \\ldots, n\\) over \\(m\\) explanatory variables \\(\\mathbf{x} = (x_1, \\ldots, x_m)\\) and the response variable \\(Y\\). We assume that \\(y_i = g_\\theta(\\mathbf{x}_i) + \\varepsilon_i\\), \\(i = 1, \\ldots, n\\) where the response function \\(g_\\theta(\\cdot): \\mathbb{R}^m \\rightarrow \\mathbb{R}\\) has a known parametric form and depends on \\(p \\le n\\) unknown parameters \\(\\theta = (\\theta_1, \\ldots, \\theta_p)\\). The LSE looks for a \\(\\hat{\\theta}\\) that yields the best fit \\(g_\\hat{\\theta}(\\mathbf{x})\\) to the observed \\(\\mathbf{y}\\) w.r.t. the Euclidean distance: \\(\\hat{\\theta} = \\arg\\min_\\theta \\sum_{i=1}^n (y_i - g_\\theta(\\mathbf{x}_i))^2\\). For linear regression, the solution is available in closed form. For non-linear regression, however, it can generally be only found numerically.\nMore generally than the three above procedures, one can consider any function \\(\\rho(\\theta, y)\\) as a measure of the goodness-of-fit and look for an estimator that maximizes or minimizes \\(\\sum_{i=1}^n \\rho(\\theta, y_i)\\) w.r.t. \\(\\theta\\). Such estimators are called M-estimators. It turns out that various well-known estimators can be viewed as M-estimators for a particular \\(\\rho(\\theta, y)\\) including \\(\\bar{Y}\\) for the sample mean as well as MLE, LSE, and a generalized version of MME not yet discussed. As we’ll see when discussing asymptotics, M-estimators share many important asymptotic properties.\nA natural question is how to compare between various estimators. First, we should define a measure of goodness-of-estimation. Recall that any estimator \\(\\hat{\\theta} = \\hat{\\theta}(Y_1, \\ldots, Y_n)\\) is a function of a random sample and therefore is a random variable itself with a certain distribution, expectation, variance, etc. A somewhat naive attempt to measure the goodness-of-estimation of \\(\\hat{\\theta}\\) would be to consider the error \\(|\\hat{\\theta}-\\theta|\\). However, \\(\\theta\\) is unknown and, as we have mentioned, an estimator \\(\\hat{\\theta}\\) is a random variable and hence the value \\(|\\hat{\\theta}-\\theta|\\) will vary from sample to sample. It may be “small” for some of the samples, while “large” for others and therefore cannot be used as a proper criterion for goodness-of-estimation of an estimator \\(\\hat{\\theta}\\). A more reasonable measure would then be an average distance over all possible samples, that is, the mean absolute error \\(E|\\hat{\\theta}-\\theta|\\), where the expectation is taken w.r.t. the joint distribution of \\(\\mathbf{Y} = (Y_1, \\ldots, Y_n)\\). It indeed can be used as a measure of goodness-of-estimation but usually, mostly due to convenience of differentiation, the conventional measure is the mean squared error (MSE) given by \\(MSE(\\hat{\\theta}, \\theta) = E(\\hat{\\theta}-\\theta)^2\\).\nThe MSE can be decomposed into two components: \\(MSE(\\hat{\\theta}, \\theta) = Var(\\hat{\\theta}) + b^2(\\hat{\\theta},\\theta)\\). The first is the stochastic error (variance) and the second is a systematic or deterministic error (bias). Having defined the goodness-of-estimation measure by MSE, one can compare different estimators and choose the one with the smallest MSE. However, since the \\(MSE(\\hat{\\theta}, \\theta)\\) typically depends on the unknown \\(\\theta\\), it is a common situation where no estimator is uniformly superior for all \\(\\theta \\in \\Theta\\).\nIdeally, a good estimator with a small MSE should have both low variance and low bias. However, it might be hard to have both. One of the common approaches is to first control the bias component of the overall MSE and to consider unbiased estimators. There is no general rule or algorithm for deriving an unbiased estimator. In fact, unbiasedness is a property of an estimator rather than a method of estimation. One usually checks an MLE or any other estimator for bias. Sometimes one can then modify the original estimator to “correct” its bias. Note that unlike ML estimation, unbiasedness is not invariant under nonlinear transformation of the original parameter: if \\(\\hat{\\theta}\\) is an unbiased estimator of \\(\\theta\\), \\(g(\\hat{\\theta})\\) is generally a biased estimator for \\(g(\\theta)\\).\nWhat does unbiasedness of an estimator \\(\\hat{\\theta}\\) actually mean? Suppose we were observing not a single sample but all possible samples of size \\(n\\) from a sample space and calculating the estimates \\(\\hat{\\theta}_j\\) for each one of them. The unbiasedness means that the average value of \\(\\hat{\\theta}\\) over the entire sample space is \\(\\theta\\), but it does not guarantee yet that \\(\\hat{\\theta}_j \\approx \\theta\\) for each particular sample. The dispersion of \\(\\hat{\\theta}_j\\)’s around their average value \\(\\theta\\) might be large and, since in reality we have only a single sample, its particular value of \\(\\hat{\\theta}\\) might be quite away from \\(\\theta\\). To ensure with high confidence that \\(\\hat{\\theta}_j \\approx \\theta\\) for any sample we need, in addition to unbiasedness, for the variance \\(Var(\\hat{\\theta})\\) to be small.\nAn estimator \\(\\hat{\\theta}\\) is called a uniformly minimum variance unbiased estimator (UMVUE) of \\(\\theta\\) if \\(\\hat{\\theta}\\) is unbiased and for any other unbiased estimator \\(\\tilde{\\theta}\\) of \\(\\theta\\), \\(Var(\\hat{\\theta}) \\le Var(\\tilde{\\theta})\\). If the UMVUE exists, it is necessarily unique. Recall that there is no general algorithm to obtain unbiased estimators in general and a UMVUE in particular. However, there exists a lower bound for a variance of an unbiased estimator, which can be used as a benchmark for evaluating its goodness.\nDefine the Fisher Information Number \\(I(\\theta) = E((\\ln f_\\theta(\\mathbf{y}))'_\\theta)^2\\). The derivative of the log density is sometimes called the Score Function. Thus, the Fisher Information Number is the expected square of the Score. The Cramer-Rao Lower Bound Theorem states that if \\(T\\) is an unbiased estimator for \\(g(\\theta)\\), where \\(g(\\cdot)\\) is differentiable, then \\(Var(T) \\ge (g'(\\theta))^2 / I(\\theta)\\) or more simply, when \\(T\\) is an unbiased estimator for \\(\\theta\\), \\(Var(T) \\ge 1 / I(\\theta)\\). We are especially interested in the case where \\(Y_1, \\ldots, Y_n\\) is a random sample from a distribution \\(f_\\theta(y)\\). In that case, \\(I(\\theta) = nI^*(\\theta)\\) where \\(I^*(\\theta) = E((\\ln f_\\theta(y))'_\\theta)^2\\) is the Fisher Information Number of \\(f_\\theta(y)\\), and, therefore, for any unbiased estimator \\(T\\) of \\(g(\\theta)\\), we have that \\(Var(T) \\ge (g'_\\theta(\\theta))^2 / nI^*(\\theta)\\). There is another, usually more convenient formula for calculating the Fisher Information Number \\(I(\\theta)\\) other than its direct definition: \\(I(\\theta) = -E(\\ln f_\\theta(\\mathbf{Y}))''_\\theta\\).\nIt is important to emphasize that the CRLB theorem is one direction only: if the variance of an unbiased estimator does not achieve the Cramer-Rao lower bound, one still cannot claim that it is not an UMVUE (it might just be the UMVUE). Nevertheless, it can be used as a benchmark for measuring the goodness of an unbiased estimator. One special result related to the exponential family distribution: the Cramer-Rao lower bound is achieved only for distributions from the exponential family.\nThe Cramer-Rao lower bound allows one only to evaluate the goodness of a proposed unbiased estimator but does not provide any constructive way to derive it. In fact, as we have argued, there is no such general rule at all. However, if one manages to obtain any initial (even crude) unbiased estimator, it may be possible to improve it. The Rao-Blackwell Theorem shows that if there is an unbiased estimator that is not a function of a sufficient statistic \\(W\\), one can construct another unbiased estimator based on \\(W\\) with an MSE not larger than the original one: let \\(T\\) be an unbiased estimator of \\(\\theta\\) and \\(W\\) be a sufficient statistic for \\(\\theta\\), and define \\(T_1 = E(T|W)\\), then \\(T_1\\) is an unbiased estimator of \\(\\theta\\) and \\(Var(T_1) \\le Var(T)\\). Thus, in terms of MSE, only unbiased estimators based on a sufficient statistic are of interest. This demonstrates again a strong sense of the notion of sufficiency.\nDoes Rao-Blackwellization necessarily yield an UMVUE? Generally not. To guarantee UMVUE an additional requirement of completeness on a sufficient statistic \\(W\\) is needed. The Lehmann-Scheffe Theorem formalizes this: if \\(T\\) is an unbiased estimator of \\(\\theta\\) and \\(W\\) is a complete sufficient statistic for \\(\\theta\\), then \\(T_1 = E(T|W)\\) is the unique UMVUE of \\(\\theta\\). Even without the Lehmann-Scheffe theorem, it can be shown under mild conditions that if the distribution of the data belongs to the exponential family and an unbiased estimator is a function of the corresponding sufficient statistic, it is an UMVUE. Note that despite its elegance, the application of the Rao-Blackwell Theorem in more complicated cases is quite limited. The two main obstacles are in finding an initial unbiased estimator \\(T\\) and calculating the conditional expectation \\(E(T|W)\\)\n\n\n3 Confidence Intervals, Bounds, and Regions\nWhen we estimate a parameter we essentially “guess” its value. This should be a “well educated guess,” hopefully the best of its kind (in whatever sense). However, statistical estimation is made with error. Presenting just the estimator, no matter how good it is, is usually not enough – one should give an idea about the estimation error. The words “estimation” and “estimate” (in contrast to “measure” and “value”) point to the fact that an estimate is an inexact appraisal of a value. Great efforts of statistics are invested in trying to quantify the estimation error and find ways to express it in a well-defined way.\nAny estimator has error, that is, if \\(\\theta\\) is estimated by \\(\\hat{\\theta}\\), then \\(\\hat{\\theta} - \\theta\\) is usually different from \\(0\\). The standard measure of error is the mean squared error \\(MSE = E(\\hat{\\theta} - \\theta)^2\\). When together with the value of the estimator we are given its MSE, we get a feeling of how precise the estimate is. It is more common to quote the standard error defined by \\(SE(\\hat{\\theta}) = \\sqrt{MSE(\\hat{\\theta}, \\theta)} = \\sqrt{E(\\hat{\\theta}-\\theta)^2}\\). Unlike the MSE, the SE is measured in the same units as the estimator.\nHowever, the MSE expresses an average squared estimation error but tells nothing about its distribution, which generally might be complicated. One would be interested, in particular, in the probability \\(P(|\\hat{\\theta}-\\theta| &gt; c)\\) that the estimation error exceeds a certain accuracy level \\(c &gt; 0\\). Markov’s Inequality enables us to translate the MSE into an upper bound on this probability: \\(P(|\\hat{\\theta}-\\theta| \\ge c) \\le MSE/c^2\\). However, this bound is usually very conservative and the actual probability may be much smaller. Typically, a quite close approximation of the error distribution is obtained by the Central Limit Theorem (discussed later).\nAbove, we suggested quoting the SE besides the estimator itself. However, standard statistical practice is different and it is not very intuitive. There are a few conceptual difficulties in being precise when talking about the error. Suppose we estimate \\(\\mu\\) with \\(\\bar{Y}\\) and we find \\(\\bar{Y}=10\\) and \\(SE=0.2\\). We are quite confident that \\(\\mu\\) is between \\(9.6\\) and \\(10.4\\). However, this statement makes no probabilistic sense. The true \\(\\mu\\) is either in the interval \\((9.6,10.4)\\) or it’s not. The unknown \\(\\mu\\) is not a random variable – it has a single fixed value, whether or not it is known to the statistician.\nThe “trick” that has been devised is to move from a probabilistic statement about the unknown (but with a fixed value) parameter to a probabilistic statement about the method. We say something like: “The interval \\((9.6,10.4)\\) for the value \\(\\mu\\) was constructed by a method which is 95% successful.” Since the method is usually successful, we have confidence in its output.\nLet \\(Y_1, \\ldots, Y_n \\sim f_\\theta(y)\\), \\(\\theta \\in \\Theta\\). A \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\theta\\) is the pair of scalar-valued statistics \\(L=L(Y_1, \\ldots, Y_n)\\) and \\(U=U(Y_1, \\ldots, Y_n)\\) such that \\(P(L \\le \\theta \\le U) \\ge 1-\\alpha\\) for all \\(\\theta \\in \\Theta\\) with the inequality as close to an equality as possible.\nWhat is the right value of \\(\\alpha\\)? Nothing in the statistical theory dictates a particular choice. But the standard values are \\(0.10\\), \\(0.05\\), and \\(0.01\\) with \\(0.05\\) being most common.\nA pivot is a function \\(\\psi(Y_1, \\ldots, Y_n; \\theta)\\) of the data and the parameters, whose distribution does not depend on unknown parameters. Note, the pivot is not a statistic and cannot be calculated from the data, exactly because it depends on the unknown parameters. On the other hand, since its distribution does not depend on the unknown parameters, we can find an interval \\(A_\\alpha\\) such that \\(P(\\psi(Y_1, \\ldots, Y_n; \\theta) \\in A_\\alpha) = 1 - \\alpha\\). Then we can invert the inclusion and define the interval (or region in general) \\(C_\\alpha = \\{\\theta : \\psi(Y_1, \\ldots, Y_n; \\theta) \\in A_\\alpha \\}\\). The set \\(C_\\alpha\\) is then a \\((1-\\alpha)100\\%\\) confidence set. Note that \\(C_\\alpha\\) is a random set because it depends on the random sample.\nDoes a pivot always exist? For a random sample \\(Y_1, \\ldots, Y_n\\) from any continuous distribution with a cdf \\(F_\\theta(y)\\), the value \\(-\\sum_{i=1}^n \\ln F_\\theta(Y_i) \\sim \\frac{1}{2} \\chi^2_{2n}\\) and, therefore, is a pivot. However, in the general case, it might be difficult (if possible) to invert the corresponding confidence interval for this pivot to a confidence interval for the original parameter of interest \\(\\theta\\). More convenient pivots are easy to find when the distribution belongs to a scale-location family.\nIs a confidence interval for \\(\\theta\\) necessarily unique? Definitely not! Different choices of pivots lead to different forms of confidence intervals. Moreover, even for a given pivot, one can typically construct an infinite set of confidence intervals at the same confidence level. What is the “best” choice for a confidence interval? A conventional, somewhat ad hoc approach is based on error symmetry: set \\(P(L&gt;\\theta) = P(U&lt;\\theta) = \\alpha/2\\). A more appealing approach would be to seek a confidence interval of a minimal expected length; however, in general, this leads to a nonlinear minimization problem that might not have a solution in closed form.\nA parameter of interest may be not the original parameter \\(\\theta\\) but its function \\(g(\\theta)\\). Let \\((L,U)\\) be a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\theta\\) and \\(g(\\cdot)\\) a strictly increasing function. Then \\((g(L),g(U))\\) is a \\((1-\\alpha)100\\%\\) confidence interval for \\(g(\\theta)\\).\nThe normal confidence intervals are undoubtedly the most important. We do not claim that most data sets are sampled from a normal distribution – this is definitely far from being true. However, what really matters is whether the distribution of an estimator \\(\\hat{\\theta} = \\hat{\\theta}(Y_1, \\ldots, Y_n)\\) is close to normal rather than the distribution of \\(Y_1, \\ldots, Y_n\\) themselves. When discussing asymptotics later, we argue that many estimators based on large or even medium sized samples are indeed approximately normal and, therefore, the normal confidence intervals can (at least approximately) be used.\n\n\n4 Hypothesis Testing\nIn many situations the research question yields one of two possible answers “yes” or “no” and a statistician is required to choose the “correct” one based on the data. The two possible answers can be viewed as two hypotheses—the null hypothesis (\\(H_0\\)) and the alternative (\\(H_1\\)). The process of choosing between them based on the data is known as hypothesis testing.\nMore formally, suppose \\(\\mathbf{Y} \\sim f_\\theta(\\mathbf{y})\\) where \\(f_\\theta(\\mathbf{y})\\) belongs to a parametric family of distributions \\(\\mathcal{F}_\\theta\\), \\(\\theta \\in \\Theta\\). Under the null hypothesis \\(\\theta \\in \\Theta_0 \\subset \\Theta\\), while under the alternative \\(\\theta \\in \\Theta_1 \\subset \\Theta\\), where \\(\\Theta_0 \\cap \\Theta_1 = \\emptyset\\). Given the data \\(\\mathbf{Y}\\) we need to find a rule (test) to decide whether we accept or reject the null hypothese \\(H_0\\) based on a test statistic \\(T(\\mathbf{Y})\\). Typically \\(T(\\mathbf{Y})\\) is chosen in such a way that it tends to be small under \\(H_0\\): the larger it is, the stronger the evidence against \\(H_0\\) in favor of \\(H_1\\). The null hypothesis is then rejected if \\(T(\\mathbf{Y})\\) is “too large” for it, that is, if \\(T(\\mathbf{Y}) &gt; C\\) for some critical value \\(C\\).\nIt is essential to understand that since the inference on the hypothesis is based on random data, there is always a possibility for a wrong decision: we might either erroneously reject \\(H_0\\) (Type I error) or accept it (Type II error). The key question in hypothesis testing is a proper choice of a test statistic and the corresponding critical value that would minimize the probability of an erroneous decision. Unfortunately, as we shall see, it is generally impossible to minimize the probability of errors of both types: decreasing one of them comes at the expense of increasing the other.\nThe probability of a Type I error (erroneous rejection of the null hypothesis) also known as the (significance) level of the test is \\(\\alpha = P_{\\theta_0} \\left( T(\\mathbf{Y}) \\ge C \\right) = P_{\\theta_0} \\left( \\mathbf{Y} \\in \\Omega_1 \\right)\\). Similarly, the probability of a Type II error (erroneous acceptance of the null hypothesis) is \\(\\beta = P_{\\theta_1} \\left( T(\\mathbf{Y}) &lt; C \\right) = P_{\\theta_1} \\left( \\mathbf{Y} \\in \\Omega_0 \\right)\\). The power of a test is \\(\\pi = 1 - \\beta = P_{\\theta_1} \\left( \\mathbf{Y} \\in \\Omega_1 \\right)\\). Note that hypothesis testing induces a partition of the sample space \\(\\Omega\\) into two disjoint regions: the rejection region \\(\\Omega_1 = \\{ \\mathbf{y}: T(\\mathbf{y}) \\ge C \\}\\) and its complement, the acceptance region \\(\\Omega_0 = \\{ \\mathbf{y}: T(\\mathbf{y}) &lt; C \\}\\).\nThe choice of \\(C\\) is usually not obvious. \\(\\alpha = P_{\\theta_0} \\left( T(\\mathbf{Y}) \\ge C \\right)\\) and \\(\\beta = P_{\\theta_1} \\left( T(\\mathbf{Y}) &lt; C \\right)\\). Increasing \\(C\\) leads to the reduction of the rejection region \\(\\Omega_1\\) and therefore to a smaller \\(\\alpha\\) but larger \\(\\beta\\) and vice versa. It is impossible to find an optimal \\(C\\) that would minimize both \\(\\alpha\\) and \\(\\beta.\\) Typically, one controls the probability of the Type I error at some conventional low level (say, 0.05 or 0.01) and finds the corresponding \\(C\\) from the equation \\(\\alpha = P_{\\theta_0} \\left( T(\\mathbf{Y}) \\ge C \\right)\\). In this case, there is no longer symmetry between the two hypotheses and the choice of \\(H_0\\) becomes crucial. Two researchers analyzing the same data might come to different conclusions depending on their choices for the null and alternative. Although there are no formal rules, the null hypothesis usually represents the current or conventional state and a “strong evidence” should be provided to reject it in favor of the alternative corresponding to a change.\nUp until now, for a given test statistic \\(T(\\mathbf{Y})\\), we calculate its value \\(t_{obs}=T(\\mathbf{y})\\) for the observed data \\(\\mathbf{y}\\), compared \\(t_{obs}\\) with a critical value \\(C\\) corresponding to a significance level \\(\\alpha\\), and rejected the null hypothesis if \\(t_{obs} \\ge C\\). We would like to standardize \\(t_{obs}\\) to a scale which will tell us directly its “unlikeness” under the null. Namely, consider the probability under \\(H_0\\) that the test statistic \\(T(\\mathbf{Y})\\) has a value of at least \\(t_{obs}\\). Such a probability is called a p-value, that is, \\(\\textrm{p-value} = P_{\\theta_0} \\left( T(\\mathbf{Y}) \\ge t_{obs} \\right)\\). A p-value can be viewed as the minimal significance level for which one would reject the null hypothesis for a given value of a test statistic.\nSo far, a test statistic \\(T(\\mathbf{Y})\\) was chosen in advance and we discussed the corresponding probabilities of both error types, the power, p-value, critical value, etc. The core question in hypothesis testing, however, is the proper choice of \\(T(\\mathbf{Y})\\). What is the “optimal” test?\nLet \\(\\mathbf{Y} \\sim f_\\theta(\\mathbf{y})\\) and test two simple hypotheses \\(H_0: \\theta = \\theta_0\\) vs. \\(H_1: \\theta = \\theta_1\\). Define the likelihood ratio\n\\[\n\\lambda(\\mathbf{Y})\n=\n\\frac{\n    L(\\theta_1; \\mathbf{Y}) }{\n    L(\\theta_0; \\mathbf{Y})}\n=\n\\frac{\n    f_{\\theta_1}(\\mathbf{Y}) }{\n    f_{\\theta_0}(\\mathbf{Y}) }\n\\]\nThe value of \\(\\lambda(\\mathbf{Y})\\), in fact, shows how much \\(\\theta_1\\) is more likely for the observed data than \\(\\theta_0\\). Obviously, the larger \\(\\lambda(\\mathbf{Y})\\), the stronger the evidence against the null. The corresponding likelihood ratio test (LRT) at a given level \\(\\alpha\\) is of the form: reject \\(H_0\\) in favor of \\(H_1\\) if \\(\\lambda(\\mathbf{Y}) \\ge C\\), where the critical value \\(C\\) satisfies \\(P_{\\theta_0} \\left( \\lambda(\\mathbf{Y}) \\ge C \\right) = \\alpha\\). As it happens, the LRT is exactly the optimal test we are looking for that is, the one with maximal power (MP) among all tests at a significance level \\(\\alpha\\), which is formalized as the Neyman-Pearson Lemma.\nA main problem is in its practical implementation, specifically of finding critical value \\(C\\). For a fixed level \\(\\alpha\\), \\(C\\) is the solution of \\(P_{\\theta_0} \\left( \\lambda(\\mathbf{Y}) \\ge C \\right) = \\alpha\\), where the distribution of \\(\\lambda(\\mathbf{Y})\\) might be quite complicated. For large samples various asymptotic approximations for the distribution of \\(\\lambda(\\mathbf{Y})\\) can be used.\nThe situation in which both the null and the alternative hypotheses are simple is quite rare in practical applications — it is usual that at least the alternative hypothesis is composite: \\(H_1: \\theta \\in \\Theta_1 \\subset \\Theta\\). So we now have the power function \\(\\pi(\\theta)\\) of a test is the probability of rejecting \\(H_0\\) as a function of \\(\\theta\\), that is, \\(\\pi(\\theta) = P_\\theta \\left( \\textrm{reject}\\ H_0 \\right) = P_\\theta \\left( \\mathbf{Y} \\in \\Omega_1 \\right)\\). The significance level is now the maximal probability of a Type I error over all possible \\(\\theta \\in \\Theta_0\\) (ie, the worst case scenario): \\(\\alpha = \\sup_{\\theta \\in \\Theta_0} \\pi(\\theta)\\). And similarly the p-value is the \\(\\sup_{\\theta \\in \\Theta_0} P_\\theta \\left( T(\\mathbf{Y}) \\ge t_{obs} \\right)\\).\nAs we have argued, the main problem in hypothesis testing is the proper choice of a test. We discussed that for simple hypothesis one compares the performance of two different tests with the same significance level by their powers and looks for the maximum power test. What happens with composite hypothesis, where there is no single power value but a power function? Consider two tests with power function \\(\\pi_1(\\theta)\\) and \\(\\pi_2(\\theta)\\) with the same significance level \\(\\alpha\\). If \\(\\pi_1(\\theta) &gt; \\pi_2(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\), we can say that the first test uniformly outperforms its counterpart. Unfortunately, this is not common. Rather it is typical for \\(\\pi_1(\\theta) &gt; \\pi_2(\\theta)\\) for only some \\(\\theta \\in \\Theta_1\\) and so neither test is uniformly better and we cannot prefer either of them. This is reminiscent of the situation of comparing estimators by MSE in Section 2.\nThe requirement of uniformly most powerfulness for all \\(\\theta \\in \\Theta_1\\) among all possible tests at given level \\(\\alpha\\) is very strong. For example, there is a UMP for a one-sided test for the normal mean, but not for a two-sided test for the normal mean. And in multiparameter cases, UMP tests do not exist at all except for some singular cases. Nevertheless, it seems tempting to generalize the likelihood ratio test of Neyman-Pearson to composite hypotheses. The natural generalization of the likelihood ratio for composite hypotheses is\n\\[\n\\lambda^*(\\mathbf{y})\n=\n\\frac{\n  \\sup_{\\theta\\in\\Theta_1} L(\\theta_1; \\mathbf{Y}) }{\n  \\sup_{\\theta\\in\\Theta_0} L(\\theta_0; \\mathbf{Y}) }\n=\n\\frac{\n  \\sup_{\\theta\\in\\Theta_1} f_{\\theta_1}(\\mathbf{Y}) }{\n  \\sup_{\\theta\\in\\Theta_0} f_{\\theta_0}(\\mathbf{Y}) }\n\\]\nUsually it is more convenient to use the equivalent generalized likelihood ratio (GLR) statistic\n\\[\n\\lambda^*(\\mathbf{y})\n=\n\\frac{ \\sup_{\\theta\\in\\Theta} L(\\theta_1; \\mathbf{Y}) }{\n       \\sup_{\\theta\\in\\Theta_0} L(\\theta_0; \\mathbf{Y}) }\n\\]\n(Note the change in subscript on \\(\\sup\\) in the numerator). Also note that there is no proof that the GLR is UMP. But, as it happens, several well-known and widely used statistical tests are GLRs: one and two-sided t-tests for normal means, \\(\\chi^2\\) test for normal variance, F-test for normal variance, F-tests for comparing nested regression models, Pearson’s goodness-of-fit \\(\\chi^2\\) test, etc.\nThere is a duality between hypothesis testing and confidence intervals. If one has derived a \\((1-\\alpha)100\\%\\) confidence interval for a single unknown parameter \\(\\theta\\) and then wants to test \\(H_0: \\theta = \\theta_0\\) against a two sided alternative \\(H_1: \\theta \\ne \\theta_0\\) at level \\(\\alpha\\), one should simply check whether \\(\\theta_0\\) lies within the confidence interval and then accept \\(H_0\\) or (if outside the interval) reject \\(H_0\\). The duality extends straightforwardly to the multiparameter case.\nFor a discussion of sequential testing or multiple testing, see section 4.5 and 4.6.\n\n\n5 Asymptotic Analysis\nIn many situations an estimator might be a complicated function of the data, and the exact calculation of its characteristics (e.g., bias, variance, and MSE) might be quite difficult if possible at all. Nevertheless, rather often, good approximate properties can be obtained if the sample size is “sufficiently large.” Intuitively, everyone would agree that a “good” estimator should improve and approach the estimated parameter as the sample size increases: the larger the poll, the more representative its results are. Conclusions based on a large sample have more ground than those based on a small sample. But what is the formal justification for this claim?\nSimilar issues arise with confidence intervals and hypothesis testing. To construct a confidence interval, we should know the distribution of the underlying statistic, and that may be difficult to calculate. Can we derive at least an approximate confidence interval, without knowing the exact distribution of the statistic for a “sufficiently large” sample? In hypothesis testing we need the distribution of the test statistic \\(T(\\mathbf{Y})\\) under the null hypothesis in order to find the critical value for a given significance level \\(\\alpha\\) by solving \\(P_{\\theta_0} \\left( T(\\mathbf{Y}) \\ge C \\right) = \\alpha\\) or to calculate the p-value. The statistic \\(T(\\mathbf{Y})\\) might be a complicated function of the data whose distribution is hard to derive, if doable at all. Is it possible then to find its simple asymptotic approximation?\nStatisticians like to answer these questions by considering a sequence of similar problems that differ from one another only in their sample size. Let \\(\\hat{\\theta}_n = \\hat{\\theta}(Y_1, \\ldots, Y_n)\\) be an estimator for \\(\\theta\\) based on a sample of size \\(n\\). We can then study the asymptotic properties fo the sequence \\(\\{\\hat\\theta_n\\}\\) as \\(n\\) increases. A thorough history (not included here) of prerequisites includes discussion of convergence of a sequence of real numbers, then a sequence of vectors, then a sequence of functions, and finally the convergence of a sequence of estimators \\(\\{\\hat\\theta_n\\}\\) to a parameter \\(\\theta\\). We consider three types of convergence of estimators: in mean-squared error, in probability, and in distribution.\nRecall that the most common measure of goodness-of-estimation for \\(\\hat\\theta_n\\) is its mean squared error \\(MSE(\\hat\\theta_n, \\theta) = E(\\hat\\theta_n-\\theta)^2\\). We say that \\(\\hat\\theta_n\\) converges in MSE to \\(\\theta\\) if \\(MSE(\\hat\\theta_n, \\theta) \\rightarrow 0\\) as the sample size \\(n\\) increases. We denote this as \\(\\hat\\theta_n \\stackrel{\\textrm{MSE}}{\\rightarrow} \\theta\\). Consistency in MSE, however, might be hard to prove and sometimes even too strong to ask for.\nA more realistic approach is then to relax it by the weaker convergence in probability. The sequence of random variables \\(\\{Y_n\\}\\) is said to converge to \\(c\\) if for any \\(\\varepsilon &gt; 0\\) the probability \\(P(|Y_n-c|&gt;\\varepsilon) \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\). We write this as \\(Y_n \\stackrel{p}{\\rightarrow} c\\). With a proof of a continuous mapping theorem — i.e., that \\(g(Y_n) \\stackrel{p}{\\rightarrow} g(c)\\) — we an define consistency in probability of an estimator (usually just called consistency for brevity) as \\(\\hat\\theta_n \\stackrel{p}{\\rightarrow} \\theta\\) as \\(n \\rightarrow \\infty\\).\nThe weak law of large numbers (WLLN) implies that a sample mean is always a consistent estimator of the expectation. Let \\(Y_1, Y_2, \\ldots\\) be i.i.d. random variables with finite second moment. Then \\(\\bar{Y}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i \\stackrel{p}{\\rightarrow} EY\\) as \\(n \\rightarrow \\infty\\). The proof follows immediately from Chebyshev’s inequality: \\(P(|\\bar{Y}_n-\\mu| \\ge \\varepsilon) \\le \\textrm{Var}(\\bar{Y}_n) / \\varepsilon^2 = \\textrm{Var}(Y)/(n\\varepsilon^2) \\rightarrow 0\\). Although consistency itself is important, the rate of convergence also matters. One would obviously prefer a consistent estimator with a faster convergence rate. We will not consider these issues in detail but just mention that in many situations, \\(n^{-1/2}\\) is the best available convergence rate for estimating a parameter; such estimator are called \\(\\sqrt{n}\\)-consistent.\nConsistency (in MSE or probability) considered in the previous sections still does not say anything about the asymptotic distribution of an estimator needed, for example, to construct an approximate confidence interval for an unknown parameter or to calculate the critical value in hypothesis testing. What we need is convergence in distribution. A sequence of random variables \\(Y_1, Y_2, \\ldots\\) with cdf’s \\(F_1(\\cdot), F_2(\\cdot),\\ldots\\) is said to converge in distribution to a random variable \\(Y\\) with cdf \\(F(\\cdot)\\) if \\(\\lim_{n\\rightarrow\\infty}F_n(x) = F(x)\\) for every continuity point \\(x\\) of \\(F(\\cdot)\\). We denote this by \\(Y_n \\stackrel{\\mathscr{D}}{\\rightarrow} Y\\). Convergence in distribution is important because it enables us to approximate probabilities. Quite often \\(F_n\\) is either complex or not explicitly known, while \\(F\\) is simple and known. If \\(Y_n \\stackrel{\\mathscr{D}}{\\rightarrow} F\\), we can use \\(F\\) to calculate approximate probabilities related to \\(Y_n\\).\nPrime examples of limiting distributions include Poisson (as an approximation for binomial random variables with large \\(n\\) and small \\(p\\)), the normal distribution (for properly scaled means), the exponential distribution (for the minimum of many random variables), and the extreme value distribution (for the distribution of extremely large values). The fact that these distributions are obtained as a limit is the reason why they are so useful.\nThe Central Limit Theorem (CLT): Suppose \\(Y_1, Y_2, \\ldots\\) are i.i.d. random variables with the common (finite) expectation \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\bar{Y}_n = \\frac{1}{n}\\sum_{i=1}^nY_i\\). Then as \\(n \\rightarrow \\infty\\), \\(\\sqrt{n}(\\bar{Y}_n - \\mu) \\stackrel{\\mathscr{D}}{\\rightarrow} \\mathscr{N}(0,\\sigma^2)\\). This is a formal way to say that for sufficiently large \\(n\\), the distribution of a sample mean \\(\\bar{Y}_n\\) is approximately normal with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). Note that it is always true that \\(E\\bar{Y}_n=\\mu\\) and \\(\\textrm{Var}(\\bar{Y}_n)=\\sigma^2/n\\). The CLT adds that the distribution of \\(\\bar{Y}_n\\) is approximately normal.\nThe CLT is a real refinement of the WLLN. Both consider the asymptotic behavior of \\(\\bar{Y}_n\\). The WLLN only says that \\(\\bar{Y}_n \\stackrel{p}{\\rightarrow} \\mu\\), or that \\(\\bar{Y}_n\\) is very close to \\(\\mu\\) when \\(n\\) is large, but does not tell us how close. The CLT gives a precise answer. It is as if we look at the cdf of \\(\\bar{Y}_n\\) through a magnifying glass: we center it around \\(\\mu\\) and magnify by \\(\\sqrt{n}\\). As \\(n\\) increases, the cdf looks more and more smooth, and becomes closer and closer to a normal cdf.\nAn estimator \\(\\hat{\\theta}_n\\) is called a consistent asymptotically normal (CAN) estimator of \\(\\theta\\) if \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\stackrel{\\mathscr{D}}{\\rightarrow} \\mathscr{N}\\left(0,\\sigma^2(\\theta)\\right)\\) for some asymptotic variance \\(\\sigma^2(\\theta)\\) that may generally depend on \\(\\theta\\). If \\(\\hat{\\theta}_n\\) is a CAN estimator of \\(\\theta\\), then it is also \\(\\sqrt{n}\\)-consistent. A theorem heavily based on the Delta Method explains why so many estimators are CAN: if \\(\\hat{\\theta}_n\\) is a CAN estimator of \\(\\theta\\) and \\(g(\\cdot)\\) is any differentiable function with \\(g'(\\theta)\\ne0\\), then \\(g(\\hat{\\theta}_n)\\) is a CAN estimator of \\(g(\\theta)\\).\nThe notations asymptotic mean and asymptotic variance for \\(\\theta\\) and \\(\\sigma^2(\\theta)\\) might be somewhat misleading: although it is tempting to think that for a CAN estimator \\(\\hat{\\theta}_n\\) of \\(\\theta\\), \\(E\\hat{\\theta}_n \\rightarrow \\theta\\) and \\(\\textrm{Var}(\\hat{\\theta}_n) \\rightarrow \\sigma^2(\\theta)\\) as \\(n\\) increases, generally it is not true. The convergence in distribution does not necessarily yield convergence of moments.\nAsymptotical normality of an estimator can be used to construct an approximate normal confidence interval when its exact distribution is complicated or even not completely known.\n\\[\nP \\left( \\hat{\\theta}_n - z_{\\alpha/2} \\frac{\\sigma(\\theta)}{\\sqrt{n}} \\le \\theta \\le \\hat{\\theta}_n - z_{\\alpha/2} \\frac{\\sigma(\\theta)}{\\sqrt{n}} \\right)\n=\nP\\left( - z_{\\alpha/2} \\le \\sqrt{n} \\frac{\\hat{\\theta}_n - \\theta}{\\sigma(\\theta)} \\le z_{\\alpha/2} \\right)\n\\rightarrow\n1 - \\alpha\n\\]\nThis equation is not much use on its own since the asymptotic variance \\(\\sigma^2(\\theta)\\) generally also depends on \\(\\theta\\) or is an additional unknown parameter. Intuitively, a possible approach would be to replace \\(\\sigma(\\theta)\\) by some estimator and hope that it will not strongly affect the resulting asymptotic coverage probability. As it happens, the idea indeed works if \\(\\hat{\\sigma}_n\\) is any consistent estimator of \\(\\sigma(\\theta)\\).\nAn example is given whereby \\(\\hat{\\lambda}\\) estimates \\(\\lambda\\) but also \\(\\hat{\\theta}\\) estimates \\(\\theta = 1/\\lambda\\) and \\(\\hat{p}\\) estimates \\(p=e^{-\\lambda}\\). Asymptotically normal confidence intervals are constructed for \\(\\lambda\\), \\(\\theta\\), and \\(p\\). These approaches, unsurprisingly, lead to different results — there is no unique way to construct a confidence interval (as already mentioned in Chapter 3). Which one is better? One likely prefers a confidence interval with shorter expected length. For asymptotic confidence intervals, another (probably even more important) issue is the goodness of normal approximation for distributions of the underlying statistics. Another possible approach to deal with an unknown asymptotic variance \\(\\sigma^2(\\theta)\\) of a CAN estimator \\(\\hat{\\theta}_n\\) when constructing an asymptotic confidence interval for \\(\\theta\\) is by the use of the variance stabilizing transformation \\(g(\\theta)\\) whose asymptotic variance \\(g'(\\theta)\\sigma^2(\\theta)\\) does not depend on \\(\\theta\\). Evidently, this transformation \\(g(\\theta)\\) should satisfy \\(g'(\\theta) = c/\\sigma(\\theta)\\) for a chosen constant \\(c\\). A \\((1-\\alpha)100\\%\\) asymptotic confidence interval for the resulting \\(g(\\theta)\\) is then \\(g(\\hat{\\theta}_n) \\pm z_{\\alpha/2}\\frac{c}{\\sqrt{n}}\\), which can be used to construct an asymptotic confidence interval for \\(\\theta\\) itself.\nMaximum likelihood estimators were introduced in Chapter 2, but the motivation for them there was mostly heuristic. The real justification for the MLE, however, is not just “philosophical.” It is genuinely a good estimation approach and its theoretical justification is based on its strong asymptotic properties: the MLE of \\(\\theta\\) is CAN with the minimum possible asymptotic variance among all “regular” estimators, that is, \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\stackrel{\\mathscr{D}}{\\rightarrow} N(0, I^*(\\theta)^{-1})\\). Note that the asymptotic variance of the MLE is exactly the Cramer-Rao variance lower bound for unbiased estimators.\nSection 5.10 provides an extension of the asymptotic results for MLEs to general M-estimators.\n\n\n6 Bayesian Inference\nSo far we have treated \\(\\theta\\) as a fixed (unknown) constant. Its estimation and inference was based solely on the observed data, which was considered as a random sample from the set of all possible samples of a given size from a population (sample space). The inference about \\(\\theta\\) was essentially based on averaging over all those (hypothetical!) samples, which might raise conceptual questions about its appropriateness for a particular single observed sample.\nIn the Bayesian framework, the initial uncertainty about \\(\\theta\\) is modeled by considering \\(\\theta\\) as a random variable with a certain prior distribution that expresses one’s prior beliefs about the plausibility of every possible value of \\(\\theta \\in \\Theta\\). At first glance, it might seem somewhat confusing to treat \\(\\theta\\) as a random variable – one does not really assume that its value randomly changes from experiment to experiment. The problem is with the interpretation of randomness: probability here is a measure of belief. Philosophical disputes on the nature of probability has a long history but, luckily, the corresponding formal probability calculus does not depend on its interpretation.\nThus, in addition to the observed data \\(\\mathbf{y}=(y_1,\\ldots,y_n)\\), a Bayesian specifies a prior distribution \\(\\pi(\\theta)\\), \\(\\theta \\in \\Theta\\) for the unknown parameter \\(\\theta\\). The model distribution for the data \\(f_\\theta(\\mathbf{Y})\\) is treated as a conditional distribution \\(f(\\mathbf{y}|\\theta)\\) of \\(\\mathbf{Y}\\) given \\(\\theta\\). Thus, the Bayesian specifies before observing the data a joint distribution for the parameter and the data. The inference on \\(\\theta\\) is based then on the posterior distribution \\(\\pi(\\theta | \\mathbf{Y}=\\mathbf{y})\\) (denoted \\(\\pi(\\theta|\\mathbf{y})\\) for brevity) calculated by Bayes’ Theorem:\n\\[\n\\pi(\\theta | \\mathbf{y}) = \\frac{f(\\mathbf{y}|\\theta)\\pi(\\theta)}{f(\\mathbf{y})}\n\\]\nwhere the marginal distribution in the denominator \\(f(\\mathbf{y}) = \\int_\\Theta f(\\mathbf{y}|\\theta')\\pi(\\theta')d\\theta'\\) is a normalizing constant that does not depend on \\(\\theta\\). Note also that \\(f(\\mathbf{y}|\\theta) = f_\\theta(\\mathbf{y})\\) is a likelihood function \\(L(\\theta; \\mathbf{y})\\) and thus the posterior is proportional to the likelihood and the prior: \\(\\pi(\\theta|\\mathbf{y}) \\propto L(\\theta; \\mathbf{y})\\pi(\\theta)\\).\nBayesian inference in inherently conditional on a specific observed sample rather than the result of averaging over the entire sample space of the experiment. In this sense some consider it as having a more “natural” interpretation and as more “conceptually appealing.” The main criticism with respect of Bayesians, however, is due to the use of a prior which is the core of the approach. A choice for \\(\\pi(\\theta)\\) is subjective and represents one’s beliefs or information about \\(\\theta\\) before the experiment. Different priors can change the results of a Bayesian analysis while in many practical cases, subjective quantification of uncertainty in terms of a prior distribution might be difficult.\nA prior distribution should ideally be defined solely on one’s prior beliefs and knowledge, but such situations are rare in the real world. One can often specify a (usually parametric) class or family of possible priors that is sufficiently “rich” to include priors that reflect various possible scenarios for the problem at hand and at the same time allow simple computation of the corresponding posterior distribution. Conjugate priors are probably the most known and used of these classes:\nA (parametric) class of priors \\(\\mathscr{P}\\) is called conjugate for the data if for any prior \\(\\pi(\\theta) \\in \\mathscr{P}\\), the corresponding posterior distribution \\(\\pi(\\theta|\\mathbf{y})\\) also belongs to \\(\\mathscr{P}\\).\nWhen choosing a parametric (e.g., conjugate) class of priors, one should still specify the corresponding parameters of a prior (called hyperparameters). Often it can be done subjectively—to define parameters of a distribution is after all a much easier task than to elicit the entire prior distribution itself!\nUnfortunately, in most cases the resulting marginal prior \\(\\pi(\\theta)\\) and the posterior \\(\\pi(\\theta|\\mathbf{y})\\) for hierarchical prior models cannot be obtained in closed form. However, the existing Bayes computational techniques allow one to derive them numerically in an efficient way.\nAlternatively, one can estimate the unknown hyperparameters of the prior from the data. Such an approach is known as empirical Bayes and utilizes the marginal distribution of the data: \\[\nf_\\alpha(\\mathbf{y}) = \\int_\\Theta f(\\mathbf{y}|\\theta)\\pi_\\alpha(\\theta)d\\theta\n\\]\nThe density \\(f_\\alpha(\\mathbf{y})\\) does not depend on \\(\\theta\\) but only on the unknown hyperparameters \\(\\alpha\\) of the prior that can be estimated by any classical method, e.g., maximum likelihood or method of moments. Although empirical Bayes essentially contradicts the core Bayesian principle that a prior should not depend on the data, it is nevertheless a useful approach in many practical situations.\nSuppose we know nothing about \\(\\theta \\in \\Theta\\). A natural way to model ignorance would then be to place a uniform prior distribution for \\(\\theta\\) over the parameter space \\(\\Theta\\). Such a prior is called noninformative or objective. The idea looks appealing but unfortunately cannot be applied in common situations, where the parameter space \\(\\Theta\\) is an infinite measure, e.g. for the normal mean where \\(\\theta = \\mu\\) and \\(\\Theta = (-\\infty, \\infty)\\).\nNevertheless, let’s “ignore” this basic probabilistic law (at least for a while) and consider a prior density \\(\\pi(\\theta) = c\\) over \\(\\Theta\\) for some fixed \\(c&gt;0\\). It is obviously an improper density since \\(\\int_\\Theta \\pi(\\theta)d\\theta = \\infty\\). However, the corresponding posterior density \\[\n\\pi(\\theta|\\mathbf{y})\n=\n\\frac{f(\\mathbf{y}|\\theta)c}{\\int_\\Theta f(\\mathbf{y}|\\theta)c d\\theta}\n=\n\\frac{L(\\theta;\\mathbf{y})}{\\int_\\Theta L(\\theta;\\mathbf{y}) d\\theta}\n\\] can still be proper (provided that the integral in the denominator is finite for all \\(\\mathbf{y}\\)), and, moreover, it does not depend on a specific \\(c\\). Thus, although the proposed prior is improper, it can be used in Bayesian inference, as long as the corresponding posterior is proper.\nThere is, however, another, probably a more serious problem with using uniform distributions (proper or improper) as a general remedy for noninformative priors. The lack of invariance of a uniform distribution under transformations leads to the logical paradox and raises then the question whether one should impose a uniform distribution on the original scale of \\(\\theta\\) or the transformed one.\nWe would like to construct a noninformative prior that will be invariant under one-to-one differentiable transformations \\(\\xi = \\xi(\\theta)\\). By standard probabilistic arguments, a prior or \\(\\xi\\) is \\(\\pi_\\xi(\\xi) = \\pi_\\theta(\\theta(\\xi)) \\cdot \\vert \\theta'_\\xi \\vert\\). The Jeffreys prior for \\(\\theta\\) is \\[\n\\pi(\\theta) = \\sqrt{I^*_\\theta(\\theta)}\n\\] where \\(I^*_\\theta(\\theta)\\) is the Fisher information number of \\(f(y|\\theta)\\). Note that in terms of \\(\\xi\\), \\(I^*_\\xi(\\xi) = I^*_\\theta(\\theta)\\cdot(\\theta'_\\xi)^2\\) and, therefore, \\(\\sqrt{I^*_\\xi(\\xi)} = \\sqrt{I^*_\\theta(\\theta(\\xi))} \\cdot \\vert \\theta'_\\xi \\vert\\). Hence, such class of priors is transformation invariant.\nAn immediate extension of Jeffreys noninformative prior for the multiparameter case is \\[\n\\pi(\\theta) \\propto \\sqrt{\\text{det}(I^*(\\theta))}\n\\]\nGiven the posterior distribution \\(\\pi(\\theta|\\mathbf{y})\\), a Bayes point estimator \\(\\hat{\\theta}\\) for \\(\\theta\\) depends on the measure of goodness-of-estimation. Similar to non-Bayesian estimation, the most conventional measure in the squared-error \\((\\hat{\\theta} - \\theta)^2\\). The difference between the two approaches is that Bayesians consider the expectation of a squared error w.r.t. the posterior distribution \\(\\pi(\\theta|\\mathbf{y})\\) rather than the data distribution \\(f(\\mathbf{y}|\\theta)\\) as in the MSE.\nThe Bayes point estimator for \\(\\theta\\) with respect to the squared error i the posterior mean \\(\\hat{\\theta} = E(\\theta|\\mathbf{y})\\). The Bayes point estimator for \\(\\theta\\) with respect to the absolute error is the posterior median \\(\\hat{\\theta} = \\text{Med}(\\theta|\\mathbf{y})\\).\nBayesian approach to interval estimation is conceptually straightforward and simple. The posterior distribution of the unknown parameter of interest essentially contains all the information available on it from the data and prior knowledge. A Bayesian analog of confidence intervals (or sets in general) are credible sets: A \\(100(1-\\alpha)\\%\\) credible set for \\(\\theta\\) is a subset (usually interval) \\(S_y \\subset \\Theta\\) such that \\(P(\\theta \\in S_y | \\mathbf{y}) = \\int_{S_y} \\pi(\\theta|\\mathbf{y}) d\\theta = 1 - \\alpha\\).\nCredible sets can usually be easily calculated from a posterior distribution in contrast to their frequentist counterparts. It is important to emphasize that the probability is on \\(\\theta\\) rather than on a set that makes probabilistic interpretation of Bayesian credible sets more straightforward.\nSimilar to confidence intervals, a credible set can usually be chosen in a nonunique way. It may be desirable then to find a credible set of a minimal size. Such a set would evidently include the “most likely” values of \\(\\theta\\). This leads one to the definition of highest posterior density sets.\nSimple hypotheses: Suppose that given the data \\(\\mathbf{Y} \\sim f_\\theta(\\mathbf{y})\\), we want to test two simple hypotheses \\(H_0: \\theta = \\theta_0\\) vs. \\(H_1: \\theta = \\theta_1\\). In such a setup, the parameter space \\(\\Theta\\) is essentially reduced to a set of two points: \\(\\Theta = \\{\\theta_0, \\theta_1\\}\\). A Bayesian defines a prior distribution \\(\\pi(\\cdot)\\) on \\(\\Theta\\), where \\(\\pi(\\theta_0) = \\pi_0\\) and \\(\\pi(\\theta_1) = \\pi_1 = 1 - \\pi_0\\), and calculates the corresponding posterior probabilities \\(\\pi(\\theta_0|\\mathbf{y})\\) and \\(\\pi(\\theta_1|\\mathbf{y})\\). In fact, it is often more convenient to work with the posterior odds ratio (or simply “posterior odds”) of \\(H_1\\) and \\(H_0\\), that is \\(\\pi(\\theta_1|\\mathbf{y}) / \\pi(\\theta_0|\\mathbf{y})\\). In particular, one does not need then to calculate the normalizing constant in the posterior distribution.\nThus, for two simple hypotheses, the posterior odds is the multiplication of the likelihood ratio and the prior odds. In particular, assuming both hypotheses to be equally likely a priori (i.e., \\(\\pi_0 = \\pi_1 = 0.5\\)), the posterior odds is identical to the likelihood ratio. Clearly, the larger the posterior odds, the larger \\(P(\\theta_1|\\mathbf{y})\\) and the more evidence there is against \\(H_0\\) in favor of \\(H_1\\). A Bayesian then sets a threshold \\(C\\) and rejects the null if the posterior odds is larger than \\(C\\).\nThe ideas for testing simple hypotheses can be extended to composite hypotheses. Let \\(H_0: \\theta \\in \\Theta_0\\) vs. \\(H_1: \\theta \\in \\Theta_1\\) where \\(\\Theta_0 \\cap \\Theta_1 = \\emptyset\\) and \\(\\Theta_0 \\cup \\Theta_1 = \\Theta\\). Consider testing a point null hypothesis about a continuous parameter \\(\\theta\\): \\(H_0: \\theta = \\theta_0\\) vs \\(H_1: \\theta \\ne \\theta_0\\). Defining a continuous prior \\(\\pi(\\theta)\\) on the entire parameter space \\(\\Theta\\) is meaningless in this case since it assigns a zero prior (and, therefor, zero posterior!) probability for the point null hypothesis \\(\\theta = \\theta_0\\). There are several Bayesian approaches to tackle this problem.\nSimilar to frequentist asymptotics, it is interesting to consider asymptotic properties of Bayesian approach when a sample size \\(n\\) tends to infinity. Intuitively, the more data we have, the more information we can elicit from it, the less should be the impact of a prior belief on inference. In this sense Bayesian and frequentist inferences should somehow merge as \\(n \\rightarrow \\infty\\). The Bernstein-von Mises theorem shows that under general regularity conditions, the posterior distribution \\(\\pi(\\theta|\\mathbf{y})\\) approaches a normal distribution centered at the MLE \\(\\hat{\\theta}_n\\) and the variance \\(I^{-1}(\\theta_0)\\), where \\(I(\\theta_0)\\) is the Firsher information number at the true \\(\\theta_0\\). Simply stated, for large samples, the posterior distribution of \\(\\theta\\) is approximately normal with mean \\(\\hat{\\theta}_n\\) and variance \\(I^{-1}(\\theta_0) = (nI^*(\\hat{\\theta}_n))^{-1}\\).\n\n\n7 Elements of Statistical Decision Theory\nWhat is statistical inference? in fact, it can generally be viewed as making a conclusion about an underlying random process from the data generated by it. Statistical inference can, therefore, be considered as a decision making under uncertainty based on analysis of data. Each decision has consequences as there is a price for an error, and the goal is then to find a proper rule for making a “good” decision. All three main problems of statistical inference – namely point estimation, confidence intervals, and hypothesis testing – can be put within the general statistical decision theory framework.\nThe main goal of statistical decision theory is to find the “best” decision rule \\(\\delta = \\delta(\\mathbf{Y})\\). How to measure the “goodness” of a rule? A first naive way would be to consider its loss \\(L(\\theta, \\delta)\\) but it is a random variable since \\(\\delta(\\mathbf{Y})\\) depends on a random sample. We define then the expected loss of a rule \\(\\delta\\) called its risk function.\nWe can compare two rules \\(\\delta_1\\) and \\(\\delta_2\\) and choose the one with the smaller risk. The problem is, however, that similar to MSE in the estimation setup, the risk function depends also on the specific value of the unknown \\(\\theta\\). Hence, we can claim that \\(\\delta_1\\) outperforms \\(\\delta_2\\) only if its risk is uniformly smaller for all \\(\\theta \\in \\Theta\\). A decision rule \\(\\delta\\) is inadmissible (w.r.t. a givel loss and class of rules) if there exists another decision rule \\(\\tilde{\\delta}\\) from the class such that \\(R(\\theta_0,\\tilde{\\delta}) \\le R(\\theta,\\delta)\\) for all \\(\\theta \\in \\Theta\\) and there exists \\(\\theta_0 \\in \\Theta\\) for which the inequality is strict. A decision rule is admissible if it is not inadmissible, that is, it cannot be uniformly improved. Generally, however, it might be hard (if possible) to check the admissibility (or inadmissibility) of a given rule directly. Remarkably, admissibility of a wide variety of rules can be easily established through their Bayesian interpretation as we a bit later.\nAdmissibility is quite a weak requirement – it essentially declares the absence of a negative property, rather than possession of a positive one. Admissibility actually only means that there exists some value(s) of \\(\\theta\\) where the rule cannot be improved, but it still does not tell anything about the goodness of the rule for other values of \\(\\theta\\).\nWe would obviously like to find the rule that outperforms all other counterparts within a given class uniformly over all \\(\\theta \\in \\Theta\\). However, as we have discussed, in most realistic situations it is impossible unless the action set is very restricted. What can be done then? Instead of looking at a risk function pointwise at each \\(\\theta\\) we need some global criteria over all \\(\\theta \\in \\Theta\\). One possible approach is to consider the worst case scenario, that is, \\(\\sup_{\\theta\\in\\Theta} R(\\theta,\\delta)\\) and seek the rule that will minimize the maximal possible risk. This rule is called minimax.\nBy its definition, the minimax criterion is conservative, where one wants to ensure a maximal protection in the worst possible case essentially ignoring all other less pessimistic scenarios. Minimax rules may not be unique. Moreover, they are not even necessarily admissible (although usually they are). Note that, if a minimax rule is unique, it is admissible.\nAn alternative global risk criterion to the conservative minimax risk approach is a weighted average risk \\(\\int_\\Theta R(\\theta,\\delta)\\pi(\\theta)d\\theta\\) (for continuous \\(\\pi(\\theta)\\)) or \\(\\sum_i R(\\theta_i, \\delta)\\pi(\\theta_i)\\) (for discrete \\(\\pi(\\theta)\\)) of a rule \\(\\delta\\), where the weight function \\(\\pi(\\theta) \\ge 0\\) gives more emphais to more “relevant” values of \\(\\theta\\) and reduces the influence of others. In this sense \\(\\pi(\\theta)\\) can be treated as a prior distribution of \\(\\theta\\) over \\(\\Theta\\) and the resulting weighted average risk is thus called a Bayes risk of a rule \\(\\delta\\).\nThe Bayes rule \\(\\delta^*_\\pi = \\delta^*_\\pi(\\mathbf{Y})\\) (w.r.t. a given loss, class of rules, and a prior) is a decision rule that minimizes the Bayes risk \\(\\rho(\\pi,\\delta)\\). The Bayes risk is not larger than the minimax risk w.r.t. the same loss.\nThe name “Bayes risk” is, in fact, somewhat misleading since the risk function \\(R(\\theta,\\delta)\\) being the average loss over all possible \\(\\mathbf{y} \\in \\Omega\\) is not considered as a relevant object by the “Bayesians” who restrict attention to quantities that are conditioned on the observed sample \\(\\mathbf{y}\\). However, this is a common terminology. Besides, one can argue that a weight function \\(\\pi(\\theta)\\) can be interpreted not necessarily as a prior but as a measure of importance of different values of \\(\\theta\\). Moreover, as we shall see next, a “pure” Bayesian approach that utilizes a posterior expected loss \\(E(L(\\theta,\\delta)|\\mathbf{y})\\) leads to the same rules. This equivalency is usually used for deriving Bayes rules.\nA natural Bayesian way to find “the best” rule w.r.t. the loss \\(L(\\theta,a)\\) and the prior \\(\\pi(\\theta)\\) is to consider a Bayesian (posterior) expected loss: \\[\nr(a,\\mathbf{y}) = E(L(a,\\theta)|\\mathbf{y})\n\\] and to find a rule (also called a Bayes action) \\(\\delta^*_\\pi = \\delta^*_\\pi(\\mathbf{y})\\) that minimizes \\(r(a,\\mathbf{y})\\) over \\(a \\in \\mathscr{A}\\) for a given \\(\\mathbf{y}\\). In particular, for the quadratic loss \\(L(\\theta,a) = (\\theta - a)^2\\) and the absolute error loss \\(L(\\theta,a) = |\\theta - a|\\) the corresponding Bayesian actions are respectively the posterior mean \\(E(\\theta|\\mathbf{y})\\) and the posterior median \\(\\text{Med}(\\theta|\\mathbf{y})\\).\nWe now show [omitted here] that the Bayes rule that minimizes the Bayes risk w.r.t. a certain loss function and a prior coincides with the Bayes action that minimizes the corresponding expected posterior loss. From now on we will not distinguish between ayesian rules and actions (though they are still conceptually different!). Based on this equivalency, Bayes rules are typically derived through minimizing the posterior expected loss.\nThe remarkable property of Bayes rules is that they are typically admissible as demonstrated in the following two theorems. If a Bayes rule \\(\\delta^*_\\pi\\) is unique and the Bayes risk \\(\\rho(\\pi,\\delta^*_\\pi) &lt; \\infty\\), then \\(\\delta^*_\\pi\\) is admissible. Let \\(\\delta^*_\\pi\\) be a Bayes rule w.r.t. a prior \\(\\pi(\\theta)\\). Suppose that one of the two following conditions holds: (1) \\(\\Theta\\) is finite and \\(\\pi(\\theta) &gt; 0\\) for all \\(\\theta \\in \\Theta\\), or (2) \\(\\Theta\\) is an interval (possibly infinite), \\(\\pi(\\theta) &gt; 0\\) for all \\(\\theta \\in \\Theta\\), the risk function \\(R(\\theta,\\delta)\\) is a continuous function of \\(\\theta\\) for all \\(\\delta\\), and the Bayes risk \\(\\rho(\\pi,\\delta^*_\\pi)&lt;\\infty\\). Then, \\(\\delta^*_\\pi\\) is admissible.\nThese results show that it is easy to verify admissibility of a rule \\(\\delta\\) once one can find a prior such that \\(\\delta\\) is the corresponding Bayes rule. Note that the prior does not have to be proper as long as the corresponding Bayes risk is finite.\nAs we have mentioned, derivation of a minimax rule is generall a nontrivial problem. It turns out that a Bayesian approach can also be useful for finding minimax risks and rules. Recall that or any prior \\(\\pi(\\theta)\\), the Bayes risk of the corresponding Bayes rule \\(\\delta^*_\\pi\\) is not larger than the minimax risk \\(\\rho(\\pi, \\delta^*_\\pi)\\) and therefore \\(\\rho(\\pi, \\delta^*_\\pi)\\) is a lower bound for the minimax risk. On the other hand, for any rule (and for \\(\\delta^*_\\pi\\) in particular) its maximum risk is evidently an upper bound for the minimax risk. We would naturally seek the least favorable prior which implies the largest Bayes risk to make the bounds for the minimax risk as tight as possible. A prior \\(\\pi(\\theta)\\) is called a least favorable prior if \\(\\rho(\\pi, \\delta^*_\\pi) \\ge \\rho(\\tilde{\\pi}, \\delta^*_\\tilde{\\pi})\\).\nLet \\(\\delta^*_\\pi\\) be the Bayes rule corresponding to a (proper) prior \\(\\pi(\\theta)\\) and \\(\\rho(\\pi, \\delta^*_\\pi) = \\sum_{\\theta \\in \\Theta} R(\\theta, \\delta^*_\\pi)\\). Then \\(\\delta^*_\\pi\\) is a minimax rule and \\(\\pi(\\theta)\\) is a least favorable prior. An important corrolary provides a particular case that is useful for deriving minimax rules: the theorem holds if \\(R(\\theta, \\delta^*_\\pi)\\) is constant. While this theorem and corrolary can be applied only for proper least favorable priors, there are examples of rules that can be viewed as Bayes w.r.t. improper priors. An improper prior can often be considered as a limiting distribution of a sequence of proper priors."
  },
  {
    "objectID": "posts/101-gumble/index.html",
    "href": "posts/101-gumble/index.html",
    "title": "Gumble Distribution in Discrete Choice Models",
    "section": "",
    "text": "This short article seeks to describe how the Gumbel (or Type-1 Generalized Extreme Value) distribution’s use in probabilistic discrete choice models leads to a closed-form expression for choice probabilities.\nA standard framework for a probabilistic discrete choice model (DCM) reflecting a random utility theory (RUT) in a cross-sectional setting is as follows:\n\nThe utility that a decision maker obtains from alternative \\(j\\) is \\(U_j\\), for \\(j = 1, \\ldots, J\\).\nThe decision maker chooses the alternative that provides the greatest utility: choose alternative \\(k\\) if and only if \\(U_k &gt; U_j \\hspace{1ex} \\forall j \\ne k\\).\nUtility is decomposed to reflect the researcher’s inability to understand or observe all factors that impact choices: \\(U_j = V_j + \\varepsilon_j\\) where the joint density of the random vector \\(\\mathbf{\\varepsilon} = (\\varepsilon_1, \\ldots, \\varepsilon_J)\\) for the decision maker is \\(f_\\mathbf{\\varepsilon}(\\mathbf{\\varepsilon})\\).\n\nThe probability that the decision maker chooses alternative \\(k\\) is:\n\\[\\begin{align*}\n    P_{k}\n    &= \\text{Prob}\\left( U_k &gt; U_j \\; \\forall j \\ne k \\right)  &\\\\\n    &= \\text{Prob}\\left( V_k + \\varepsilon_k &gt; V_j + \\varepsilon_j \\; \\forall j \\ne k \\right)  &\\\\\n    &= \\text{Prob}\\left( \\varepsilon_j &lt; \\varepsilon_k + V_k - V_j \\; \\forall j \\ne k \\right)  &\\\\\n\\end{align*}\\]\nA common simplifying assumption about the joint distribution of vector \\(\\mathbf{\\varepsilon}\\) is that its components are independent and identically distributed as Gumbel random variables. Specifically, the cumulative distribution function for each \\(\\varepsilon_{j}\\) is: \\[\n\\text{Prob}(\\varepsilon_j \\leq x) = \\exp(-\\exp(-x)) = e^{-e^{-x}}\n\\] The Gumbel density and distribution are plotted below (in black) alongside the Normal (in dashed red) for comparison.\n\n\n\n\n\n\n\n\n\n\nBecause each \\(\\varepsilon_j\\) is assumed to be independently distributed, the probability that the decision maker chooses alternative \\(k\\) for a given value of \\(\\varepsilon_k\\) can be written as the product of the \\(J-1\\) conditional distributions:\n\\[\\begin{align*}\n    P_k | \\varepsilon_k\n    &= \\prod_{j \\ne k} \\exp(-\\exp(-\\varepsilon_k)) &\\\\\n    &= \\exp \\left( -\\sum_{j \\ne k} \\exp \\left( - ( \\varepsilon_k + V_k - V_j) \\right) \\right) &\\\\\n    &= \\exp \\left( - \\exp(\\varepsilon_k) \\times \\sum_{j \\ne k} \\exp \\left( V_j - V_k) \\right) \\right)\n\\end{align*}\\]\nBut of course \\(\\varepsilon_k\\) is not given, so to obtain the desired probability \\(P_k\\), we must integrate over all possible values of \\(\\varepsilon_k\\), weighted by its marginal density \\(f_{\\varepsilon_k}(\\varepsilon_k) = \\exp(-\\varepsilon_k) \\times \\exp \\left( -\\exp(\\varepsilon_k) \\right)\\): \\[\nP_k = \\int_{-\\infty}^{\\infty} \\exp(-\\varepsilon_k) \\times \\exp \\left( - \\exp(\\varepsilon_k) \\times \\sum_{j=1}^J \\exp \\left( V_j - V_k) \\right) \\right) \\, d \\varepsilon_k\n\\]\nFor notational simplicity, denote \\(a = \\sum_{j=1}^J \\exp ( V_j - V_k)\\) and define the transformation of variables \\(z = \\exp(-\\varepsilon_k)\\). Then \\(dz = -\\exp(-\\varepsilon_k) \\, d \\varepsilon_k = -z \\, d \\varepsilon_k\\) and the integral becomes: \\[\nP_k = \\int_\\infty^0 z \\times \\exp ( - z \\times a ) / (-z) \\, dz = \\int_0^\\infty \\exp (-z \\times a ) \\, dz\n\\]\nEvaluating the integral yields: \\[\nP_k = - \\left[ \\frac{1}{a} \\times \\left( 0-1 \\right) \\right] = \\frac{1}{a} = \\frac{1}{\\sum_{j=1}^J \\exp(V_j-V_k)} = \\frac{\\exp(V_k)}{\\sum_{j=1}^J \\exp(V_j)}\n\\]\nThis is the familiar multinomial logit (MNL) choice probability.\nSection 3.10 on page 74 of Kenneth Train’s book Discrete Choice Methods with Simulation provides a related derivation. If you are reading this post but have not yet read Train’s book, you should go read it now."
  },
  {
    "objectID": "posts/021-host-on-github/index.html",
    "href": "posts/021-host-on-github/index.html",
    "title": "Host Your Quarto Website on GitHub",
    "section": "",
    "text": "Updated August 2024\nThis tutorial assumes you have a rendered website on your local machine (perhaps from following this post on how to create a Quarto website) and would like to host that website publicly on GitHub."
  },
  {
    "objectID": "posts/021-host-on-github/index.html#step-1-get-git-and-gcm",
    "href": "posts/021-host-on-github/index.html#step-1-get-git-and-gcm",
    "title": "Host Your Quarto Website on GitHub",
    "section": "Step 1: Get Git and GCM",
    "text": "Step 1: Get Git and GCM\nGit is a free, open source, and distributed version control system. Practically, what this means is that you can use Git to track a history of your work on a project, to “back up” your project on a cloud-based Git repository service like GitHub, and/or to collaborate simultaneously but asynchronously with others on a project. It is the the most popular VCS; anyone who writes any amount of code should be familiar with the basics of Git.\nFirst, check to see if you have Git already installed. On Windows, this amounts to checking to see if you have an application called Git-Bash accessible from the Start Menu. On Mac or Linux, run git --version in the Terminal. If a version number is returned, you have Git.\nWhether or not you have Git, follow the remaining instructions to install the Git Credential Manager (GCM), which substantially improves the process of authenticating your computer with GitHub when you push local files to your remote respository.\nOn Windows, go to git.scm.com and download “Git for Windows” which is a bundle of software that includes Git, the Git-Bash Terminal, and the Git Credential Manager.\nOn Mac, I recommend using Homebrew, which is a package manager (very loosely, it’s like an App Store for your Terminal). Unlike the Windows instructions just above, this is a multi-step process.\n\nCheck to see if you have Homebrew installed by running brew --version in the Terminal. If a version number is returned, you do. If not, visit https://brew.sh, copy the line of code at the top of their webpage, and paste/run that line of code in your Terminal. It should look something like:\n\n\n\nTerminal\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nFollow along at the Terminal, as you may need to enter your computer log-in password and/or indicate you agree to various terms or licenses. Note that when you type your password at the command line in the terminal, the cursor will not move. This is for security and privacy reasons. But rest assured that your input is being received. So type out your password when/where necessary and hit enter.\n\nNext, install Git by running brew install git at the Terminal. If you receive an error like “zsh: command not found: brew” run the following line of code in the Terminal and then try again to install Git. (What’s going on here is that Homebrew makes its location known to the Bash shell but not to Zsh, which is the newer and now-default shell on MacOS, so the following line of code tells Zsh where to look for Homebrew.)\n\n\n\nTerminal\n\nexport PATH=\"/opt/homebrew/bin:$PATH\" &gt;&gt; ~/.zshrc\n\n\nWith Git successfully installed, next install the Git Credential Manager by running the following at the terminal. As before, follow along at the Terminal, as you may need to enter your computer log-in password and/or indicate you agree to various terms or licenses.\n\n\n\nTerminal\n\nbrew install git-credential-manager"
  },
  {
    "objectID": "posts/021-host-on-github/index.html#step-2-introduce-yourself-to-git",
    "href": "posts/021-host-on-github/index.html#step-2-introduce-yourself-to-git",
    "title": "Host Your Quarto Website on GitHub",
    "section": "Step 2: Introduce Yourself to Git",
    "text": "Step 2: Introduce Yourself to Git\nGit needs to know your name and email address because it records the author of each commit (ie, each “save state”). We’ll pass the --global option so you only ever need to do this once. At the Terminal, run the following but use your name and email address (not mine!).\n\n\nTerminal\n\ngit config --global user.name \"Dan Yavorsky\"\ngit config --global user.email \"dyavorsky@gmail.com\"\n\nYou can check your info by running git config --global --list.\nLastly for this step – and we really shouldn’t need this, but we’ll do it just in case – but we want to ensure we use the branch name “main” instead of the depreciated branch name “master”. We can specify this with the setting:\n\n\nTerminal\n\ngit config --global init.defaultBranch main"
  },
  {
    "objectID": "posts/021-host-on-github/index.html#step-3-turn-git-on-for-your-website",
    "href": "posts/021-host-on-github/index.html#step-3-turn-git-on-for-your-website",
    "title": "Host Your Quarto Website on GitHub",
    "section": "Step 3: Turn Git “On” for Your Website",
    "text": "Step 3: Turn Git “On” for Your Website\nAt the Terminal, use cd to navigate to your website’s directory. Then initialize the directory as a Git repository (ie, turn Git “on” for your website) by running git init:\n\n\nTerminal\n\ngit path/to/quarto_website\ngit init\n\nThis will create a hidden .git folder in the quarto_website directory. There’s no need to view this .git folder, but if you would like to, you may need to enable your file explorer or IDE to show hidden files:\n\nIn the Mac Finder, toggle show/hide of hidden files with Cmd+Shift+.Cmd+Shift+.\nOn Windows, use the search box on the Taskbar and type “show hidden files” and toggle the top result\nIn RStudio, select the gear icon in the Files tab and check the “Show Hidden Files” menu item\nIn VS Code, you need to alter the “files.exclude” setting, see this\n\n\n\n\n\n\n\nNoteScreenshots\n\n\n\n\n\n\nMac FinderRStudio"
  },
  {
    "objectID": "posts/021-host-on-github/index.html#step-4-save-your-website",
    "href": "posts/021-host-on-github/index.html#step-4-save-your-website",
    "title": "Host Your Quarto Website on GitHub",
    "section": "Step 4: “Save” Your Website",
    "text": "Step 4: “Save” Your Website\nAt this point, Git is now tracking your directory, but it hasn’t “saved” the history of your work. You create this history by taking snapshots of the state of your project at various times. These snapshots are called “commits”. Taking a snapshot is a two-step process: you first stage the files you want in your snapshot with git add and then you take the snapshot with git commit. All commits include a message.\nYou can take snapshots with helpful tools built into RStudio or VS Code, or you can use the command line in the Terminal.\nIn the Terminal, run the follow to stage all new or modified files and commit them:\n\n\nTerminal\n\ngit add .\ngit commit -m \"type a commit message in here\"\n\nIn RStudio, use the Git tab. Check the box to the left of each file to stage it, then click the little “Commit” button at the top of the tab. This pops up a window where you can enter your commit message. Then click the big “Commit” button to finalize your snapshot. Close the pop-up window.\nIn VS Code, use the third-from-top button in the Activity Bar at the far-left of the application to access the Source Control panel. There you’ll find helpful buttons like the plus-sign button to stage all files, a text box to enter your commit message, and the big blue “Commit” button.\n\n\n\n\n\n\nNoteScreenshots\n\n\n\n\n\n\nRStudio Git TabRStudio Commit WindowVS Code Source Control\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt this point, everything is all set locally on your machine. Next we head to GitHub."
  },
  {
    "objectID": "posts/021-host-on-github/index.html#step-5-create-a-github-account",
    "href": "posts/021-host-on-github/index.html#step-5-create-a-github-account",
    "title": "Host Your Quarto Website on GitHub",
    "section": "Step 5: Create a GitHub Account",
    "text": "Step 5: Create a GitHub Account\nGo to github.com and sign up for a free account.\nPick a username that is short, lowercase, professional, and timeless. For example, dyavorsky is a good choice while cooldudeUCSD2026 is not."
  },
  {
    "objectID": "posts/021-host-on-github/index.html#step-6-create-a-github-repository",
    "href": "posts/021-host-on-github/index.html#step-6-create-a-github-repository",
    "title": "Host Your Quarto Website on GitHub",
    "section": "Step 6: Create a GitHub Repository",
    "text": "Step 6: Create a GitHub Repository\nSelect the “Repositories” tab in GitHub (this is also accessible by clicking on your profile photo in the top-right corner).\nThe click the green “New” button on the top-right.\nIf you name your repository your_username.github.io, then your website will have the same thing as its URL, which is pretty nice. However, if you name your repository something like quarto_website, then your URL will be your_username.github.io/quarto_website. If you don’t like either of those URLs, you can always purchase a domain from a provider like GoDaddy. I did this so my URL would be www.danyavorsky.com instead of dyavorsky.github.io/quarto_website.\nOnce you create your repository, GitHub will helpfully list a bunch of terminal commands for you. Copy the 3-line command under the heading “…or push an existing repository from the command line”"
  },
  {
    "objectID": "posts/021-host-on-github/index.html#step-7-connect-your-local-directory-to-your-github-repo",
    "href": "posts/021-host-on-github/index.html#step-7-connect-your-local-directory-to-your-github-repo",
    "title": "Host Your Quarto Website on GitHub",
    "section": "Step 7: Connect your Local Directory to your GitHub Repo",
    "text": "Step 7: Connect your Local Directory to your GitHub Repo\nIn order to sync our local work with the new GitHub remote repository, we need to connect the two. Those 3 lines you just copied do the syncing. Paste/run those 3 lines in the Terminal:\n\n\nTerminal\n\ngit remote add origin https://github.com/username/repo-name.git\ngit branch -M main\ngit push -u origin main\n\n\nThe first command indicates that we want to connect our local repository to a remote repository and use the shortname “origin” instead of the full URL going forward.\nThe second command renames the current branch to “main”. This isn’t necessary for us, but here it does no harm, so we leave it in.\nThe third command pushes the commit(s) from our local repository up to the remote repository.\n\nRefresh the webpage in your browser of your GitHub repo and you should see all the files from your computer’s website directory copied up to the GitHub repo. Exciting stuff!"
  },
  {
    "objectID": "posts/021-host-on-github/index.html#step-8-tell-github-your-repo-is-a-website",
    "href": "posts/021-host-on-github/index.html#step-8-tell-github-your-repo-is-a-website",
    "title": "Host Your Quarto Website on GitHub",
    "section": "Step 8: Tell GitHub your Repo is a Website",
    "text": "Step 8: Tell GitHub your Repo is a Website\nIn your website repo, click on “Settings” toward the top-right.\nIn the left-side menu, select “Pages”.\nUnder “Branch” in the middle of the page\n\nensure the branch is “main” rather than “none”\nensure the folder is “docs” rather than “(root)”\n\nHave some patience and give GitHub a moment (usually only a minute or so) to host your site. GitHub will report “Your site is live” at the top of the page when it’s deployed. You man need to refresh the page to receive this message.\n\n\n\n\n\n\nNoteScreenshot"
  },
  {
    "objectID": "posts/021-host-on-github/index.html#step-9-confirm-your-public-website",
    "href": "posts/021-host-on-github/index.html#step-9-confirm-your-public-website",
    "title": "Host Your Quarto Website on GitHub",
    "section": "Step 9: Confirm Your Public Website",
    "text": "Step 9: Confirm Your Public Website\nEnter the URL of your website in a browser or click the “Visit Site” button in GitHub to see your public website."
  },
  {
    "objectID": "posts/021-host-on-github/index.html#workflow-to-update-your-website",
    "href": "posts/021-host-on-github/index.html#workflow-to-update-your-website",
    "title": "Host Your Quarto Website on GitHub",
    "section": "Workflow to Update Your Website",
    "text": "Workflow to Update Your Website\nAs you work on your website, your workflow will look something like this:\n\nfrequently make changes to files (mostly .qmd files) or add new files (e.g. images)\nfrequently render your website to see your edits\noccasionally stage and commit those changes\ninfrequently push those commits to GitHub\n\nFor example, if I worked on something for 3 hours, I edit files constantly. I render every few minutes. I might make a commit once an hour or so. I may push my work just once at the end of those 3 hours.\nRemember that the workflow provided in this tutorial requires you to render locally and then push everything in its final form to GitHub. I often make a local edit, forget to render, but commit that edit and push it to GitHub. Then I look at my website and wonder why the edit isn’t there. It’s because I edited the .qmd file, but since I didn’t render, the html file was never updated. Be sure to render locally before you commit and push!"
  },
  {
    "objectID": "posts/851-good-habits-bad-habits/index.html",
    "href": "posts/851-good-habits-bad-habits/index.html",
    "title": "Good Habits, Bad Habits",
    "section": "",
    "text": "Intelligence and motivation have little to do with getting things done on a regular basis.\nWe are not one single unified whole.\nOur minds are composed of multiple separate but interconnected mechanisms that guide behavior.\nExecutive control functions are thoughtful cognitive processes to select and monitor actions.\nSkip the debate chamber and get to work. That’s exactly what habits are for.\nWe have little conscious experience of forming a habit or acting out of habit.\nThe purpose of this book it to show how we can use conscious understanding of our goals to orient our habitual selves.\nFor these one-off, occasional behaviors, conscious decisions ruled, and people with strong attitudes just did them.\nWith some behaviors, people’s attitudes and plans had little impact on how they acted.\nPeople could consciously report strong attitudes and plans, but they continued their past actions regardless.\nPersistence mostly did not reflect strong attitudes and plans.\nWe now know that it’s habit that creates persistence.\nThis book explains what we have learned about how to create habits.\nThis is desire’s ironic twist. Trying to suppress it undermines our best intentions and makes our goals harder to achieve. It confounds our good behavior by turning it into torture.\nWe are engaging in executive control, or top-down processing, by controlling our unwanted habits with our better intentions.\nThis is the way many of us live. Our conscious decision-making self is pitted against our habitual, automatic responses. We are wrenched over and over by bad habits, in a sort of internal war. But there is another way.\nWhen our automatic response is the desired one, our habits and goals are in harmony. We no longer have to rely on will. This is the payoff to this book: understanding how to form good habits amid the pitfalls of daily life.\n\n\n\n\n\nWhat actually are habits?\nThey are tricky to study because they are inherently unknowable to the person performing them.\nJust being around people is enough to turn the spotlight inward and to start to monitor what you would normally do without much scrutiny at all. This is potentially useful if you ever feel like you just aren’t very aware of your habitual self (and would like to be). Go public. You’ll have a better sense of self in no time.\nYou can make pretty much any behavior more habitual, as long as you do it the same way each time.\nHabit refers to how you perform an action, not what the action is.\nThe key feature of habit: it works outside of our conscious awareness.\nOur curiosity about ourselves has already been satisfied by the belief that we do the things we do because we “will” them. It’s flattering and empowering, but it’s also false.\nIf our noisy, egotistical consciousness takes all of the credit for the actions of our silent habitual self, we’ll never learn how to properly exploit this hidden resource.\n\n\n\n\n\nHabits are a kind of action that is relatively insensitive to rewards.\nRewards are important when you first do something.\nThat’s just step one.\nOur responses are no longer aimed at seeking outcomes; instead they are triggered automatically by the performance context.\nFor habitual runners, running was triggered quickly when first cued by the places they tipically run… Goals seemed important for the occasional runners.\nGoals and rewards, it seems, are critical for starting to do something repeatedly. They are what lead us to form many beneficial habits in the first place.\nA working definition of habit emerged: a mental association between a context cue and a response that develops as we repeat an action in that context for a reward (mature habits can operate without the ongoing presence of a reward).\nBut a shorthand definition is this: automaticity in lieu of conscious motivation.\nIt’s not picky about what it learns. Give it repetition, rewards, and contexts.\nProcedural memory: it’s such an important repository of information that only the most frequently repeated patterns get stored like this.\nOther habits are almost as sticky… they fade only slowly as you fail to use them.\nFirefighters and football players (in the examples) both apparently identify a cue and have learned, through extensive practice, the right response.\n\n\n\n\n\nTheir intentions were no match for their habits. Another way to say this is: we often don’t realize what our habits are doing.\nThe parts of our lives that haven’t been claimed by our habitual self are indeed still receptive to our will – and receptive to new habit formation.\nWith enough practice, all can learn habit associations between contexts and the rewarded response.\nIn humans, habit learning isn’t superseded or subordinated by more thoughtful learning systems.\nThe default-interventionist system: The idea is that we’re using the default mode of autopilot most of the time, unless there’s a good enough reason to intervene with conscious thought.\n\n\n\n\n\nImportance of situations.\nParticipants who scored highest in self-control seldom reported resisting desires, period. They just didn’t experience many unwanted desires in the first place.\nPeople high in self control are not living a life full of self-denial and deprivation. Somehow they are managing their lives better.\nBut once they get started, they don’t think much about stopping or whether they are uncomfortable. They have a set pattern, and they follow it. They are not making decisions.\nHere’s the very happy implication: the worst, most effortful run will be that first one. Or the second, perhaps.\nHigh self-controllers achieved desired outcomes by streamlining, not struggling.\nResearchers anticipated that high self-controllers would especially shine at difficult tasks that required the central executive. After all, this is what, until recently, we though the self-control scale measures – sheer force of will. But even in these more controlled studies, the data did not support this idea. Instead, high self-control people performed better at the more habitual, automatic tasks than low self-control ones. High self-controllers were simply proficient at automating.\nPeople who score high in self-control seem to be doing nothing that the scale was ever designed to assess. They do not experience many unwanted desires, almost as if they had neutralized the temptations in their environments. They also know how to form habits by repeating the same things at the same times and in the same places.\nHabits that enable them to reliably meet goals without much struggle.\nSelf-control is simple when you understand that it involves putting yourself in the right situations to develop the right habits.\nA habit happens when a context cue is sufficiently associated with a rewarded response to become automatic."
  },
  {
    "objectID": "posts/851-good-habits-bad-habits/index.html#part-1-how-we-really-are",
    "href": "posts/851-good-habits-bad-habits/index.html#part-1-how-we-really-are",
    "title": "Good Habits, Bad Habits",
    "section": "",
    "text": "Intelligence and motivation have little to do with getting things done on a regular basis.\nWe are not one single unified whole.\nOur minds are composed of multiple separate but interconnected mechanisms that guide behavior.\nExecutive control functions are thoughtful cognitive processes to select and monitor actions.\nSkip the debate chamber and get to work. That’s exactly what habits are for.\nWe have little conscious experience of forming a habit or acting out of habit.\nThe purpose of this book it to show how we can use conscious understanding of our goals to orient our habitual selves.\nFor these one-off, occasional behaviors, conscious decisions ruled, and people with strong attitudes just did them.\nWith some behaviors, people’s attitudes and plans had little impact on how they acted.\nPeople could consciously report strong attitudes and plans, but they continued their past actions regardless.\nPersistence mostly did not reflect strong attitudes and plans.\nWe now know that it’s habit that creates persistence.\nThis book explains what we have learned about how to create habits.\nThis is desire’s ironic twist. Trying to suppress it undermines our best intentions and makes our goals harder to achieve. It confounds our good behavior by turning it into torture.\nWe are engaging in executive control, or top-down processing, by controlling our unwanted habits with our better intentions.\nThis is the way many of us live. Our conscious decision-making self is pitted against our habitual, automatic responses. We are wrenched over and over by bad habits, in a sort of internal war. But there is another way.\nWhen our automatic response is the desired one, our habits and goals are in harmony. We no longer have to rely on will. This is the payoff to this book: understanding how to form good habits amid the pitfalls of daily life.\n\n\n\n\n\nWhat actually are habits?\nThey are tricky to study because they are inherently unknowable to the person performing them.\nJust being around people is enough to turn the spotlight inward and to start to monitor what you would normally do without much scrutiny at all. This is potentially useful if you ever feel like you just aren’t very aware of your habitual self (and would like to be). Go public. You’ll have a better sense of self in no time.\nYou can make pretty much any behavior more habitual, as long as you do it the same way each time.\nHabit refers to how you perform an action, not what the action is.\nThe key feature of habit: it works outside of our conscious awareness.\nOur curiosity about ourselves has already been satisfied by the belief that we do the things we do because we “will” them. It’s flattering and empowering, but it’s also false.\nIf our noisy, egotistical consciousness takes all of the credit for the actions of our silent habitual self, we’ll never learn how to properly exploit this hidden resource.\n\n\n\n\n\nHabits are a kind of action that is relatively insensitive to rewards.\nRewards are important when you first do something.\nThat’s just step one.\nOur responses are no longer aimed at seeking outcomes; instead they are triggered automatically by the performance context.\nFor habitual runners, running was triggered quickly when first cued by the places they tipically run… Goals seemed important for the occasional runners.\nGoals and rewards, it seems, are critical for starting to do something repeatedly. They are what lead us to form many beneficial habits in the first place.\nA working definition of habit emerged: a mental association between a context cue and a response that develops as we repeat an action in that context for a reward (mature habits can operate without the ongoing presence of a reward).\nBut a shorthand definition is this: automaticity in lieu of conscious motivation.\nIt’s not picky about what it learns. Give it repetition, rewards, and contexts.\nProcedural memory: it’s such an important repository of information that only the most frequently repeated patterns get stored like this.\nOther habits are almost as sticky… they fade only slowly as you fail to use them.\nFirefighters and football players (in the examples) both apparently identify a cue and have learned, through extensive practice, the right response.\n\n\n\n\n\nTheir intentions were no match for their habits. Another way to say this is: we often don’t realize what our habits are doing.\nThe parts of our lives that haven’t been claimed by our habitual self are indeed still receptive to our will – and receptive to new habit formation.\nWith enough practice, all can learn habit associations between contexts and the rewarded response.\nIn humans, habit learning isn’t superseded or subordinated by more thoughtful learning systems.\nThe default-interventionist system: The idea is that we’re using the default mode of autopilot most of the time, unless there’s a good enough reason to intervene with conscious thought.\n\n\n\n\n\nImportance of situations.\nParticipants who scored highest in self-control seldom reported resisting desires, period. They just didn’t experience many unwanted desires in the first place.\nPeople high in self control are not living a life full of self-denial and deprivation. Somehow they are managing their lives better.\nBut once they get started, they don’t think much about stopping or whether they are uncomfortable. They have a set pattern, and they follow it. They are not making decisions.\nHere’s the very happy implication: the worst, most effortful run will be that first one. Or the second, perhaps.\nHigh self-controllers achieved desired outcomes by streamlining, not struggling.\nResearchers anticipated that high self-controllers would especially shine at difficult tasks that required the central executive. After all, this is what, until recently, we though the self-control scale measures – sheer force of will. But even in these more controlled studies, the data did not support this idea. Instead, high self-control people performed better at the more habitual, automatic tasks than low self-control ones. High self-controllers were simply proficient at automating.\nPeople who score high in self-control seem to be doing nothing that the scale was ever designed to assess. They do not experience many unwanted desires, almost as if they had neutralized the temptations in their environments. They also know how to form habits by repeating the same things at the same times and in the same places.\nHabits that enable them to reliably meet goals without much struggle.\nSelf-control is simple when you understand that it involves putting yourself in the right situations to develop the right habits.\nA habit happens when a context cue is sufficiently associated with a rewarded response to become automatic."
  },
  {
    "objectID": "posts/851-good-habits-bad-habits/index.html#part-2-the-three-bases-of-habit-formation",
    "href": "posts/851-good-habits-bad-habits/index.html#part-2-the-three-bases-of-habit-formation",
    "title": "Good Habits, Bad Habits",
    "section": "Part 2: The Three Bases of Habit Formation",
    "text": "Part 2: The Three Bases of Habit Formation\n\n6. Context\n\nOnce the environment changed, so did the habit.\nSome of the pressures that act on us come from inside ourselves, in the from of our goals, feelings, and attitudes. For Lewin, the contexts we are in (which he called “environments”) also generated forces on our behavior.\nRestraining forces are like a kind of friction.\nMarketing: a driving force, designed to reduce friction.\nWe can use driving and restraining forces to our advantage.\nThere is perhaps no simpler context influence we can engineer in our lives than sheer proximity. Proximity determines the external forces to which we are exposed. We engage with what is near us and tend to overlook what is farther away.\nWhat did you do last time you tried to change your behavior? Probably you thought about what you were doing wrong and why you wanted to change it. You focused on your desire to be successful. You acted as if your desires were in charge.\nA belief in free will has many advantages. But is also leads us to overlook the powerful influence of the physical and social worlds we inhabit. Our strong intentions blind us to the friction in our everyday surroundings.\nWe underestimate the impact of our environments.\nWe tend to overlook the influence of our surroundings, even as we are responding to them in our behavior and self-assessments. No surprise, then, that when we try to change, our go-to approach is will-power and motivation. We don’t realize how much our actions are driven by our surroundings and the pressures on us. But our habits do.\nAddress the context.\n\n\n\n7. Repetition\n\nYou’ve arranged your context. You’ve recognized the restraining forces, the driving forces, and the pitfalls of your introspection illusion. You’ve turned your life into a clearinghouse for good… so when does the magic happen? There are a couple more critical ingredients.\nAt first, it feels exciting. You’re proud of your new sense of responsibility.\nThe first time, it’s hard work.\nThe truth is, it’s impossible to know.\nYou’re going to keep doing it… until you aren’t doing it anymore.\nThe magic begins silently, and you won’t realize when it kicks in. You have to trust that it will happen.\nUntil we have laid down a habit in neural networks and memory systems, we must willfully decide to repeat a new action again and again, even when it’s a struggle. At some point, it becomes second nature, and we can sit back and let autopilot drive.\nA new action is difficult to sustain when the only driving forces are internal motivators.\nThis is the crucial point. You can miss a day or two and you will not be back to zero.\nOn average, it took participants 66 days of repeating a simple health behavior until they experienced it as automatic.\nAn implication is that you can lower your magic number by establishing forces that push you to repeat in the same way each time. With bigger, louder cues, your habit potentially matures faster.\nA wholesome new habit has to reckon with something else, though. Very little of our lives is a black space.\nAlong with the challenges of adopting new behaviors, you still have to fight off the old ones.\nThis is where repetition of the new action becomes especially useful as a tool (and not just an inert description of what habits look like).\nRepetition, then, should be thought of not as some kind of magical primer for habits, but rather as a way to induce speedy mental action.\nSpeed isn’t the only factor at work. It goes hand in hand with another consequence of repetition: streamlined decision-making. We stop considering alternative actions.\nHabits come from repetition. Behavior begets behavior. There isn’t a further, more complicated, rare, or special ingredient. That should be wonderfully liberating. That should make you optimistic. If you just keep doing it, it’ll start happening with more and more ease.\nExcellence and repetition are not the same.\nWe all know that repetition is necessary to excel, but it’s less clear that it’s sufficient.\nWith more practice, people did better at games, music, and sports, but sill 75 percent or more of their success or failure was due to factors such as native talent, opportunity, and having great trainers.\n\n\n\n8. Reward\n\nContext will smooth the way, and repetition will jump-start the engine, but if you aren’t getting even a minor reward for your initial effort along the way, you won’t get that habit to start operating on its own.\nRewards, to have a role in habit formation, have to be bigger and better than what you would normally experience.\nReward prediction error: In the brain, unexpected rewards spur the release of dopamine.\nIt then sets up the neural bases for habit formation, as neurons, synapses, and pathways work together to record and respond to what just happened.\nDopamine seems to promote habit learning for less than a minute.\nRewards have to be experienced right after we do something in order to build habit associations (context-response) in memory.\nGiven this timing, the most effective habit-building rewards are often intrinsic, or part of the action itself.\nStudents who did not experience the rewards that create automaticity from repetition had to keep consciously making themselves get to the track or gym, without a helpful habit taking over.\nThey are too far removed from the behavior you are trying to change, and they are not necessarily tied to any specific repetition. Given the way that dopamine works to create habit associations in memory, immediate rewards for lots of repetitions are key.\nThere’s more to the dopamine story and immediacy. As we discussed, dopamine responds to uncertainty in the form of reward prediction errors, which enable us to learn from the experience. This means that we learn from unusual or unexpected rewards.\nHabits depend on surprise; uncertain rewards matter most.\nFor scientists, insensitivity to reward is the gold standard for identifying a habit. The only way to know for sure if an action is habitual is to test what happens when the reward changes. If we persist even when we don’t value the reward as much or it’s no longer as available, then it’s a habit.\n\n\n\n9. Consistency Is for Closers\n\nHabits, as we have seen, thrive on reward uncertainty. Beyond this, habits don’t crave variety. In fact, they hate it. Variety weakens habit. This is because variety is the enemy of stable contexts, which, as we have learned, are the sine qua non of habits.\nIn this chapter, we’re going to learn how important it is to keep your habit-promoting context as stable as possible.\nThese studies are good illustrations that “context” definitely does not just mean “physical environment.” Location is important, but your context can also consist of intangible things: the time of day, for instance, or your state of mind. One of your most important possible contexts is other people.\nForming habits, it seems, is about establishing stable cues that support your desired actions.\n\n\n\n10. Total Control\n\nMise en place is French for “put in place.”\n“Harnessing friction” offers a whole new way to think about changing behavior. The promise is that, by altering contexts that create friction in our lives, we can learn to automatically repeat rewarding actions. But first, we have to identify these contexts. And they are not always obvious.\nIf this sounds like a lot of work for your executive, conscious mind, you’re absolutely right.\nThe greatest source of friction in this world is other people. They are both helpful and detrimental forces on our desired selves.\nIf you leave this book with one word and one idea, I hope it’s friction. It is simple and intuitive, and can be manipulated to help accomplish astounding things.\nHabits are more likely to form when we act repeatedly without planning and deliberating."
  },
  {
    "objectID": "posts/851-good-habits-bad-habits/index.html#part-3-special-cases-big-opportunities-and-the-world-around-us",
    "href": "posts/851-good-habits-bad-habits/index.html#part-3-special-cases-big-opportunities-and-the-world-around-us",
    "title": "Good Habits, Bad Habits",
    "section": "Part 3: Special Cases, Big Opportunities, and the World Around Us",
    "text": "Part 3: Special Cases, Big Opportunities, and the World Around Us\nOmitted from this summary\n\n\nJump Through Windows\n\n\nThe Special Resilience of Habit\n\n\nContexts of Addiction\n\n\nHappy with Habit\n\n\nYou Are Not Alone"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Content for You or Future Me",
    "section": "",
    "text": "How I Like Things on My Computers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbbreviated Fundamentals of Version Control\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbbreviated Fundamentals of the Command Line Interface\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYet another beginner’s cheat sheet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA tutorial to create a website skeleton with an About page, Resume page, and blog for hosting a project portfolio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA tutorial for pushing a local git repo to GitHub and deploying the website with GitHub pages\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html#posts",
    "href": "posts/index.html#posts",
    "title": "Content for You or Future Me",
    "section": "",
    "text": "How I Like Things on My Computers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbbreviated Fundamentals of Version Control\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbbreviated Fundamentals of the Command Line Interface\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYet another beginner’s cheat sheet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA tutorial to create a website skeleton with an About page, Resume page, and blog for hosting a project portfolio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA tutorial for pushing a local git repo to GitHub and deploying the website with GitHub pages\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html#discrete-choice",
    "href": "posts/index.html#discrete-choice",
    "title": "Content for You or Future Me",
    "section": "Discrete Choice",
    "text": "Discrete Choice\n\n\n\n\n\n\n\n\nGumbel Distribution in Discrete Choice Models\n\n\nDerivation of Choice Probabilities\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html#book-notes",
    "href": "posts/index.html#book-notes",
    "title": "Content for You or Future Me",
    "section": "Book Notes",
    "text": "Book Notes\n\n\n\n\n\n\n\n\n\n\nStatistical Theory: A Concise Introduction (2ed)\n\n\nby Felix Abramovich and Ya’acov Ritov\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood Habits, Bad Habits\n\n\nWendy Wood\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Learning Works\n\n\nSusan Ambrose, et al.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPeak: The Secrets from the New Science of Expertise\n\n\nAnders Ericsson and Robert Pool\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStolen Focus: Why You Can’t Pay Attention and How to Think Deeply Again\n\n\nJohann Hari\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScott Pressfield Books\n\n\nWar of Art, Turning Pro, Do the Work, Nobody Wants to Read Your Sh*t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Making of a Manager\n\n\nJulie Zhou\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWho Gets What and Why\n\n\nAlvin Roth\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html#solutions",
    "href": "posts/index.html#solutions",
    "title": "Content for You or Future Me",
    "section": "Solutions",
    "text": "Solutions\n\n\n\n\n\n\n\n\n\n\nStatistical Computing with R (2ed)\n\n\nMaria Rizzo\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/852-how-learning-works/index.html",
    "href": "posts/852-how-learning-works/index.html",
    "title": "How Learning Works",
    "section": "",
    "text": "Introduction\nWe define learning as the process that leads to change, which occurs as a result of experience and increases the potential for improved performance and future learning.\n\nProcess not product\n(Because learning occurs in the mind, we can only infer that it had occurred from students’ products or performances)\nThe change is lasting\nNot done to students but rather something that they do\n\n\n\n1. How Does Students’ Prior Knowledge Affect Their Learning?\n\nStudent’s prior knowledge can help or hinder learning.\n\nStories:\n\nStats students say they learned stats last year, but couldn’t do a t test\nNegatives reinforcement definition in psych class\n\nDifferent types of knowledge: declarative (knowing what) and procedural (knowing how and when).\nWhen students can connect what they are learning to accurate and relevant prior knowledge, they learn and retain more. Must avoid knowledge that is inaccurate or irrelevant for the appropriate context. Inappropriate knowledge must be addressed repeatedly; one explanation often isn’t enough.\nTeacher needs to help students activate prior knowledge. And it must be sufficient; don’t build on a shaky foundation.\n\n\n2. How Does the Way Students Organize Knowledge Affect Their Learning?\n\nHow students organize knowledge influences how they learn and apply what they know.\n\nStories:\n\nArt history presented in chronological order\nAnatomy course in major systems of the body\n\nExperts have a complex network related facts. Students don’t yet have a meaningful way of connecting facts, and often what they put together on their own is superficial.\nProvide students with a structure for organizing information. Make connections explicit. Provide boundary cases and contrasting cases to highlight organizational features.\n\n\n3. What Factors Motivate Students to Learn?\n\nStudents’ motivation generates, directs, and sustains what they do to learn.\n\nStories:\n\nTeaching continental philosophy by reading from primary sources\nIntroducing a tough engineering class by telling students only 2/3 will pass\nInstructors are failing to recognize that the things that motivated them may not be the same things that motivated the current group of students.\n\nExpectancies and value influence motivation, which leads to goal-directed behavior, which supports learning and performance.\nStudents often have multiple goals they are pursuing. They may also misconstrue learning goals into something other than learning: performance-approach goals and performance-avoidance goals are focus on attaining competence or avoiding incompetence. This results in students doing only what is necessary to get a good grade instead of seeking exploration and taking intellectual risks in order to develop a deep understanding.\nSubjective value of goals has 3 determinants: attainment value, intrinsic value, instrumental value. Students must find value in order to be motivated.\nPeople are motivated to pursue outcomes they believe they can successfully achieve. Students must hold positive outcome expectancies (doing work will yield good grade) and efficacy expectancies (belief in personal agency – I can do it).\n\n\n4. How Do Students Develop Mastery\n\nTo develop mastery, students must acquire component skills, practice integrating them, and know when to apply what they have learned.\n\nStories:\n\nManagement class students do worse on team assignments than individual offers\nAdvanced acting class students seem to have forgotten all the fundamentals\nHere, instructors may be asking more than they realize. Think about driving. It’s automatic now, but first you had to learn a sequence of steps, integrate them, and then understand the appropriate contexts in which to apply them.\n\nExpertise can be a liability when it comes to teaching.\n\nUnconscious incompetence: you don’t know what you don’t know\nConscience incompetence: increasingly aware of what you don’t know\nConscience competence: considerable competence but must think and act deliberately\nUnconsciousness competence: automatic and instinctive\n\nComponent skills: The way experts chunk knowledge can make it difficult to break down a skill so that it is clear to students.\nIntegration: Experts can perform complex tasks and combined multiple tasks easily not because they necessarily have more cognitive resources but, because of the high level of fluency they’ve achieved in performing key skills, they can do more with what they have.\nApplication: far transfer is the goal of education. Students need to understand what is not context-dependent, and not just the “what” but also the “why.” Flexible learning can be aided by combining experience with abstract information, and through structured comparisons.\n\n\n5. What Kinds of Practice and Feedback Enhance Learning\n\nGoal-directed practice coupled with targeted feedback are critical to learning.\n\nStories:\n\n3 diff public policy papers assigned, prof spend lots of time grading the first one\nGlitzy presentations for medical anthropology class\nTheme is time being misspent. Students need more opportunities to get it right or more feedback with which to incorporate.\n\nPractice: Deliberate focus on a specific goal, at an appropriate level of challenge, with sufficient time on task. It takes much more than one trial to learn something new, especially if the goal is for that new knowledge to be retained across time and transferred to be contexts.\nFeedback: Like use of a map. The important aspects are content and timing. Formative feedback communicates specific aspects of performance regarding progress toward some criteria whereas summative feedback is final judgement. Too much feedback and students will only focus on the detailed and easy-to-fix parts, and not the bigger and more substantive elements that you want them to focus on. Short delay is best; immediate opportunity for students to fix errors themselves, but not so much delay so as to be discontinuous.\nSuggestions: add scaffolding to assignments, show students what you do not want for comparison purposes, prioritize feedback.\n\n\n6. What Do Student Development and Course Climate Matter for Student Learning?\n\nStudents’ current level of development interacts with the social, emotional, and intellectual climate of the course to impact learning.\n\nStories:\n\nEcon class re immigration\nFemale electrical engineering students and sexist TA\nHere’s we have unanticipated social ands emotional dynamics in the classroom.\n\nStudent development: competence, emotions, autonomy, identity, interpersonal relationships, purpose, integrity.\nIntellectual development: duality, multiplicity, relativism, commitment.\nSocial identity development: naive, acceptance, resistance, immersion, disintegration, redefinition and internalization.\nClassroom climate: a continuum from marginalizing to inclusive. Can be explicit or implicit. Chapter focused on stereotypes, tone, interpersonal interactions, and content.\nBuild on student comments in a productive and validating way. Make uncertainty safe; resist a single right answer. Individuals do not speak for groups. Need to view students holistically as intellectual, social, and emotional beings.\n\n\n7. How Do Students Become Self-Directed Learners?\n\nTo become self-directed learners, students must learn to assess the demands of the task, evaluate their own knowledge and skills, plan their approach, monitor their progress, and adjust their strategies as needed.\n\nStories:\n\nSupposedly good student doing poorly on a paper\nOK student who works hard doing very poor in exams\nBoth students using approaches that worked well in high school, but aren’t working well in college – shortcomings in meta-cognition.\n\n“Want to talk to you about my grade” instead of “about my paper”.\nStudents overestimate their own skill, especially novices. Experts spend more time planning before jumping into something. Students who think that knowledge is malleable show more improvement.\nShow students how you do it. Scaffold assignments.\n\n\nConclusion\nAll of this applies to us as students of pedagogy. Use these techniques on yourself to learn how to most effectively teach."
  },
  {
    "objectID": "posts/001-my-setup/index.html",
    "href": "posts/001-my-setup/index.html",
    "title": "My Computing Environment Preferences",
    "section": "",
    "text": "Terminal & Shell\n\nOn Mac\nI want to be a person that sits on the command line, but I’m just not. For the rare times that I do, I use the Ghostty terminal application with Mac’s default zsh shell.\nMy preferred configurations include:\n\nghostty config: I use the Catppuccin Mocha theme\nprompt config: I use Oh My Posh following this tutorial\nzsh config: zsh-autosuggestions, zsh-syntax-highlighting, eza (a modern ls replacement), bat (cat replacement), ripgrep (grep replacement) and the ubiquitous fuzzy finder fzf alongside a more visual terminal file manager yazi, along with vim keybindings and a shortlist of aliases. It’s unremarkable, but the config file can be found on my github here\nNeovim config: As an aspiring vim user, I’m in the process of carefully configuring Neovim as if productivity is just one more plugin away. I’ll update this bullet once my config file no longer requires its own readme.\n\n\n\n\n\n\n\nNoteScreenshot\n\n\n\n\n\n\n\n\n\n\n\nOn Windows\nI use Ubuntu via the Windows Subsystem for Linux (WSL). I use Zsh as the shell, with the Oh My Zsh configuration framework and PowerLevel10k prompt theme. I will likely change this setup in the near future.\n\n\n\n\n\n\nNoteScreenshot\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio & Positron\nI use RStudio and Positron equally.\n\nRStudio Visual Look\nFor the visual look of RStudio, I use the RStudio theme Darkstudio and my own version of Atom’s One Dark editor theme, along with the Fira Code font. They can be installed as follows:\n\n\n\n\n\n\nNoteRStudio Theme Installation\n\n\n\n\n\n\n# rstudio theme\ndevtools::install_github(\"https://github.com/rileytwo/darkstudio\")\n\npath_to_index &lt;- \"/Applications/RStudio.app/Contents/Resources/app/www/index.htm\"\ndarkstudio::activate(path_to_index) # on mac\ndarkstudio::activate() # on windows (open rstudio with \"run as admin\")\n\n# editor theme\nurl &lt;- \"https://github.com/dyavorsky/atom-inspired-rstudio-theme/blob/main/atom_inspired.rstheme\"\nrstudioapi::addTheme(url, apply = TRUE, force = TRUE)\n\n\n\n\n\n\n\n\n\n\nNoteScreenshot\n\n\n\n\n\n\n\n\n\n\n\nRStudio Shortcuts\nI run this complete set of customized shortcuts in order to:\n\navoid conflicts with system shortcuts and window management shortcuts\nuse a consistent set of key bindings for both Windows and Mac\nremap bindings for operations I rarely use, to core part of my workflow\n\n\n\nPositron Visual Look\nI use the ‘One Dark Pro Darker’ theme, with minimal other customization. I love how Positron ‘just works’ out of the box.\n\n\n\n\n\n\nNoteScreenshot\n\n\n\n\n\n\n\n\n\n\n\n\n\nMac Shortcuts\nI aggressively edit keyboard shortcuts on my mac. I find most of the defaults unnecessary, which I disable. I heavily utilize spaces, speech-to-text, and screenshots, adding shortcuts for each. Some apps require their own special additions – for example, I can’t believe Command-Enter doesn’t send a message in Mail. I also find the Vimium Chrome extension (which enables keyboard-only navigation within the browser) to be essential.\n\n\n\nOther Software\n\nTodoist to track things that need to get done. There’s a version for every device and, as a backup, you can always access it from a browser\nLastPass is my password manager. If you don’t have one yet, you won’t believe how much simpler and more secure authentication can be\nZotero for tracking academic literature. While not perfect, it’s substantially better than a system based on a tree of sub-directories and verbose file names\nTypora for writing markdown in a minimal editor (be sure to grab the onedark theme)\nObsidian for maintaining a database of inter-linked markdown-based notes"
  },
  {
    "objectID": "posts/857-who-gets-what-and-why/index.html",
    "href": "posts/857-who-gets-what-and-why/index.html",
    "title": "Who Gets What and Why",
    "section": "",
    "text": "Economics is about the efficient allocation of scarce resources, and about making resources less scarce.\nMatching is economist-speak for how we get the things we choose in life that also must choose us.\nOften there is a structured matchmaking environment – some kind of application and selection process – through which that courtship and choosing takes place.\nUntil recently, economists often passed quickly over matching and focused primarily on commodity markets, in which prices alone determine who gets what. In a commodity market, you decide what you want and if you can afford it, you get it.\nBut in matching markets, prices don’t work that way.\nSome matches don’t use money at all. Kidney transplants cost a lot, but cash doesn’t decide who gets a kidney.\nSometimes a matching process, whether formal or ad hoc, evolves over time. But sometimes, especially recently, it is designed. The new economics of market design brings science to matchmaking and markets generally. That’s what this book is about.\nMarket design helps solve problems that existing marketplaces haven’t been able to solve naturally. Our work gives us new insights into a really makes “free markets” free to work properly.\nThe first task of a successful marketplace is bringing together many participants who want to transact, so they can seek out the best transactions. Having a lot of participants makes a market thick.\nEfforts to keep markets thick often concerned the timing of transactions. When should offers be made? How long should they be left open?\nCongestion is a problem that marketplace can face once they’ve achieved thickness. It’s the economic equivalent of a traffic jam, a curse of success.\nAlthough it’s great to have a marketplace that gives you an abundance of opportunities, these may be illusory if you can’t evaluate them, and they can cause a market to lose much of its usefulness.\nWhile buyers like to see many sellers, and sellers like to see multitudes of buyers, sellers aren’t so wild about competing with all those other sellers, nor are buyers necessarily glad to have such a crush of competition.\nOne thing that all markets challenge participants to do is to decide what they like.\nDecisions that depend on what others are doing, are called strategic decisions and are the concern of the branch of economics called game theory. Strategic decision making plays a big role in determining who does well or badly in many selection processes.\nStories about market design often begin with failure – failure to provide thickness, to ease congestion, or to make participation safe and simple.\n\n\n\nThe Chicago Board of trade made wheat into a commodity by classifying it on the basis of its quality (number 1 being the best) and type (winter or spring, hard or soft, red or white).\nCommodifying wheat via a reliable grading system helped make the market safe.\nTurning a market into a commodity market helps make it really thick, because any buyer can buy from any seller, and any seller can sell to any buyer. At the same time, it also helps the market deal with one of the main sources of congestion in matching markets, since in a commodity market each offer to sell can be made to all buyers, and each offer to buy can be made to all sellers. So unlike in the market for jobs or houses, no one has to wait for an offer to be made to him personally; anyone who sees (or hears) a price he likes can take it.\nNotice the tension between commoditization and product differentiation – that is, between wanting to sell in a thick market to buyers, even if they don’t care who you are, and trying to make your product special enough that many buyers will care enough about you to seek you out. Sellers enjoy selling in a thick market of buyers, but they don’t enjoy being interchangeable with other sellers.\nCredit cards offer merchants safety, but that safety came at the cost of transaction fees. Most merchants were willing to pay those fees because accepting credit cards brought in customers they might otherwise have missed, and also because credit cards make it safe for them to take non-cash payment from customers they didn’t know well, since the bank guaranteed payment as a form of insurance.\nThe bank that handles Amazon’s transactions, or the one that manages the account of your favorite restaurant, is typically different from the bank that issued your credit card and takes your payment. So behind-the-scenes, there is an interbank market, too, through which payments flow.\nInstances in which consumers recoil from offers that strike them as unfair are more common than you might think.\nEach of these ubiquitous marketplaces has found a way to succeed, not only in making market thick, uncongested, and safe, but also in making them simple to use. Making a market simple to use, however, may not be simple.\nOne thing we’ll see is that the “magic” of the market doesn’t happen by magic: many marketplaces fail to work well because of poor design.\n\n\n\nMarkets and marketplaces come in many forms, some of which don’t conform to conventional notions of markets, and some in which money may play little or no role.\nHow can people trade indivisible goods if everyone needs just one, has one to trade, and can’t use money?\nShapley and Scarf showed that for any preferences that patients and their surgeons might have regarding which kidneys they would like, there was always a way to find a set of cyclical trades they called “top trading cycles” with the property that no group of patients and donors could go off on their own and find a cycle of trades that they liked better.\nI was able to show that top trading cycles made it possible to organize a clearinghouse in such a way as to guarantee to patients and their surgeons that it was safe for them to be completely candid in revealing this kind of information.\nNowadays, the directors of transplant centers have become strategic players, and the biggest challenge is designing exchange clearinghouses in a way that makes it safe for hospitals to enroll all their patient-donor pairs, not just the ones that are hardest to match. At the moment, some hospitals are hanging onto their easy-to-match pairs so they can do their exchanges in-house.\nWithholding easy-to-match exchanges is a common temptation in markets with middlemen.\nIt was clear from the outset that the best way to make the market thick enough to find all potential exchanges would be to organize a national kidney exchange. But two problems immediately presented themselves: one technical and computational, the other political and organizational.\nKidney exchange is very different from the markets we saw in chapter 2. But as I’ve tried to show, market design for kidney exchange is still about making the market thick, uncongested, safe and simple, and efficient.\nThe general lesson to keep in mind as we look at more usual markets is that not only do marketplaces have to solve the problems of creating a thick market, managing congestion, and ensuring that participation is safe and simple, but they also have to keep solving and resolving these problems as markets evolve."
  },
  {
    "objectID": "posts/857-who-gets-what-and-why/index.html#markets-are-everywhere",
    "href": "posts/857-who-gets-what-and-why/index.html#markets-are-everywhere",
    "title": "Who Gets What and Why",
    "section": "",
    "text": "Economics is about the efficient allocation of scarce resources, and about making resources less scarce.\nMatching is economist-speak for how we get the things we choose in life that also must choose us.\nOften there is a structured matchmaking environment – some kind of application and selection process – through which that courtship and choosing takes place.\nUntil recently, economists often passed quickly over matching and focused primarily on commodity markets, in which prices alone determine who gets what. In a commodity market, you decide what you want and if you can afford it, you get it.\nBut in matching markets, prices don’t work that way.\nSome matches don’t use money at all. Kidney transplants cost a lot, but cash doesn’t decide who gets a kidney.\nSometimes a matching process, whether formal or ad hoc, evolves over time. But sometimes, especially recently, it is designed. The new economics of market design brings science to matchmaking and markets generally. That’s what this book is about.\nMarket design helps solve problems that existing marketplaces haven’t been able to solve naturally. Our work gives us new insights into a really makes “free markets” free to work properly.\nThe first task of a successful marketplace is bringing together many participants who want to transact, so they can seek out the best transactions. Having a lot of participants makes a market thick.\nEfforts to keep markets thick often concerned the timing of transactions. When should offers be made? How long should they be left open?\nCongestion is a problem that marketplace can face once they’ve achieved thickness. It’s the economic equivalent of a traffic jam, a curse of success.\nAlthough it’s great to have a marketplace that gives you an abundance of opportunities, these may be illusory if you can’t evaluate them, and they can cause a market to lose much of its usefulness.\nWhile buyers like to see many sellers, and sellers like to see multitudes of buyers, sellers aren’t so wild about competing with all those other sellers, nor are buyers necessarily glad to have such a crush of competition.\nOne thing that all markets challenge participants to do is to decide what they like.\nDecisions that depend on what others are doing, are called strategic decisions and are the concern of the branch of economics called game theory. Strategic decision making plays a big role in determining who does well or badly in many selection processes.\nStories about market design often begin with failure – failure to provide thickness, to ease congestion, or to make participation safe and simple.\n\n\n\nThe Chicago Board of trade made wheat into a commodity by classifying it on the basis of its quality (number 1 being the best) and type (winter or spring, hard or soft, red or white).\nCommodifying wheat via a reliable grading system helped make the market safe.\nTurning a market into a commodity market helps make it really thick, because any buyer can buy from any seller, and any seller can sell to any buyer. At the same time, it also helps the market deal with one of the main sources of congestion in matching markets, since in a commodity market each offer to sell can be made to all buyers, and each offer to buy can be made to all sellers. So unlike in the market for jobs or houses, no one has to wait for an offer to be made to him personally; anyone who sees (or hears) a price he likes can take it.\nNotice the tension between commoditization and product differentiation – that is, between wanting to sell in a thick market to buyers, even if they don’t care who you are, and trying to make your product special enough that many buyers will care enough about you to seek you out. Sellers enjoy selling in a thick market of buyers, but they don’t enjoy being interchangeable with other sellers.\nCredit cards offer merchants safety, but that safety came at the cost of transaction fees. Most merchants were willing to pay those fees because accepting credit cards brought in customers they might otherwise have missed, and also because credit cards make it safe for them to take non-cash payment from customers they didn’t know well, since the bank guaranteed payment as a form of insurance.\nThe bank that handles Amazon’s transactions, or the one that manages the account of your favorite restaurant, is typically different from the bank that issued your credit card and takes your payment. So behind-the-scenes, there is an interbank market, too, through which payments flow.\nInstances in which consumers recoil from offers that strike them as unfair are more common than you might think.\nEach of these ubiquitous marketplaces has found a way to succeed, not only in making market thick, uncongested, and safe, but also in making them simple to use. Making a market simple to use, however, may not be simple.\nOne thing we’ll see is that the “magic” of the market doesn’t happen by magic: many marketplaces fail to work well because of poor design.\n\n\n\nMarkets and marketplaces come in many forms, some of which don’t conform to conventional notions of markets, and some in which money may play little or no role.\nHow can people trade indivisible goods if everyone needs just one, has one to trade, and can’t use money?\nShapley and Scarf showed that for any preferences that patients and their surgeons might have regarding which kidneys they would like, there was always a way to find a set of cyclical trades they called “top trading cycles” with the property that no group of patients and donors could go off on their own and find a cycle of trades that they liked better.\nI was able to show that top trading cycles made it possible to organize a clearinghouse in such a way as to guarantee to patients and their surgeons that it was safe for them to be completely candid in revealing this kind of information.\nNowadays, the directors of transplant centers have become strategic players, and the biggest challenge is designing exchange clearinghouses in a way that makes it safe for hospitals to enroll all their patient-donor pairs, not just the ones that are hardest to match. At the moment, some hospitals are hanging onto their easy-to-match pairs so they can do their exchanges in-house.\nWithholding easy-to-match exchanges is a common temptation in markets with middlemen.\nIt was clear from the outset that the best way to make the market thick enough to find all potential exchanges would be to organize a national kidney exchange. But two problems immediately presented themselves: one technical and computational, the other political and organizational.\nKidney exchange is very different from the markets we saw in chapter 2. But as I’ve tried to show, market design for kidney exchange is still about making the market thick, uncongested, safe and simple, and efficient.\nThe general lesson to keep in mind as we look at more usual markets is that not only do marketplaces have to solve the problems of creating a thick market, managing congestion, and ensuring that participation is safe and simple, but they also have to keep solving and resolving these problems as markets evolve."
  },
  {
    "objectID": "posts/857-who-gets-what-and-why/index.html#how-marketplaces-fail",
    "href": "posts/857-who-gets-what-and-why/index.html#how-marketplaces-fail",
    "title": "Who Gets What and Why",
    "section": "How Marketplaces Fail",
    "text": "How Marketplaces Fail\n\n4. Too Soon\nGaming the system when the system is “first come, first served” can mean contriving to be earlier than your competitors. That’s why, for example, the recruitment of college freshmen to join fraternities and sororities is called “rush.” … It’s also the reason that Oklahomans are called “Sooners.”\nSometimes the problems of going too soon are subtler. Jumping the gun can cause potentially thick markets to unravel. They become thin when too many participants try to transact before their competitors are fully awake and present in the market. College football Bowl Games are one example.\nOne of the dangers associated with early transactions: they can come well before important information is available.\nIt’s often hard to get quantitative measurement of how well a matching market is performing in some ultimate sense.\nRushing to be sooner isn’t just something in the history books or on the sports pages. If you know a recent college graduate who recently took a job with a big investment bank such as Goldman Sachs, there’s a good chance that she’ll get a call soon after beginning to work. It will be from a big private equity firm interested in signing her to a contract that would take effect after she’s worked for Goldman for two years.\nWhen a market’s organization predictably leads to trouble, economists start asking whether it might be inefficient, meaning that a different organization might make everyone involved better off.\nIf making offers very early makes it hard to identify good job candidates, you might think that some firms would take a little more time and make offers to candidates who had already received at least one offer from another firm. But the firms that made early offers prevented this by making their offers exploding. … Exploding offers make markets thin as well as early, and so participants are deprived of information about both the quality of matches and what kind of matches the market might offer. In that situation, nobody has enough information to make an optimal decision.\nMore than the other sources of market failure that we’ll explore, unraveling is a failure of self-control. Participants just can’t stop themselves from transacting early, because if they resist the urge, they’ll lose out to someone else.\nLaws [to prevent early transactions] have proved difficult to enforce, because private and informal matchmaking arrangements have emerged.\nSo unraveling is hard to control in a market that relies on self-control. Even if you have a lot of self-control, all you need is the suspicion that other participants might jump the gun and you will do so too. It would be irrational not to.\nWe next asked organizations if they could pass a resolution that would empower fellowship applicants who had accepted very early offers to change their minds if, later, at the time of the clearinghouse, they regretted their early decisions. … By freeing fellows to change their minds if they accepted an early offer, the new approach deprived program directors of the incentive to make early offers and relieved them of the fear that others would do so. Thus they could safely wait and match to a great candidate later when the clearinghouse opened. [Similarly] almost all American universities have agreed that new PhD students shouldn’t have to accept their offers before April 15. This single rule has virtually eliminated all exploding offers for PhD candidates in the United States.\nAnother part of our evidence was theoretical. Exploding offers won’t occur when everyone has enough experience with the market to know what to expect.\nSuccessful designs depend greatly on the details of the market, including the culture and psychology of the participants.\nMarkets unravel despite the collective benefit of having a thick market in which lots of people are present at the same time, with many opportunities to be considered and compared. Without a good market design, individual participants may still find it profitable to go a little early and engage in a kind of claim jumping. That’s why self control is not a solution: you can control only yourself, and if other jump ahead of you, it might be in your self-interest to respond in kind. As we’ll see in the next chapter, making the market operate within a narrow time frame – but without providing something, such as a clearinghouse, that brings order to the market at that time – usually isn’t a good enough solution to the problem of unraveling.\n\n\n5. Too Fast\nBeing too early isn’t the only way speed can prevent markets from achieving the thickness they need to succeed. Markets can also move too fast. … Speed helps participants in a thick market to evaluate and process lots of potential transactions quickly. But sometimes making markets work faster also makes them work worse.\nEven in the ultra-fast world of finance, many milliseconds can pass with no trades taking place at all. Thus a market that looks thick on a human timescale, with hundreds of opportunities to trade in the course of a single second, can look comparatively thin to a computer.\nThere’s a similar concern [to insider trading] when competition based on speed displaces competition based on price. Researchers proposed these same financial markets run only once per second. But in the absence of sufficient pressure by regulators, a brand-new market design is seldom adopted before a market becomes so dysfunctional that its users grow desperate for something new. As the tale of these financial markets makes clear, a superior market design isn’t always implemented.\nCongestion: insufficient time to make and consider as many opportunities as needed to make a good decision.\nAlthough the almost-tops [judges] could still expect to hire very good clerks, their chances of hiring clerks who might subsequently be hired by one of the nine Supreme Court justices would take a big hit. It takes a very confident (or foolish) student about to begin her third year of law school to turn down a plum job just because it isn’t the plummiest job in the whole country. So it was the almost-top judges who started the unraveling.\nThe history of this [law] market is a case study in how incremental changes in design can fail when the address only the symptoms of market failure and not the causes. It wasn’t sufficient to adopt rules that stopped offers from being made early.\n\n\n6. Congestion: Thicker Needs to be Faster\nWhen it comes to speed, markets tend to follow a kind of “Goldilocks principle”: they mustn’t be too hot or too cold.\nMarkets can be too slow, or congested even on the internet.\nThe opportunity to do well by building a good marketplace can arise whenever there are desirable but underused resources that take too much time to find and transfer. [AirBnB and Uber are examples.]\nThick markets need to be quick, but it’s hard to be quick – no matter how fast the technology – if people have to wait for other people to make and act on their decisions.\nThe key to speeding up the New York school choice system wasn’t to just design a computerized process, although that was part of it: computers are fast… The solution turned out to be letting people indicate their school preferences all at one time and then using those preferences to process decisions quickly.\nThe key to such a clearinghouse is making it safe for people to state their preferences honestly.\n\n\n7. Too Risky\nExamples of risks: robbery, physical harm, not getting what you paid for, not being the quality you expected, your credit card information being stolen.\nWhen everyone is in the same place, good reputations can be earned naturally by honest business people. But on the Internet, no one knows you’re a reputable businessman.\nSo for a marketplace to be truly trustworthy, it must be safe; participants on both sides of a transaction must be able to rely on each other and on the technology.\nUp to this point, market designs for trustworthiness have focused on providing secure methods to make payments, providing insurance for transactions that go bad, and building feedback systems that allow reliable sellers, and sometimes buyers, to develop and display good reputations.\nWhen participants in a market are reluctant to reveal crucial information, the market may run inefficiently. On eBay, concealing bid information from other bidders by sniping makes prices unpredictable, and when there’s a lot of sniping, not every auction is won by the person who is willing to pay the most [bidders may forget to bid, or their big may not be processed in time].\n[A lengthy example of the Boston Public School system was provided.] The fact that the student assignment policy made it necessary for people to game the system meant it was a failed system."
  },
  {
    "objectID": "posts/857-who-gets-what-and-why/index.html#design-interventions",
    "href": "posts/857-who-gets-what-and-why/index.html#design-interventions",
    "title": "Who Gets What and Why",
    "section": "Design Interventions",
    "text": "Design Interventions\n\n8. The Match: Strong Medicine for New Doctors\nThe solutions to problems in market design are sometimes invented, sometimes discovered, and often a bit of both.\nGale and Shapley called their version the Deferred Acceptance Algorithm:\n\nApplicants and employers privately submit preferences to a clearinghouse\nEach employer offers jobs to its top-choice candidates. Each applicant looks at all the offers he or she has received, and tentatively accepts the best one.\nEach employer that had a job offer rejected in the previous step offers that job to its next choice, if one remains. Each applicant considers the offer he or she has been holding together with his or her new offer(s) and tentatively accepts the most preferred.\nThe algorithm ends when no offer is rejected\n\nThe astonishing thing that Gale and Shapley proved is that the final matching is always stable with respect to the preferences submitted by the employers and applicants, whatever those preferences happen to be.\nThe Medical Residency Match succeeded as a marketplace because it solved the problems that had caused previous ways or organizing the market to fail. It was attractive enough to students and hospitals to make the market thick, and it discouraged unraveling., since it was worth waiting for. It wasn’t congested, since it asked for decisions in advance and executed the process of finding the outcome of all those decisions quickly. And it made it safe for doctors to reveal their true preferences.\nElliott played a role that I have since found is essential whenever I’ve successfully helped design a complex market: he was the expert guide. As an economist approaching a new market, I’m something of a generalist, sort of like an experienced mountain climber approaching a new mountain. Even if I’ve studied the market from a distance, there are details I still have to learn, because details matter in design.\nI was asked, could I somehow tweak the match to send more residents to rural hospitals that traditionally didn’t fill all their available positions? I proved a mathematical result now know as the Rural Hospitals Theorem: the answer is no. It turns out that when a hospital doesn’t fill all its positions at some stable outcome, it gets exactly the same set of doctors at every stable outcome.\n\n\n9. Back to School\n[This chapter is basically the same story as the medical match, but using the deferred acceptance algorithm for the NYC and Boston school markets.]\nWhen students can list as many choices as they want, the deferred acceptance algorithm allows them to safely list schools in true order of preference; they won’t lost a place just because someone else applied earlier in the algorithm. This works because even if a student doesn’t get into her first-choice school, she has just as much chance of getting into her second-choice school as if she had listed it first.\n\n\n10. Signaling\nMarkets can be dramatically improved when their design encourages people to communicate essential information they might otherwise have kept to themselves. But sometimes markets suffer from too much communication. It is a paradox of market design that as communication gets easier and cheaper, it sometimes also gets less informative.\nI’ve just spoken about signals for two quite different kinds of information: qualification and interest… So signals, and how to send them, can be an integral part of a market’s design.\nYou may have to send costly signals even though those costs are partly wasted. For instance, if you go to college and study hard to signal that you’re good at learning and then take a job for which what you learned in college isn’t helpful, you paid a heavy cost that doesn’t directly help your employer.\nThere’s an ancient method of signaling in which the cost of the signals to the signaler is exactly equal to the benefit to the recipient. I’m speaking of auctions… One reason that auctions are such ancient and useful ways to sell something is that the high bidder sends a costly signal that he values the object more than any of the other bidders, and the cost of his signal isn’t wasted at all: the seller and the auctioneer receive the money that the high bidder spends.\nIt is safe for you to bid your true value in a second-price auction since you can’t do better by bidding anything else.\nIt doesn’t necessarily impose a cost on the seller, even though the seller receives only the amount of the second-highest bid. That’s because in a first-price sealed bid auction, for example, it isn’t safe for bidders to bid their true value; they have to bid less than that if they are going to make any profit.\nThe situation changes when you don’t know how much the object is worth to you… you might be a signal of how much you should be willing to pay by hearing the bids of other bidders, which would convey some information about those other companies’ estimates.\nMost of the signals we’ve talked about so far are signals that people send about themselves. You could think about all those signals as being sent from the seller to the buyer. Signals about quality are like that: they’re of the form, I’m a good student. Signals about interest are also from seller to buyer: they’re of the form, I’m really interested in attending your college."
  },
  {
    "objectID": "posts/857-who-gets-what-and-why/index.html#forbidden-markets-and-free-markets",
    "href": "posts/857-who-gets-what-and-why/index.html#forbidden-markets-and-free-markets",
    "title": "Who Gets What and Why",
    "section": "Forbidden Markets and Free Markets",
    "text": "Forbidden Markets and Free Markets\n\n11. Repugnant, Forbidden, … and Designed\nLet’s call a transaction repugnant if some people want to engage in it and other people don’t want them to (and we’ll exclude negative externalities from this). [The example is given that the state of California voted it illegal in 1998 to sell horsemeat for human consumption, whereas in many part of Europe humans do consume horsemeat.]\nLet’s call a transaction protected if many people would like to promote it, in the sense that they are eager to protect others’ rights to engage in it, even if they don’t wish to engage in it themselves. Farming by small farmers falls into this category.\nBoth repugnant and protected transactions are in the eye of the beholder, and can change over time.\nOne common occurrence is that some transactions that may not be repugnant – and may even be protected – when no cash changes hands become repugnant when money is added to the mix [eg, paying birth mothers of children that get adopted, paying for kidneys that get transplanted, paying to be someone’s dinnerguest at their house].\nSuch concerns about the monetization of transactions seem to fall into three principal classes: objectification, coercion and exploitation, and a slippery slope.\n…emphasize the larger issue that even when thinking about the most difficult markets – those that may arouse our repugnance – we should never forget that markets are human artifacts. Market design allows us to think about how to try and get the benefits of markets to the people who need them.\nBecause markets are collective enterprises, we can design them but not necessarily control them. This is part of why there is so much sentiment in favor or making some markets illegal rather than trying to design them to circumvent their repugnant aspects.\n\n\n12. Free Markets and Market Design\nRegulations can be supplied both by governments and by private entities, such as franchises, malls, or industry associations.\nWhile good market design may emerge slowly over time as old rules and regulations are modified, bad designs can persist for a long time… because there may be lots of market participants with a state in the status quo, and many interests are involved in coordinating any market-wide change.\nChanging the [U.S] market for health care is famously difficult.\nThe lesson or marked design for political debate is that to understand how markets should be operated and governed, we need to understand what rules particular markets need. That’s a different question than whether some rules will apply to many markets, as regulations, and whether government is best suited to make some of those rules.\nThe fact is that both governments and private market makers have a role to play, and they both can sometimes err by regulating too slowly and not vvigorously enough, but also by regulating too hastily.\nWhen we think about helping markets work well, we also need to think about what is means to work well. Markets that function well give us choices, and so markets that operate freely are related to our liberty as well as to our prosperity.\nWe’ve seen how our choices may be limited in markets that aren’t thick, or are congested, or that make it unnecessarily risky to even try to get what we would choose if we could. And, of course, markets are connected.\nMarkets, like languages, come in many varieties. Commodity markets are impersonal, but matching markets can be deeply personal, as personal as a job offer or a marriage proposal. And once you observe that matching is one of the major things that markets do, you realize that matching markets – markets in which prices don’t do all of the work, and in which you care about whom you deal with – are everywhere…\nAs we start to understand better how markets and marketplaces work, we realize that we can intervene in them, redesign them, fix them when they’re broken, and start new ones where they will be useful."
  },
  {
    "objectID": "posts/010-git-basics/index.html",
    "href": "posts/010-git-basics/index.html",
    "title": "Get Going with Git",
    "section": "",
    "text": "Git is a free and open source distributed version control system.\nThis post serves as a set of notes to quickly refer to. For detailed explanations, see these excellent free resources:\n\nPro Git by Scott Chacon and Ben Straub\nHappy Git and GitHub for the UserR by Jenny Bryan"
  },
  {
    "objectID": "posts/010-git-basics/index.html#whats-git",
    "href": "posts/010-git-basics/index.html#whats-git",
    "title": "Get Going with Git",
    "section": "",
    "text": "Git is a free and open source distributed version control system.\nThis post serves as a set of notes to quickly refer to. For detailed explanations, see these excellent free resources:\n\nPro Git by Scott Chacon and Ben Straub\nHappy Git and GitHub for the UserR by Jenny Bryan"
  },
  {
    "objectID": "posts/010-git-basics/index.html#getting-going",
    "href": "posts/010-git-basics/index.html#getting-going",
    "title": "Get Going with Git",
    "section": "Getting Going",
    "text": "Getting Going\n\nCheck to see if you have Git installed already:\n\nOn Windows, open the command prompt and type git --version or search for the application Git Bash.\n\nIf you see git version 2.43.0.windows.1 or something similar, you’re good to go.\n\nIf you see git is not recognized as an internal or external command, you need to install Git.\n\nOn Mac, open the terminal and type which git or git --version.\n\nIf you see something like /usr/local/bin/git or git version 2.39.3 (Apple Git-145), you’re good to go.\n\nIf you see command not found, you need to install Git.\n\n\n\n\nTo download or update Git\n\ngo to https://git-scm.com/downloads and install like any other software\nto update on Mac with Homebrew: brew upgrade git\nto update on Windows: git update git-for-windows\n\n\n\nTell Git who you are\n\nat the terminal (mac) or command prompt or git bash (Windows)\n\ntype git config --global user.name \"your name\" (can be your full name or github username)\ntype git config --global user.email \"your email\" (should be the email you use with GitHub)\ncheck the result with git config --list\n\n\n\n\nRegister a free GitHub account\n\npick a username that is short, timeless, professional, and (preferably) all lowercase\n\n\n\nLink your GitHub account and your local Git\n\nGit Credential Manager is the best way to store your GitHub credentials\n\non Windows, GCM is included with Git for Windows\non Mac, you can install GCM via Homebrew with brew install --cask git-credential-manager"
  },
  {
    "objectID": "posts/010-git-basics/index.html#git-commands",
    "href": "posts/010-git-basics/index.html#git-commands",
    "title": "Get Going with Git",
    "section": "Git Commands",
    "text": "Git Commands\n\nThe basics\nTo copy down a GitHub repository to your local machine\n\ngit clone https://github.com/your-username/your-repo.git to copy down a repository from GitHub to your local machine\ngit remote --verbose to check the remote was cloned successfully\n\nTo add a remote to an existing local repository\n\ngit remote add origin https://github.com/your-username/your-repo.git to add a remote to an existing local repository with nickname origin\ngit remote --verbose to check the remote was added successfully\ngit remote show origin to see more details about the remote\ngit remote remove origin or git remote set-url origin _url_ to remove or edit the url aliased by origin\n\nMost common workflow commands\n\ngit add newdoc.txt to stage a file for a commit\ngit commit -m \"a commit message\" to commit all staged files\ngit push origin main to push local commits (on branch main) up to the remote repository (origin)\ngit push -u origin main to push local commits up to the remote repository and have local main track remote main\ngit pull origin main to pull down the latest changes (on branch main) from the remote repository (origin)\n\nChecking status\n\ngit status to check on the state of your repository\ngit log --oneline to see a list of commits\ngit diff to see the changes you’ve made since the last commit\n\nBranching\n\ngit branch [branch-name] to create a new branch\ngit checkout [branch-name] to switch to the branch\ngit checkout -b [branch-name] to create a new branch and switch to it in one command\ngit checkout main to switch back to the main branch\ngit merge [branch-name] (when on main) to merge the branch into the main branch\n\n\n\nIntermediate\nComing Soon"
  },
  {
    "objectID": "posts/854-stolen-focus/index.html",
    "href": "posts/854-stolen-focus/index.html",
    "title": "Stolen Focus: Why You Can’t Pay Attention and How to Think Deeply Again",
    "section": "",
    "text": "Ch0: Introduction\n\nFifty years ago there was very little obesity, but today it is endemic in the Western world. This is not because we suddenly became greedy or self-indulgent. He said: “Obesity is not a medical epidemic – it’s a social epidemic.” (p11)\nI found strong evidence that our collapsing ability to pay attention is not primarily a personal failing on my part, or your part, or you kid’s part. This is being done to us all. It is being done by very powerful forces. …there are twelve deep forces at work that are damaging our attention. (p12)\nThere are real steps you can take as an isolated individual to reduce this problem for yourself… But I have to be honest with you, in a way that I fear previous book on this topic were not. Those changes will only get you so far. … Systemic problems require systemic solutions. (p12)\n\n\n\nCh1: The Increase in Speed, Switching, and Filtering\n\nSo if you spend your time switching a lot, then the evidence suggests you will be slower, you’ll make more mistakes, you’ll be less creative, and you’ll remember less of what you do. (p40)\n\n\n\nCh2: The Crippling of Our Flow States\n\nHe called it a “flow state.” This is when you are so absorbed in what you are doing that you lose all sense of yourself, and time seems to fall away, and you are flowing into the experience itself. It is the deepest for of focus and attention that we know of. (p55)\nRelaxing rarely gets you into a flow state. (p55)\nThree core components: (1) choose a clearly defined goal, … (2) that’s meaningful, and … (3) that’s at the edge of your abilities. (p55-56)\nSo, to find flow, you need to chose one single goal; make sure your goal is meaningful to you; and try to push yourself to the edge of your abilities. (p56)\nTo recover from our loss of attention, it is not enough to strip out our distractions. That will just create a void. We need to strip out our distractions and to replace them with sources of flow. (p60)\n\n\n\nCh3: The Rise of Physical and Mental Exhaustion\n\nWhen people are kept awake, “one of the first things to go is the ability to focus our attention.” (p66)\nToday 40 percent of Americans are chronically sleep-deprived, getting less than the necessary minimum of seven hours of sleep per night. … Only 15 percent of us wake up from our sleep feeling refreshed. This is new. Since 1942, the average amount of time a person sleeps has been slashed by an hour a night. (p66)\n“You can deprive yourself of sleep and live. We could never raise children if we couldn’t drop down on our sleep, right? We’d never survive hurricanes. You can do that – but it comes at a cost. The cost is [that] your body shifts into the sympathetic nervous system zone – so your body is like, ’Uh-oh, you’re depriving yourself of sleep, must be an emergency.” (p69)\nThe less you sleep, the more the world blurs in every way – in your immediate focus, in your ability to think deeply and make connections, and in your memory. (p70)\nWe live in an apparent paradox. Many of the things we need to do are so obvious they are banal; slow down, do one thing at a time, sleep more. But even though at some level we all know them to be true, we are in fact moving in the opposite direction: toward more speed, more switching, less sleep. We live in a gap between what we know we should do and what we feel we can do. (p77-78)\n\n\n\nCh4: The Collapse of Sustained Reading\n\nThe medium is the message … The way information gets to you is more important than the information itself. (p83)\nI like the person I become when I read a lot of books. I dislike the person I become when I spend a lot of time on social media. (p86)\n\n\n\nCh5: The Disruption of Mind-Wandering\n\nMind wandering … you are slowly making sense of the world. (p95)\nWhen your mind wanders, it starts to make new connections between things. (p96)\nTo be productive, you can’t aim simply to narrow your spotlight as much as possible. (p97)\nMind-wandering can easily descent into rumination. … In situations of low stress and safety, mind-wandernig will be a gift, a pleasure, a creative force. In situations of high stress or danger, mind-wandering will be a torment. (p100)\n\n\n\nCh6-7: The Rise of Technology That Can Track and Manipulate You\n\nInside of Facebook’s servers, inside of Google’s servers, there is a little voodoo doll, [and it is] a model of you. It starts by not looking much like you. … They’re reassembling all that metadata you don’t really think is meaningful, so that doll looks more and more like you. (p126)\nWhenever something is provided by a tech company for free, it’s always to improve the voodoo doll. Surveillance capitalism. (p127)\nFor years, I had blamed my deteriorating powers of attention simply on my own failings or on the existence of the smartphone itself as technology. … Tristan taught me that the phones we have, and the programs that run on them, were deliberately designed by the smartest people in the world to maximally hold our attention. (p128)\nUnfortunately, there’s a quirk of human behavior. On average, we will stare at something negative and outrageous for a lot longer than we will stare at something positive and calm. … It’s called “negativity bias.” (p131)\nSo an algorithm that prioritizes keeping you glued to the screen will – unintentionally but inevitably – prioritize outraging and angering you. If it’s more enraging, it’s more engaging. (p131)\nFirst, these sites and apps are designed to train our minds to crave frequent rewards. … Once you have been conditioned to need these reinforcements, it’s very hard to be with reality, the physical world, the built world – because it doesn’t offer as frequent and as immediate rewards as this thing does. (p133)\nWe don’t just pay attention as individuals; we pay attention together, as a society. (p134)\nOver time, you expose any country to all this for long enough, it will become a country so lost in rage and unreality that it can’t make sense of it’s problems and it can’t build solutions. (p140)\n\n\n\nCh7: The Rise of Cruel Optimism\n\nIf we are going to overcome this process of becoming hooked on our apps and devices, we have to develop individual skills to resist the part inside all of us that succumbs to these distractions. He argues that to do that, we primarily have to look inward – to the reasons why we want to use them compulsively in the first place. … It’s not your fault. I never said it’s your fault. It’s your responsibility. (p144-145)\nAn internal trigger is an uncomfortable emotional state. … It’s all about avoidance. … He believes we all need to explore our triggers nonjudgmentally, think about them, and find ways to disrupt them. (p145)\nTwo-thirds of people with a smartphone never change their notification settings. What? Right? This is not hard stuff. (P146)\nHow many of us plan our day? Actually what do I want to do with my time? (p147)\nCruel Optimism: this is when you take a really big problem with deep causes in our culture – like obesity, or depression, or addiction – and you offer people, in upbeat language, a simplistic individual solution. (p150)\nWhile at first glance, cruel optimism seems kind and optimistic, it often has an ugly after-effect. It ensures that when the small, cramped solution fails, as it will most of the time, the individual won’t blame the system – she will blame herself. It whispers: the problem isn’t in the system; the problem is in you. (p151)\n\n\n\nCh9: The First Glimpse of the Deeper Solution\n\nThe business model can only be changed by regulation imposed on these companies by governments. (p161)\n\n\n\nCh10: The Surge in Stress and How It Is Triggering Vigilance\n\nThese technologies arrived in our lives at a moment when we were unusually vulnerable to them. (p171)\nTop reasons attention is getting worse: stress, change in life circumstances, difficult or disturbed sleep, phones. (p172)\nTo pay attention in normal ways, you need to feel safe. (p177)\n\n\n\nCh11: The Places that Figures Out How to Reverse the Surge in Speed and Exhaustion\n\nWhen people work less, their focus significantly improves. (p190)\nWe live in a culture that gest us to walk faster, talk faster, work longer, and we are taught to think that is where productivity and success come from. (P190)\nBut Covid also showed us something else that is relevant to a four-day week. It demonstrated that businesses can change their working practices radically, in a very short period of time, and continue to fuction well. (p193)\nThe way we work seems fixed and unchangeable – until it changes, and then we realize it didn’t have to be like that in first place. (p193)\n\n\n\nCh12: Our Deteriorating Diets and Rising Pollution\n\nYet every day, all over the Western world, we are putting into our bodies substances “which are so far removed from what was intended for human fuel.” Achieving sustained attention, is a physical process that requires your body to able to do certain things. So if you disrupt your body – by depriving it of the nutrients it needs, or by pumping it full of pollutants – your ability to pay attention will also be disrupted. (p197-198)\nThree broad ways in which how we eat now is harming our focus: (1) we currently eat a diet that causes regular energy spikes and energy crashes, (2) most of us now eat in a way that deprives us of the nutrients we need for our brains to develop and function fully, and (3) our current diets aren’t just lacking in what we need – they also actively contain chemicals that seem to act on our brains almost like drugs. (p198-201)\n[Healthier countries] are leaving out the crap that’s making us sick in the first place. They’re all leaving out the refined carbohydrates, the processed food, the junk oils. That’s the key. That’s the magic bullet – just go back to whole foods. (p202)\nThere is growing evidence that pollution is seriously damaging our ability to focus. (p204)\n\n\n\nCh13: The Rise of ADHD and How We Are Responding to It\n\n[to be added]\n\n\n\nCh14: The Confinement of Our Children, Both Physically and Psychologically\n\n[to be added]\n\n\n\nCh00: Conclusion – Attention Rebellion\n\nIf this was a self-help book, I would be able to serve up a delightfully simple conclusion to this story. … But this is not a self-help book, and what I have to say to you is more complex, and it means starting with an admission: I have not entirely solved this problem in myself. (p267)\nI made six big changes in my life: (1) I used pre-commitment to stop switching tasks so much. (2) I have changed the way I respond to my own sense of distraction. I ask: what could you do now to get into a flow state, and access your mind’s own ability to focus deeply? (3) I now take six months of the year totally off social media. (4) I realized that letting your mind wander is not a crumbling of attention, but in fact a crucial form of attention in its own right. (5) I used to see sleep as a luxury, or worst as an enemy. Now I am strict with myself about getting eight hours every night. (6) I try to give as much of my time to my children as free play as I can. (p268-270)\nYour individual efforts to improve your attention can be dwarfed by an environment full of things that wreck it. (p271)\nIf we continue to be a society of people who are severely under-slept and overworked; who switch tasks every three minutes; who are tracked and monitored by social-media sites designed to figure out our weaknesses and manipulate them to make us scroll and scroll and scroll; who are so stressed that we become hypervigilant; who eat diets that cause our energy to spike and crash; who are breathing in a chemical soup of brain-inflaming toxins every day – then, yes, we will continue to be a society with serious attention problems. (p272-273)\nYour focus needs certain things to be present: play for children and flow states for adults, to read books, to discover meaningful activities that you want to focus on, to have space to let your mind wander so you can make sense of your life, to exercise, to sleep properly, to eat nutritious food that makes it possible for you to develop a healthy brain, and to have a sense of safety. (p273)\nWe are living in an economic machine that requires greater speed to keep going – and that inevitably degrades our attention over time. (p279)\nMost people don’t want a fast life – they want a good life. (p279)"
  },
  {
    "objectID": "posts/012-vim-motions/index.html",
    "href": "posts/012-vim-motions/index.html",
    "title": "Vim Motions",
    "section": "",
    "text": "Image from YouTube channel @sagaratytube\n\n\nVim is a powerful, open-source text editor. It is unique in that you spend most of your time in Normal Mode where key presses execute commands that move and edit text, as opposed to Insert Mode where key presses enter text like a traditional word processor. There is also a Visual Mode and Command Mode, subjects for another post.\n\nMovement Commands\n\n\n\n\n\n\n\nh, j, k, l\nleft, down, up, right\n\n\nw, W, b, B, e, E\nword forward/backward, to start/end\n\n\n0, ^, $\nbeginning, first non-blank, or end of line\n\n\nf, t, F, T\nfind (or up to) forward/backward\n\n\n(, ), {, }\nsentence or paragraph, forward/backward\n\n\nH, M, L\nmove cursor to top, middle, bottom of screen\n\n\nzt, zz, zb\nmove screen so cursor is top, middle, bottom\n\n\nCtrl-F, Ctrl-B, Ctrl-D, Ctrl-U\nfull page, half page, up and down\n\n\ngg, G\ntop, bottom of doc\n\n\n/, ?, n, N\nsearch forward/backward, next result\n\n\n*, #\nnext/previous instance of word\n\n\ndouble back tick\njump to last cursor location\n\n\n%\njump to matching enclosing (paren, bracket, brace)\n\n\n\n\n\nEditing Commands\n\n\n\n\n\n\n\ni, a, I, A, o, O\ninsert before/after cursor, start/end of line, next/previous line\n\n\nx, r, s\ndelete/replace character and go to Normal/Insert Mode\n\n\ny, Y, yy, p\ncopy (end of line, whole line), paste\n\n\nd, dd, c\ndelete (whole line), change\n\n\nu, ., Ctrl-r\nundo, repeat, redo\n\n\n&gt;&gt;, &lt;&lt;\nindent, outdent"
  },
  {
    "objectID": "posts/853-peak/index.html",
    "href": "posts/853-peak/index.html",
    "title": "Peak: The Secrets from the New Science of Expertise",
    "section": "",
    "text": "Intro: The Gift\n\nBut since the 1990s brain researchers have come to realize that the brain - even the adult brain - is far more adaptable than anyone ever imagined, and this gives us a tremendous amount of control over what our brains are able to do.\nThe brain and the body are more adaptable in young children than in adults.\nThis book is about… the ability to create, through the right sort of training and practice, abilities that they would not otherwise possess by taking advantage of the incredible adaptability of the human brain and body. Furthermore, it is a book about how anyone can put this gift to work in order to improve in an area they choose. And finally, in the broadest sense this is a book about a fundamentally new way of thinking about human potential, one that suggests we have far more power than we ever realized to take control of our own lives.\nOver the past few years a number of books have argued that people have been overestimating the value of innate talent and underestimating the value of such things as opportunity, motivation, and effort. I cannot disagree with this, and it is certainly important to let people know that they can improve – and improve a lot – with practice… But sometimes these books leave the impression that heartfelt desire and hard work alone will lead to improved performance – “Just keep working at it, and you’ll get there” – and this is wrong. The right sort of practice carried out over a sufficient period of time leads to improvement. Nothing else.\nThe most effective approaches to improving performance all follow a single set of general principles. We named this universal approach “deliberate practice.”\n\n\n\n1. The Power of Purposeful Practice\n\nBut while the abilities are extraordinary, there is no mystery at all about how these people developed them. They practiced. A lot.\nWe all follow pretty much the same pattern with any skill we learn… We start off with a general idea of what we want to do, get some instruction from a teacher or a coach or a book or a website, practice until we reach an acceptable level, and then let it become automatic. And there’s nothing wrong with that. For much of what we do in life, it’s perfectly fine to reach a middling level of performance and just leave it like that.\nBut there is one very important thing to understand here: once you have reached this satisfactory skill level and automated your performance, you have stopped improving.\nResearch has shown that, generally speaking, once a person reaches that level of “acceptable” performance and automaticity, the additional years of “practice” don’t lead to improvement. If anything, the doctor or the teacher or the driver who’s been at it for twenty years is likely to be a bit worse that the one who’s been doing it for only five, and the reason is that these automated abilities gradually deteriorate in the absence of deliberate efforts to improve.\nPurposeful practice:\n\nhas well-defined, specific goals\nis focused\ninvolves feedback\nrequires getting out of one’s comfort zone\n\ngetting out of your comfort zone means trying to do something that you couldn’t do before.\ngenerally the solution is not “try harder” but rather “try differently.”\n\n\nOne caveat here is that while it is always possible to keep going and keep improving, it is not always easy.\nThere is an important lesson here: although it is generally possible to improve to a certain degree with focused practice and staying out of your comfort zone, that’s not all there is to it. Trying hard isn’t enough. Pushing yourself to your limits isn’t enough. There are other, equally important aspects to practice and training that are often overlooked.\n\n\n\n2. Harnessing Adaptability\n\nThe human body has a preference for stability.\nAs long as the physical exercise is not so strenuous that it strains the body’s homeostatic mechanisms, the exercise will do very little to prompt physical changes in the body. From the body’s perspective, there is no reason to change; everything is working as it should.\nIt’s a different matter when you engage in sustained, vigorous physical activity that pushes the body beyond the point where the homeostatic mechanisms can compensate.\nThis is how the body’s desire for homeostasis can be harnessed to drive changes: push it hard enough and for long enough, and it will respond by changing in ways that make that push easier to do.\nThis explains the importance of staying just outside your comfort zone: you need to continually push to keep the body’s compensatory changes coming, but if you push too far outside your comfort zone, you risk injuring yourself and actually setting yourself back.\nThe brain, like the body, changes most quickly in that sweet spot where it is pushed outside – but not too far outside – its comfort zone.\nAlthough the specific details vary from skill to skill, the overall pattern is consistent: regular training leads to changes in the parts of the brain that are challenged by the training. The brain adapts to these challenges by rewiring itself in ways that increase its ability to carry out the functions required by the challenges.\nThe effects of training on the brain can vary with age.\nDeveloping certain parts of the brain through prolonged training can come at a cost: in many cases people who have developed one skill or ability to an extraordinary degree seem to have regressed in another area.\nThe cognitive and physical changes caused by training require upkeep. Stop training, and they start to go way.\nThe reason that most people don’t possess these extraordinary physical capabilities isn’t because they don’t have the capacity for them, but rather because they’re satisfied to live in the comfortable rut of homeostasis and never do the work that is required to get out of it.\nBut it’s important to remember that the option exists. If you wish to become significantly better at something, you can.\nAnd here is the key difference between the traditional approach to learning and the purposeful-practice or deliberate-practice approaches: the traditional approach is not designed to challenge homeostasis.\nWith deliberate practice, however, the goal is not just to reach your potential but to build it, to make things possible that were not possible before. This requires challenging homeostasis – getting out of your comfort zone – and forcing your brain or your body to adapt.\n\n\n\n3. Mental Representations\n\nMuch of deliberate practice involves developing ever more efficient mental representations that you can use in whatever activity you are practicing.\nThese representations are preexisting patterns of information – facts images, rules, relationships, and so on – that are held in long-term memory and that can be used to respond quickly and effectively in certain types of situations. The thing all mental representations have in common is that they make it possible to process large amounts of information quickly, despite the limitations of short-term memory.\nWhat sets expert performers apart from everyone else is the quality and quantity of their mental representations. Through years of practice, they develop highly complex and sophisticated representations of the various situations they are likely to encounter in their fields… These representations allow them to make faster, more accurate decisions and respond more quickly and effectively in a given situation. This, more than anything else, explains the difference in performance between novices and experts.\nThe main thing that sets experts apart from the rest of us is that their years of practice have changed the neural circuity in their brains to produce highly specialized mental representations, which in turn make possible the incredible memory, pattern recognition, problem solving, and other sorts of advanced abilities needed to excel in their particular specialties.\nFor the experts we just described, the key benefit of mental representations lies in how they help us deal with information: understanding and interpreting it, holding it in memory, organizing it, analyzing it, and making decisions with it.\nThe more you study a subject, the more detailed your mental representation of it become, and the better you get at assimilating new information.\nThe superior organization of information is a theme that appears over and over again in the study of expert performers.\nThe main practice of deliberate practice is to develop effective mental representations, and, as we will discuss shortly, mental representations in turn play a key role in deliberate practice. The key change that occurs in our adaptable brains in response to deliberate practice is the development of better mental representations, which in turn open up new possibilities for improved performance.\nIn general, mental representations aren’t just the result of learning a skill; they also can help us learn.\nIn any area, not just musical performance, the relationship between skill and mental representations is a virtuous circle: the more skilled you become, the better your mental representations are, and the better your mental representations are, the more you can practice to hone your skill.\nIn these areas too, the virtuous circle rules: honing the skill improves mental representation, and mental representation helps hone the skill. There is a bit of a chicken-and-egg component to this. Take figure skating: it’s hard to have a clear mental representation of what a double axle feels like until you’ve done it, and likewise, it is difficult to do a clean double axle without a good mental representation of one. That sounds paradoxical, bit it isn’t really. You work up to a double axle bit by bit, assembling the mental representations as you go.\n\n\n\n4. The Gold Standard\n\nWhat is missing from purposeful practice? What is required beyond simply focusing and pushing beyond one’s comfort zone? Let’s talk about it.\nPurposeful practice as done by different people can have very different results.\nIn every area, some approaches to training are more effective that others. In this chapter, we’ll explore the most effective method of all: deliberate practice. It is the gold standard., the ideal to which anyone learning a skill should aspire.\nThese fields (classical music performance, mathematics, and ballet, as opposed to playing pop music, solving crossword puzzles, and folk dancing) have several characteristics in common.\n\nFirst, there are always objective ways to measure performance.\nSecond, these fields tend to be competitive enough that performers have strong incentive to practice and improve.\nThird, these fields are generally well established.\nAnd fourth, these fields have a subset of performers who also serve as teachers and coaches.\n\nEveryone from the very top students to the future music teachers agreed: improvement was hard, and they didn’t enjoy the work they did to improve.\nWe found that the best violin students had, on average, spent significantly more time that the better violin students had spent, and that the top two groups – better and best – had spent much more time on solitary practice that the music-education students.\nLooking more closely, we found that the largest differences in practice time among the three groups of students had come in the preteen and teenage years.\nBut two things were strikingly clear from the study: first, to become an excellent violinist requires several thousand hours of practice. We found no shortcuts and no “prodigies” who reached an expert level with relatively little practice. And, second, even among these gifted musicians – all of whom had been admitted to the best music academy in Germany – the violinists who had spent significantly more hours practicing their craft were on average more accomplished than those who had spent less time practicing.\nBy now it is safe to conclude from many studies on a wide variety of disciplines that nobody develops extraordinary abilities without putting in tremendous amounts of practice.\nThe improvement in performance generally has gone hand in hand with the development of teaching methods, and today anyone who wishes to become an expert in these fiends will need an instructor’s help.\nIn short, we were saying that deliberate practice is different from other sorts of purposeful practice in two important ways: first, it requires a field of study that is already reasonably well developed… Second, deliberate practice requires a teacher who can provide practice activities designed to help a student improve…\nWith this definition we are drawing a clear distinction between purposeful practice – in which a person tries very hard to push himself or herself to improve – and practice that is both purposeful and informed… Deliberate practice is purposeful practice that knows where it is going and how to get there.\nDeliberate practice:\n\ndevelops skills that other people have already figured out how to do and for which effective training techniques have been established\ntakes place outside one’s comfort zone… it demands near-maximal effort, which is generally not enjoyable\nrequires a person’s full attention and conscious actions\ninvolves feedback and modification of efforts in response to that feedback\nboth produces and depends on effective mental representations.\nnearly always involves building or modifying previously acquired skills.\n\nThis is the basic blueprint for getting better in any pursuit: get as close to deliberate practice as you can… In practice this often boils down to purposeful practice with a few extra steps: first, identify the expert performs, then figure out what they do that makes them so good, then come up with training techniques that allow you to do it, too.\nBe careful when identifying expert performs. Ideally you want some objective measure of performance.\nSeek out persons that professionals themselves seek out when they need help with a particularly difficult situation.\nThe ideal is to find objective, reproducible measures that consistently distinguish the best from the rest.\nOnce you’ve identified the expert performs in a field, the next step is to figure out specifically what they do that separates them from other, less accomplished people in the same field, and what training methods helped them get there.\nIn many fields it is the quality of mental representations that sets apart the best form the rest, and mental representations are, by their nature, not directly observable.\nFortunately, in some cases you can bypass figuring out what sets experts themselves apart from others and simply figure out what sets their training apart.\nIn all of this keep in mind that the idea is to inform your purposeful practice and point it in the directions that will be more effective.\nAnd finally remember that, whenever possible, the best approach is almost always to work with a good coach or teacher.\nA knowledgeable instructor can lead the student to develop a good foundation and then gradually build on that foundation to create the skills expected in that field.\nA good teacher can give you valuable feedback you couldn’t get any other way.\nThe distinction between deliberate practice aimed at a particular goal and generic practice is crucial because not every type of practice leads to the improved ability.\nBecoming accomplished in any field in which there is a well-established history of people working to become experts requires a tremendous amount of effort exerted over many years.\nIn pretty much any area of human endeavor, people have a tremendous capacity to improve their performance, as long as they train in the right way.\nThere is no point at which performance maxes out.\n\n\n\n5. Principles of Deliberate Practice on the Job\n\nHis message to clients starts with a mindset. The first step toward enhancing performance in an organization is realizing that improvement is possible only if participants abandon business-as-usual practices. Doing so requires recognizing and rejecting three prevailing myths.\nThe first is our old friend, the belief that one’s abilities are limited by one’s genetically prescribed characteristics… The right sort of practice can help pretty much anyone improve in just about any area they choose to focus on.\nThe second myth holds that if you do something for long enough, you’re bound to get better at it. Again, we know better. Doing the same thing over and over again in exactly the same way is not a recipe for improvement; it is a recipe for stagnation and gradual decline.\nThe third myth states that all it takes to improve is effort… Unless you are using practice techniques specifically designed to improve those particular skills, trying hard will not get you very far.\nThe deliberate-practice mindset offers a very different view: anyone can improve, but it requires the right approach. If you are not improving, it’s not because you lack innate talent; it’s because you’re not practicing the right way. Once you understand this, improvement becomes a matter of figuring out what the “right way” is.\nFor anyone in the business or professional word looking for an effective approach to improvement, my basic advice is to look for one that follows the principals of deliberate practice:\n\nDoes it push people to get outside their comfort zones and attempt to do things that are not easy for them?\nDoes it offer immediate feedback on the performance and on what can be done to improve it?\nHave those who developed the approach identified the best performs in that particular area and determined what sets them apart from everyone else?\nIs the practice designed to develop the particular skills that experts in the field possess?\n\nOne of the major challenges facing anyone trying to apply the principles of deliberate practice is figure out exactly what the best performers do that sets them apart.\nPractice skills over and over again with plenty of feedback and without the usual costs of failure\nTraining with immediate feedback… can be an incredibly powerful way to improve performance.\nThere is an emphasis on doing. The bottom line is what you are able to do, not what you know, although it is understood that you need to know certain things in order to be able to do your job.\nThis distinction between knowledge and skills lies at the heart of the difference between traditionally paths toward expertise and the deliberate-practice approach… Deliberate practice focuses solely on performance and how to improve it.\nWhen you look at how people are trained in the professional and business worlds, you find a tendency to focus on knowledge at the expense of skills. The main reasons are tradition and convenience: it is much easier to present knowledge to a large group of people than it is to set up conditions under which individuals can develop skills through practice.\nContinuing medical education can improve doctors’ performance, but the effect is small, and the effects on patient outcomes are even smaller. In addition, it is mainly those education approaches with some interactive component that have an effect; lectures, seminars, and the like to little or nothing to help doctors improve their practice.\nIn general, professional schools focus on knowledge rather than skills because it is much easier to teach knowledge and then create tests for it. The general argument has been that the skills can be mastered relatively easily if the knowledge is there… The assumption is that simply accumulating more experience will lead to better performance.\nThe right question is, How do we improve the relevant skills? rather than, How do we teach the relevant knowledge?\n\n\n\n6. Principles of Deliberate Practice in Everyday Life\n\nWhen you’re practicing by yourself, you have to rely upon your own mental representations to monitor your performance and determine what you might be doing wrong. This is not impossible, but it is much more difficult and less efficient that having an experienced teacher watching you and providing feedback. It is particularly difficult early in the learning process, when your mental representations are still tentative and inaccurate.\nMany accomplished performers are terrible teachers because they have no idea how to teach. Just because they themselves can do it doesn’t mean they can teach others how to do it.\nThe basic principle – the importance of engaging in purposeful practice instead of mindless repetition without any clear plan for getting better… Remember: if your mind is wandering or you’re relaxed and just having fun, you probably won’t improve.\nAnd, of course, in fields where strength and endurance are not so important – intellectual activities and so on – there is little point at all to practicing if you don’t focus.\nMaintaining this sort of focus is hard work, however, even for experts who have been doing it for years.\nFocus and concentration are crucial… so shorter training sessions with clearer goals are the best way to develop new skills faster. It is better to train at 100 percent effort for less time that at 70 percent efforts for a longer period.\nThe hallmark of purposeful or deliberate practice is that you try to do something you cannot do – that takes you out of your comfort zone – and that you practice it over and over again, focusing on exactly how you are doing it, where you are falling short, and how you can get better. Real life seldom gives us the opportunity for this sort of focused repetition, so in order to improve, we must manufacture our own opportunities.\nNote that these students weren’t simply doing the same thing over and over again: they were paying attention to what they got wrong each time and correcting it. This is purposeful practice. it does no good to do the same thing over and over again mindlessly; the purpose of the repetition is to figure out where your weaknesses are and focus on getting better in those areas, trying different methods to improve until you find something that works.\nTo effectively practice a skill without a teacher, it helps to keep in mind three Fs: focus, feedback, fix it. Break the skill down into components that you can do repeatedly and analyze effectively, determine your weaknesses, and figure out ways to address them.\nDespite the first word in the term “mental representation,” pure mental analysis is not nearly enough. We can only form effective mental representations when we try to reproduce what the expert performer can do, fail, figure out why we failed, try again, and repeat – over and over again. Successful mental representations are inextricably tied to actions, not just thoughts, and it is the extended practice aimed at reproducing the original product that will produce the mental representations we seek.\nThe plateau Josh encountered is common in every sort of training. When you first start learning something new, it is normal to see rapid – or at least steady – improvement, and when that improvement stops, it is natural to believe you’ve hit some sort of implacable limit. So you stop trying to move forward, and you settle down to life on that plateau. This is the major reason that people in every area stop improving.\nThe best way to move beyond it is to challenge your brain or you body in a new way.\nAny reasonably complex skill will involve a variety of components, some of which you will be better at than others. Thus, when you reach a point at which you are having difficulty getting better, it will be just one or two of the components of that skill, not all of them that are holding you back. The question is, which ones? To figure that out, you need to find a way to push yourself a little – not a lot – harder than usual. This will often help you figure out where your sticking points are.\nWhen other techniques have failed, push yourself sell outside of your comfort zone and see what breaks down first.\nGetting started is easy. How do you keep going?\nSo that’s the problem in a nutshell: purposeful practice is hard work… What can you do about it.\nIn answering that question, the first thing to note is that, despite the effort that it takes, it certainly is possible to keep going.\nWillpower is a very situation-specific attribute. People generally find it much easier to push themselves in some areas than in others.\nIt is much more useful, I believe, to talk about motivation. Motivation is quite different from willpower. We all have various motivations - some stronger, some weaker – at various times and in various situations. The most important question to answer then becomes, what factors shape motivation?\nA similar thing is true for those who maintain purposeful or deliberate practice over the long run. They have generally developed various habits that help them keep going.\nThus to maintain your motivation you can either strengthen the reasons to keep going or weaken the reasons to quit. Successful motivation efforts generally include both.\nGood planning can help you avoid many of the things that might lead you to spend less time on practice than you wanted.\nExpert performers do two things - both seemingly unrelated to motivation – that can help. The first is general physical maintenance: getting enough sleep and keeping healthy… The second thing is to limit the length of your practice sessions to about an hour.\nStudies of expert performs tell us that once you have practice for a while and can see the results, the skill itself can become part of your motivation. You take pride in what you do, you get pleasure from your friends’ compliments, and your sense of identity changes.\nAnother key motivational factor in deliberate practice is a belief that you can succeed.\nOne of the strongest forms of extrinsic motivation is social motivation. This can take several forms. One of the simplest and most direct is the approval and admiration of others.\nSurrounding yourself with supportive people is easiest in activities that are done in groups or teams.\nOf course, at its core, deliberate practice is a lonely pursuit.\nOne of the best bits of advice is to set things up so that you are constantly seeing concrete signs of improvement, even if it is not always major improvement.\n\n\n\n7. The Road to Extraordinary\n\nPsychologists have found that an expert’s development passes through four distinct stages.\nIn the first stage, children are introduced in a playful way to what will eventually become their field of interest.\nSimply by interacting strongly with their children, parents motivate their children to develop similar interests.\nA child who sees an older sibling performing an activity and getting attention and praise from a parent will naturally want to join in and garner some attention and praise as well.\nOnce a future expert performer becomes interested and shows some promise in an area, the typical next step it to take lessons from a coach or teacher.\nDuring the first part of this state, the encouragement and support of parents and teachers was crucial to the child’s progress, but eventually the students began to experience some of the rewards of their hard work and became increasingly self-motivated.\nFinally, as the students continued to improve, they started to seek out better-qualified teachers and coaches who would take them to the next level.\nAfter two to five years at this stage, the future experts began to identify themselves more in terms of the skill they were developing and less in terms of other areas of interest, such as school or social life. They saw themselves as “pianists” or “swimmers” by the age of eleven or twelve.\nGenerally when they’re in the early or mid teens, the future experts make a major commitment to becoming the best that they can be. This commitment is the third stage.\nNow students will often seek out the best teachers or schools for their training, even if it requires moving across country.\nIn Bloom’s study, all 120 experts had begun their climb toward that pinnacle as children, which is typical among expert performers. But people frequently ask me what the possibilities are for someone who doesn’t begin training until later in life. While the specific details vary by field, there are relatively few absolute limitations on what is possible for people who begin training as adults. Indeed, the practical limitations – such as the fact that few adults have four to five hours a day to devote to deliberate practice – are often more of an issue than any physical or mental limitations.\nBeethoven, Newton, Darwin, Einstein, Tiger Woods… These are the people whose contributions leave their fields forever changed, the pathfinders who lead the way into new territory so that others can follow. This is the fourth stage of expert performance, where some people move beyond the existing knowledge in their field and make unique creative contributions. It is the least well understood of the four stages and the most intriguing.\nHaving studied many examples of creative genius, it’s clear to me that much of what expert performs do to move the boundary of their fields and create new things is very similar to what they were doing to reach that boundary in the first place.\nThe most important lesson they gleaned form their teachers is the ability to improve on their own.\nThere are no big leaps, only developments that look like big leaps to people from the outside because they haven’t seen all of the little steps that comprise them.\nCreativity goes hand in hand with the ability to work hard and maintain focus over long stretches of time – exactly the ingredients of deliberate practice that produced their expert abilities in the first place.\nProgress is made by those who are working on the frontiers of what is known and what is possible to do, not by those who haven’t put in the effort needed to reach that frontier.\n\n\n\n8. But What About Natural Talent\n\nExpert performers develop their extraordinary abilities through years and years of dedicated practice, improving step-by-step in a long, laborious process. There are no shortcuts. Various sorts of practice can be effective, but the most effective of all is deliberate practice. Deliberate practice takes advantage of the natural adaptability of the human brain and body to create new abilities. Most of these abilities are created with the help of detailed mental representations, which allow us to analyze and respond to situations much more effectively than we could otherwise.\nI get it. People want to believe that there is magic in life, that not everything has to abide by the staid, boring rules of the real world. And what could be more magical than being born with some incredible ability that doesn’t require hard work or discipline to develop? There is an entire comic-book industry build on that premise – that sometimes something magical happens, and you suddenly acquire incredible powers.\nBut my decades of research in the area of expertise have convinced me that there is no magic. By examining the case of someone with exceptional abilities through the lens of those two earlier questions I posed – What is the talent? What practice led to the talent? – you can pull back the curtain and find what is really going on.\nPeople do not stop learning and improving because they have reached some innate limits on their performance; they stop learning and improving because, for whatever reasons, they stopped practicing – or never started. There is no evidence that any otherwise normal people are born without the innate talent to sing or do math or perform any other skill.\nThink back to when you were a kid and you were just starting to learn to play the piano or to throw a baseball or to draw something… In all of these cases, when you looked around you would have noticed that some of your friends or classmates or peers were doing better than others, and some were doing worse. There are always obvious difference sin how quickly different people pick something up. Some just seem to have an easier time playing a musical instrument. Some just seem to be natural athletes. Some just seem to be naturally good with numbers. And so on. And because we see such differences in beginners, it’s natural to assume that those differences will persist – that the same people who did so well in the beginning will continue to breeze through later on. These lucky people, we imagine, were born with innate talents that smooth the way and lead them to excel. This is an understandable result of observing the beginning of the journey and concluding that the rest of the journey will be similar. It is also wrong.\nAnd here we find our major take away message: in the long run it is the ones who practice more who prevail, not the ones who had some initial advantage in intelligence or some other talent.\nAnd over time these children will become better artists or better musicians than their peers – not because they are innately more talented in the sense that they have some genes for musical or artistic ability, but because something – perhaps genetic – pushed them to practice and thus develop their skills to a greater degree than their peers.\nSome people might, for instance, be naturally able to focus more intently for longer periods of time that others.\nOne could even imagine difference in how the brain responds to the challenges so that practice would be more effective in some people than in others in building new brain structures and mental capacity.\nMuch of this remains speculative at this point. But since we know that practice is the single most important factor in determining a person’s ultimate achievement in a given domain, it makes sense that if genes to play a role, their role would play out through shaping how likely a person is to engage in deliberate practice or how effective that practice is likely going to be. Seeing it in this way puts genetic differences in a completely different light.\nI’ve argued that while innate characteristics may influence performance among those who are just learning a new skill or ability, the degree and the effectiveness of training plays a more significant role in determining who excels among those who have worked to develop a skill.\nThis is the dark side of believing in innate talent. it can beget a tendency to assume that some people have a talent for something and others don’t and that you can tell the difference early on. If you believe that, you encourage and support the “talented” ones and discourage the rest, creating the self-fulfilling prophecy.\n\n\n\n9. Where Do We Go from Here?\n\nA major difference between the deliberate-practice approach and the traditional approach to learning lies with the emphasis placed on skills versus knowledge – what you can do versus what you know. Deliberate practice is all about the skills. You pick up the necessary knowledge in order to develop the skills; knowledge should never be an end in itself. Nonetheless, deliberate practice results in students picking up quite a lot of knowledge along the way.\nIf you teach a student facts, concepts, and rules, those things to into long-term memory as individual piece, and if a student then wishes to do something with them – use them to solve a problems, reason with them to answer a question, or organize and analyze them to come up with a theme or a hypothesis - the limitations of attention and short-term memory kick in. The student must keep all of these different, unconnected pieces in mind while working with them toward a solution. However, if this information is assimilated as part of building mental representations aimed at doing something, the individual pieces become part of an interconnected pattern that provides context and meaning to the information, making it easier to work with.\nYou don’t build mental representations by thinking about something; you build them by trying to do something, failing, and revising, and trying again, over and over. When you’re done, not only have you developed an effective mental representation for the skill you were developing, but you have also absorbed a great deal of information connected with that skill.\nThe questions and tasks were also designed to push the (physics) students outside their comfort zones – to ask them questions whose answers they’d have to struggle for – but not so far outside their comfort zones that they wouldn’t know how to start answering them.\nFinally, the classes were structured so that the students would have the opportunity to deal with the various concepts over and over again, getting feedback that identified their mistakes and showed how to correct them.\nBegin by identifying what students should learn how to do. The objectives should be skills, not knowledge. In figuring out the particular way students should learn a skill, examine how the expert do it. In particular, understand as much as possible about the mental representations that experts use, and teach the skill so as to help students develop similar mental representations. This will involve teaching the skill step by step, with each step designed to keep students out of the comfort zone but not so far out that they cannot master that step. Then give plenty of repetition and feedback; the regular cycle of try, fail, get feedback, try again, and so on is how the students will build their mental representations."
  },
  {
    "objectID": "posts/011-terminal-on-mac/index.html",
    "href": "posts/011-terminal-on-mac/index.html",
    "title": "Shell and Terminal on Mac",
    "section": "",
    "text": "Most users of personal computers today are intimately familiar with the graphical user interface (GUI) where interactions with the computer typically involve using a mouse (and sometimes the keyboard) to click and navigate through windows, icons, menus, and buttons.\nBefore GUIs became common, computers were often operated through a text-based method called a command-line interface (CLI). And this approach is sill widely used today.\nIn the “old days,” most computers would boot into the CLI because there was no graphical altnernative. Today, every major operating system still includes a way to access the command line through an application called a terminal emulator (or just terminal for short). On macOS, the app is unceremoniously named “Terminal” and can be found in the /Applications/Utilities directory. But you can download other terminal applications, including iTerm2, Warp, Alacritty, Hyper, Tabby, Kitty, Rio, or others.\nEvery terminal application has a text-based input field called the command line that begins with a prompt (usually &gt; or $). When you type and submit a command, the Shell Command Line Interpreter (or just Shell) parses and interprets the text you have typed, carrying out your instructions and possibly returning information to you in the terminal. This Read-Eval-Print loop (REPL) is common when computer programs allow “live” interaction between the user and computer.\nThe default shell on macOS at the time of writing is zsh (pronounced the “Z shell”) which replaced bash (the “Bourne Again SHell”) as the macOS default starting with the release of macOS Catalina in 2019. bash, as evidenced by its name, was an evolution of the Bourne shell (with the short name sh). To round out the list, the only other popular shell today is fish (the “friendly” shell)."
  },
  {
    "objectID": "posts/011-terminal-on-mac/index.html#whats-what",
    "href": "posts/011-terminal-on-mac/index.html#whats-what",
    "title": "Shell and Terminal on Mac",
    "section": "",
    "text": "Most users of personal computers today are intimately familiar with the graphical user interface (GUI) where interactions with the computer typically involve using a mouse (and sometimes the keyboard) to click and navigate through windows, icons, menus, and buttons.\nBefore GUIs became common, computers were often operated through a text-based method called a command-line interface (CLI). And this approach is sill widely used today.\nIn the “old days,” most computers would boot into the CLI because there was no graphical altnernative. Today, every major operating system still includes a way to access the command line through an application called a terminal emulator (or just terminal for short). On macOS, the app is unceremoniously named “Terminal” and can be found in the /Applications/Utilities directory. But you can download other terminal applications, including iTerm2, Warp, Alacritty, Hyper, Tabby, Kitty, Rio, or others.\nEvery terminal application has a text-based input field called the command line that begins with a prompt (usually &gt; or $). When you type and submit a command, the Shell Command Line Interpreter (or just Shell) parses and interprets the text you have typed, carrying out your instructions and possibly returning information to you in the terminal. This Read-Eval-Print loop (REPL) is common when computer programs allow “live” interaction between the user and computer.\nThe default shell on macOS at the time of writing is zsh (pronounced the “Z shell”) which replaced bash (the “Bourne Again SHell”) as the macOS default starting with the release of macOS Catalina in 2019. bash, as evidenced by its name, was an evolution of the Bourne shell (with the short name sh). To round out the list, the only other popular shell today is fish (the “friendly” shell)."
  },
  {
    "objectID": "posts/011-terminal-on-mac/index.html#shell-basics",
    "href": "posts/011-terminal-on-mac/index.html#shell-basics",
    "title": "Shell and Terminal on Mac",
    "section": "Shell Basics",
    "text": "Shell Basics\nFirst, let’s determine what shell you are using. Open your terminal application and type echo $SHELL at the command prompt. If the response is /bin/zsh then its zsh.\nMost commands tend to have the following structure\n[command] [options] [arguments]\nYou’ll need help. Often. The commands run-help and man (short for manual) following by a command name will bring up the command’s documentation using the terminal pager less. For navigation, arrow keys move one line at a time, page up/down and the spacebar move one page at a time, and q exits. For more succinct documentation, visit https://tldr.sh/ in a browser, which can be used interactively or downloaded.\n\nMoving around\nNavigating the hierarchy of directories and performing simple operations on files are fundamental to working from the command line. Note that many of the following tools take the verbose -v option, which prints a confirmation message in the terminal and is useful to “see” what these commands do, especially while learning them.\n\npwd prints the current (working) directory\ncd changes the working directory\nls list the contents of a directory\n.. denote the parent directory\n. denotes the current directory (useful, eg, when copying)\n\nCreating, moving, copying, and deleting files and directories are common operations:\n\ntouch updates the access and modification times of files, but also creates a file if it does not yet exist\nmv move (or rename) one or more files or directories\ncp copy one or more files or directories\nrm remove (ie, permanently delete) file(s). A good alternative is to “trash” them with mv file_name ~/.Trash\nmkdir create a new directory\nrmdir removes directories (use with caution!) A good alternative is to “trash” them by installing the trash command with brew install trash and then running trash dir_name\n\nIt is often useful to use wildcards to return a “group” of files. Common wildcards include:\n\n* denotes zero or more characters\n? a single character\n[abc] a list of permited characters\n[a-z] a set of characters\n\n\n\nEditing Text at the Command Line\nWhen working in a terminal, you’ll often need to edit what you’ve typed, e.g., to fix a typo, move back a few words, delete a chunk of text, or recall and modify a previous command. While you can do this with arrow keys and backspace, you’ll become dramatically faster once you learn a handful of editing shortcuts.\nMost shells (including zsh and bash) provide “line editing,” meaning you can edit the current command before you run it. These editing features come from a library called readline (used by bash) or a similar system called zle (the Zsh Line Editor). Both support two classic styles of keybindings: Emacs-style editing (the default in most shells) and Vi-style editing (popular among Vim users). These styles trace back to two influential text editors from the 1970s: Emacs and vi, which shaped how programmers interact with text even to this day.\n\nEmacs-style keybindings (default)\nIn Emacs mode, you use Control key combinations to move and edit efficiently. These are widely supported across shells and many terminal programs. Here are the most useful ones to memorize. Note that on the macOS Terminal, “Alt” is often the Option key.\n\nCtrl-A jump to the beginning of the line\nCtrl-E jump to the end of the line\nCtrl-B move back one letter\nCtrl-F move forward one letter\nAlt-B or Esc-B move back one word\nAlt-F or Esc-F move forward one word\nCtrl-L clear the screen\nCtrl-U delete from cursor to the beginning of the line (“undo this whole front part”)\nCtrl-K delete from cursor to the end of the line (“kill to end”)\nCtrl-W delete the word behind the cursor\nCtrl-P previous command (same as up arrow)\nCtrl-N next command (same as down arrow)\nCtrl-R reverse search through history (Start typing part of a previous command and press Ctrl-R repeatedly to cycle matches.)\n\n\n\nVi-style keybindings (modal editing)\nVi mode is “modal,” meaning you switch between modes: “Insert” mode where you type normally, or “Normal” mode where keys perform editing commands. This feels strange at first, but you can become extremely fast once it clicks. Vi’s reputation is that you can move mountains in a few keystrokes, but with the cost that even simple operations also require a few keystrokes.\nTo change from the default emacs-style key binding to vi-style for the current shell session, run bindkey -v or set -o vi. To enable vi-style keybinding every time you start an interactive shell, add one of those commands to your shell configuration file (typically ~/.zshrc for zsh or ~/.bashrc for bash). Then either restart your terminal or reload the configuration with source ~/.zshrc.\nOnce enabled, when you’re typing a command you’re usually in insert mode. Press:\n\nEsc switch to normal mode\ni return to insert mode at the cursor\na return to insert mode after the cursor\n\nIn normal mode, a typical operation folows the pattern count-operator-motion/object. For example, 3dw deletes forward three words. Omitting the count is typica, for example ci( changes the text inside the current set of parentheses. Common commands include:\nMovement of cursor\n\nh/l left/right\n0 beginning of line\n$ end of line\nw forward one word\nb back one word\n\nEditing (normal mode)\n\nx delete character under cursor\ndw delete a word\ndd delete the entire line\ncw change a word (delete word and enter insert mode)\nD delete from cursor to end of line\n\nHistory search\n\nj/k up/down\n/text search backward in history for “text”\nn repeat the last search\n\nA common workflow looks like: type a command; hit Esc; quickly edit with w, b, dw, etc.; press i to insert again; run.\n\n\n\nCombining tools\nThe Unix philosophy is that command line tools are designed to do one thing and to do it well. Therefore, complex operations are created through a composition of tools. This is made possible by managing the communication streams of these tools. Each has 3 standard communication streams: standard input (stdin), standard output (stdout), and standard error (stderr). Often, the keyboard will not be the source of input, but rather it will be from the output generated by other tools and contents of files.\n\n&lt; takes input from a file (eg, &lt; file.txt wc)\n&gt; directs output to a file and overwrites (eg, ls &gt; dir_contents.txt)\n&gt;&gt; directs output to a file and appends\n| pipes output from one command to input of another (eg, ls | less)\n\nCommon tools for inspecting files, or for gathering and summarizing their contents include:\n\ncat for concatenating files; can be used to print file contents in the terminal for short files\nfind for recursively searching a directory hierarchy returning names of files and/or directories\nwc for counting characters, words, and lines\nhead for obtaining the first few lines\ntail for obtaining the last few lines\nsort for sorting the contents of a file\nuniq for removing adjacent duplicates\n\nMore-advanced tools that deserve their own section and explanation\n\ngrep for matching a Regular Expression pattern in one or more files\nsed a stream editor for modifying data\nawk a data processing language named after its creators\n\n\n\nRegular Expressions\n\n\nSections to add\n\nFile permissions (read/write/execute)\nA whole section on each of grep/sed/awk\nConnecting: ssh/ftp/curl\nEnvironment variables (incl. customizing the prompt)\nWriting programs (incl. if/for/while)"
  },
  {
    "objectID": "posts/855-war-of-art/index.html",
    "href": "posts/855-war-of-art/index.html",
    "title": "Scott Pressfield Books",
    "section": "",
    "text": "The War of Art\n\nThere’s a secret that real writers know that wannabe writers don’t, and the secret is this: It’s not the writing part that’s hard. What’s hard is sitting down to write.\n\n\n1. Resistance\n\nResistance is the enemy within. 8\nResistance will tell you anything to keep you from doing your work. It will assume any form, if that’s what it takes to deceive you. 9\nWhen a writer begins to overcome her Resistance she may find that those close to her begin acting strange. They are trying to sabotage her. 19\nThis second, we can sit down and do our work. 22\n[Resistance is something] you can tell by the measure of hollowness you feel afterward. 23\nTrouble is a cheap way to get attention. Cruelty to others is a form of resistance. 24\nCasting yourself as a victim is the antithesis of doing your work. Don’t do it. 28\nWhat finally convinced me to go ahead was simply that I was so unhappy not going ahead. As soon as I sat down and began, I was okay. 30\nWhat does resistance feel like? First, unhappiness. We feel like hell. A low-grade misery pervades everything. We’re bored, we’re restless. We can’t get no satisfaction. There’s guilt but we can’t put our finger on the source. We want to go back to bed; we want to get up and party. We feel unloved and unlovable. We’re disgusted. We hate our lives. We hate ourselves. 31\nThe human being isn’t wired to function as an individual. We’re wired tribally, to act as part of a group. 33\nThe paradox seems to be, as Socrates demonstrated long ago, that the truly free individual is free only to the extent of his own self-master. While those who will not govern themselves are condemned to find masters to govern over them. 37\nIf you find yourself criticizing other people, you’re probably doing it out of resistance. When we see other beginning to live their authentic selves, it drives us crazy if we have not lived out our own. Individuals who are realized in their own lives almost never criticize others. 38\nThe counterfeit innovator is wildly self-confident. The real one is scared to death. 39\nThe more scared we are of a work or calling, the more sure we can be that we have to do it. 40\nThe professional concentrates on the work and allows rewards to come or not, whatever they like. 43\nWhat counted was that I had, after years of running from it, actually sat down and done my work. 50\nWhat’s particularly insidious about the rationalizations that resistance presents to us is that a lot of them are true. They’re legitimate. What resistance leaves out, of course, is that all this means diddly. 55\n\n\n\n2. Turning Pro\n\nYou must know the difference between what is urgent and what is important, and you must do what’s important first. What’s important is the work. 65\nThe marine corps teaches you how to be miserable. The artist must be like that marine. He has to know how to be miserable. He has to love being miserable. 68\nWe show up every day; no matter what; all day. 70\nSo you’re taking a few blows. That’s the price for being in the arena and not on the sidelines. 72\nResistance gets us to plunge into a project with an overambitious and unrealistic timetable for its completion. It knows we can’t sustain that level of intensity. We will hit the wall. We will crash. The professional arms himself with patience. He knows that any job takes twice as long as he thinks and costs twice as much. He accepts that. 75\nThe professional shuts up. She does her work. 78\nThe amateur believes he must first overcome his fear; then he can do his work. The professional knows that fear can never be overcome. 79\nThe professional dedicates himself to mastering technique not because he believes technique is a substitute for inspiration but because he wants to be in possession of the full arsenal of skills when inspiration does come. 84\nThe professional will work harder. 88\nTiger Woods could have groaned or sulked or surrendered mentally to this injustice, this interference, and used it as an excuse to fail. He didn’t. No matter what blow had befall him from an outside agency, he himself still had his job to do. 92\nMaking yourself a corporation (or just thinking of yourself in that way) reinforces the idea of professionalism. 97\nThe essence of professionalism is the focus upon the work and its demands, while we are doing it, to the exclusion of all else. 99\nThere’s no mystery to turning pro. It’s a decision brought about by an act of will. We make up our mind to view ourselves as pros and we do it. Simple as that. 101\n\n\n\n3. The Higher Realm\n\nThe most important thing about art is to work. Nothing else matters except sitting down every day and trying. It’s an attitude of egolessness and service. 108\nNobody knew I was done. Nobody cared. But I knew. Next morning I went over to Paul’s for coffee and told him I had finished. “Good for you,” he wait without looking up. “Start the next one today.” 112\nWhatever you can do, or dream you can, begin it. Boldness has genius, magic, and power in it. Begin it now. 122\nAngels are like muses. They know stuff we don’t. They want to help us. They’re on the other side of a pane of glass, shouting to get our attention. But we can’t hear them. We’re too distracted by our own nonsense. Ah, but when we begin. When we make a start. When we conceive an enterprise and commit to it in the fact of our fears, something wonderful happens. When we make a beginning, we get out of our own way and allow the angels to come in and do their jobs. 123\nFear of failure. These are serious fears. But they’re not the real fear. Fear that we will succeed. We fear discovering that we are more than we think we are. 142\nIndividuals define themselves in one of two ways: by their rank within a hierarchy or by their connection to a territory. Hierarchical seems to be the default setting. It’s only later in life, usually after a stern education in the university of hard knocks, that we begin to explore the territorial alternative. 147\nfor the artist to define himself hierarchically is fatal. But the artist cannot look to others to validate his efforts or his calling. The artist must operate territoriality. He must do his work for its own sake. 151\nThe hack is scared of being authentic. In other words, the hack writes hierarchically. He writes what he imagines will play well in the eyes of other. He does not ask himself, what do I myself want to write? 152\nA territory can only be claimed alone. A territory can only be claimed by work. 155\nIf I were the last person on earth, would I still do it? 158\nWe must do our work for its own sake, not for fortune or attention or applause. 161\nIn the end, the question can only be answered by action. Do it or don’t do it. Creative work is not a selfish act or a bid for attention on the part of the actor. It’s a gift to the world and every being in it. Don’t cheat us of your contribution. Give us what you’ve got. 165\n\n\n\n\n\nTurning Pro\n\n1. The Amateur Life\n\nThe thesis of this book is that what ails you and me has nothing to do with being sick or being wrong. What ails us is that we are living our lives as amateurs. What we get when we turn pro is, we find our power. We find our will and our voice and we find our self-respect. We become who we always were but had, until then, been afraid to embrace and to live out. 5\nAmbition. 9\nI was hiding. 12\nBut a shadow career entails no real risk. If we fail at a shadow career, the consequences are meaningless to us. Are you pursuing a shadow career? 13\nI wasn’t facing my demons. 15\nIn the shadow life, we live in denial and we act by addition. 18\nA professional has professional habits. 20\nWhen you turn pro, life gets very simply. The amateur is an egotist. He takes the material of his personal pain and uses it to draw attention to himself. 24\nWe’re asleep. We know only that something is wrong and we don’t know how to fix it. Addition replaces aspiration. 25\nAll additions embody repetition without progress and produce incapacity as a payoff. 34\nResistance hates two qualities above all others: concentration and depth. 39\nWhat you and I are really seeking is our own voice, our own truth, our own authenticity. 40\nI didn’t talk to anybody during my year of turning pro. I didn’t hang out. I just worked. That year made me a pro. It gave me, for the first time in my life, an uninterrupted stretch of month after month that was mine alone, that nobody knew about but me, when I was truly productive, truly facing my demons, and truly working my shit. 42–44\n\n\n\n2. Self-Inflicted Wounds\n\nThe habits and addictions of the amateur are conscious or unconscious self-inflicted wounds. Their payoff is incapacity. 51\nFear is the primary color of the amateur’s interior world. Fear of failure, fear of success, fear of looking foolish, fear of under-achieving and fear of over-achieving, fear of poverty, fear of loneliness, fear of death. 53\nThe professional, by the way, is just as terrified as the amateur. 54\nThe amateur is a narcissist. He views the world hierarchically. He continuously rates himself in relation to others. 55\nThe amateur allows his worth and identity to be defined by others. The amateur is tyrannized by his imagined conception of what is expected of him. He is imprisoned by what he believes he ought to think, how he ought to look, what he ought to do, and who he ought to be. 56\nIn his heart, the amateur knows he’s hiding. If the amateur had empathy for himself, he could look into the mirror and not hate what he sees. Achieving this compassion is the first powerful step toward moving from being an amateur to being a pro. 61\nThe force that can save the amateur is awareness. To act upon this self-awareness would mean defining herself. 67\nHere is the truth: the tribe doesn’t give a shit. 67\nWhen we turn pro, everything becomes simple. Our aim centers on the ordering of our days in such a way that we overcome the fears that have paralyzed us in the past. This changes our days completely. 72\nRe-commit every day. 74\n\n\n\n3. The Professional Mindset\n\nThe professional, unlike the amateur, shows up every day, all day; is committed; and the stakes are high and real. Further, the professional is patient, seeks order, demystifies, acts in the face of fear, accepts no excuses, plays it as it lays, is prepared, does not show off, dedicates himself to mastering technique, does not hesitate to ask for help, does not take failure or success personally, doe not identify with his or her instrument, endures adversity, self-validates, reinvents herself, and is recognized by other professionals. 90–91\nThe professional does not wait for inspiration, he acts in anticipation of it. 99\nThe professional is happy to teach, but refuses to be iconized. 102\nFirst, the pro mindset is a discipline that we use to overcome resistance. To defeat the self-sabotaging habits of procrastination, self-doubt, susceptibility to distraction, perfectionism, and shallowness, we enlist the self-strengthening habits of order, regularity, discipline, and a constant striving after excellence. That’s not hard to understand. In order to achieve “flow,” magic, “the zone,” we start by being common and ordinary and workmanlike. 103\nThere is a financial salary and a psychological salary. 105\nwhen we do the work for itself alone, it turns into a practice. 106\nIntention. Dedication. Commitment. 110\nThe amateur believes she must have all her ducks in a row before she can start. The professional knows better. Athletes play hurt. Warriors fight scared. 112\n\n\n\n\n\nDo The Work\n\n1. Beginning\n\nDon’t prepare. Begin. Start before you are ready. 18\nWe want to work, not prepare to work. 20\nWhat is your work about? When you know that, you’ll know the end state. And when you know the end state, you’ll know the steps to take to get there. 27\nThese are not thoughts. They are chatter. I was thirty years old before I had an actual thought. 29\n\n\n\n2. Middle\n\nDo research early or late. Don’t stop working. Never do research during prime working time. 35\nOne rule for first full working drafts: get them done ASAP. Don’t’ worry about quality. Act, don’t reflect. Momentum is everything.\nYou are not allowed to judge yourself. Suspending self-judgment doesn’t just mean blowing off the “you suck” voice in our heads. It also means liberating ourselves from conventional expectation – from what we think our work “ought” to be or “should” look like. 39\nKeep working. Keep working. Keep working. Keep working. Keep working. 47\nThere is an enemy. Step one is to recognize this. The enemy is inside us. 56–61\nThere is no way to be nice to the dragon. 62\nThe dream is your project, your vision, your symphony, your startup. The love is the passion and enthusiasm that fill your heart when you envision your project’s completion. 66\nHow bad do you want it? Why do you want it? 68\nInevitably, everything crashes. Bank on it. It’s gonna happen. 72\nSolve the problem. Creative panic is good. 76\nThe problem is not us. The problem is the problem. Work on the problem. 78\nIt’s hard because it’s hard. 79\n\n\n\n3. End\n\nBecause finishing is the critical part of any project. If we don’t finish, all our work is for nothing. 87\nSlay that dragon once, and he will never have power over you again. Yeah, he’ll still be there. Year, you still have to duel him every morning. But you will have beaten him once, and you’ll know you can beat him again. 94–95\nI finally, after 17 years of trying, finished my first novel. I drove over to my friend’s house and told him what I had done. “Good for you,” he said. “Now start the next one.”\n\n\n\n\n\nNobody Wants to Read Your Sh*t\n\nI learned practically nothing because i was alone and kept making the same mistakes over and over. 1\nNobody wants to read your shit. 1\nIn the real world, no one is waiting to read what you’ve written. Sight unseen, they hate what you’ve written. Why? because they might have to actually read it. Nobody wants to read anything. 4\nMake it so compelling that a person would have to be crazy NOT to read it. When you understand that nobody wants to read your shit, you develop empathy. 5\nJust start. Don’t wait. 7\nIf you want to write and be recognized, you have to do it yourself. 8\n\n\n1. Advertising\n\nThe ad writer must come up with some ingenious way of making her material irresistible. There must be a message, and that message must stick. 17\nAll you do all day is think. That’s your job. Sit there and come up with idea. They struggle at first because they’ve never spent all day living entirely inside their heads. 30\nWhen you try too hard, you have bad ideas. When you work mechanically, you have bad ideas. When you follow formula, you have bad ideas. When you’re desperate or panicky, you have bad ideas. 31\n\n\n\n2. Fiction\n\nThere existed inside my head an invisible, insidious, intractable, indefatigable force whose sole object was to keep me from doing my work and ultimately to destroy me physically, psychologically, and spiritually. All I knew was that I couldn’t finish anything. 43\nI was excruciatingly aware, however, not just that my writing was inauthentic, but that I myself was inauthentic. 45\nHow do we form ourselves? How do we uncover our nature through action? 47\nI worked for 26 months straight. Nobody knew what I had done. Nobody cared. But I knew. “Good for you,” he said without looking up. “Start the next one today.” 49\nThe lessons can’t be taught. The agony cannot be inoculated against. The process is about pain. The lessons come the hard way. 51\nAnd yet you’re learning. You don’t know what. You can’t say how. But the months and years, the millions of keystrokes and erasures go into the bank somehow. The cells remember. Something changes. 54\n\n\n\n3. Hollywood\n\nThe mind-blowing thought that this stuff could be taught. You could study. You could learn. You could get better. 72\nWhen I watched a movie now, I studied it. When I read a book, I put it under the microscope. 78\nStart at the end. First figure out where you want to finish. Then work backward to set up everything you need to get you there. 81\nStories work. Tell it to me as a story. Write your PhD dissertation the same way. 83\nA script is nothing until it’s made into a movie. The medium is motion pictures, not screenplays. 96\nBe unforgettable. Actors, remember, are thinking in terms of their careers. They want to pile up roles that, over time, create a film persona that will endure. 101\nThere will be an all is lost moment. 104\nThe all is lost moment is followed almost immediately by a breakthrough insight or epiphany, an awakening for the hero, and “aha” moment. From this point, the pedal-to-the-metal-rush begins. 105 -You don’t really learn an art or a craft in school. In the real world, the process is more like an apprenticeship, multiple apprenticeships under multiple masters. It happens on the street and it happens in the studio. It happens in bed. It happens sober and it happens stoned. It happens getting up early and it happens staying up late. You kiss ass. You work for free. You do stuff that nobody else with do. In other words, you’re in the trenches, getting hosed and head-banged and dismissed and ignored. You’re invisible. You’re held in contempt. You’re exploited. People farther up the food chain take your time, your energy, your body. You let them. You want them to take those things. It’s the price you pay to learn. 112\nMake it work. That’s how you learn. Then there’s the way you really learn. Alone at your keyboard. Trying to answer the eternal question: “why is this fucking thing not working?” We learn by increments. One word, one image, one piece of code at a time. 113\nShould I? Do I have to? You do. That’s how you learn. 114\n\n\n\n4. Fiction, The Second Time\n\nIt was easy. Why? Because I was bringing all of the principles I had learned in 27 years of working in other fields. These were the skills necessary to conduct oneself as a professional – the inner capacities for managing your emotions, your expectations (of yourself and of the world), and your time. How to start a project. How to keep going through the horrible middle. How to finish. How to fail and keep going. How to self-motivate, self-validate, self-reinforce. How to believe in yourself when no one else on the planet shares that belief. 121\nSomething mysterious and wonderful happens when we write what we don’t know. 122\nNovels are about the long game. A novel will take you two years to write. Or three or four or five. Can you do that? Can you sustain yourself financially? Emotionally? Can your spouse and children handle it? Can you maintain your motivation over that length of time? Your self-belief? Your sanity? If necessary, can you scrap your first eighteen months’ work and start over from scratch? 126\nA novel is too long to be organized efficiently. Too much shit happens. A novel is like an acid trip. For the first 45 minutes you’re thinking, “hmmm, this isn’t so intense. I can handle this.” Then you look down at your hands and flames are coming out of them. 127\nConsider what you’re letting yourself in for: a two- to three-year siege with no external validation or reinforcement, no paycheck, and no day-to-day structure except that which you impose yourself. Support from friends and family? Dubious. Future rewards? Iffy at best. And we’re not even talking about the work. No one, trust me, can write a novel and not become completely submerged in it. You have to or you can’t keep going. 128\nOne of the weirdest things in the world is to look in the mirror (and I mean really look) when you’re in the throws of writing a novel. You don’t even recognize yourself. 129\nYou the pioneer must master the art of delayed gratification. Remember, the enemy in an endurance enterprise is not time. The enemy is resistance. 131\nIf we know we’re going to do fifteen drafts before we’re done, we don’t panic when draft six is still a mess. 132\nWe can’t simply narrate. Why not? Because nobody wants to read your shit. 139\nWhat is this damn thing about? 141\n\n\n\n4. Nonfiction\n\nIf you want your factual dissertation or TED Talk to be powerful and engaging and to hold the reader and audience’s attention, you must organize your material (even though it’s technically not a story and not fiction) as if it were a story and as if it were fiction. 147\nHook. Build. Payoff. 150\nStart with theme. You have to work hard here. This is the toughest and most important part of the whole project. Why do we want to write about this subject? Find that issue. Break it down into a single sentence. 152\nCut everything that’s not on theme. Of what remains, present it as on-theme. 155\nIdentify a villain. Break the narrative into three acts. 159\n\n\n\n6–8 The Rest\n\nIntroduce. Cite examples, Recap and sum up. 163\nAuthority is critical. 164\nIt all felt random. But when I looked back, I could see not just a pattern. I could see a career. It had been there all along, infallibly working itself out. 179\nResistance is real. 180\nSit down. Open the faucet. The stuff that will appear, sometimes anyway, will exceed your fondest visions. 181\nAn artist enters the void with nothing and comes back with something. 184\nI am a writer now. I have paid my dues. 185\nIs there a white whale out there for you? There is or you wouldn’t be reading this book. You’ll know that whale by these qualities: It’s accomplishment will seem beyond your resources. Your pursuit of it will bear you into waters where no one before you has sailed. To hunt this beast will require everything you’ve got. 187\nIt’s okay. It’s all part of the journey. What you learn in Wrong Career #1 will serve you in Off-Key Career #2 and in Out-Of-Kilter Career #3, and the wisdom you acquire in #1, #2, and #3 will form the foundation of Real Calling #4 (or #5 or #6 or however long it takes). 188 -What nobody wants to read your shit means is that none of us wants to hear your self-centered, ego-driven, unrefined demands for attention. Why should we? It’s boring. There’s nothing in it for us. Make it beautiful. Make it fun and sexy and interesting and I’ll buy it. I’ll wear it. I’ll tell my friends about it."
  },
  {
    "objectID": "posts/856-making-of-a-manager/index.html",
    "href": "posts/856-making-of-a-manager/index.html",
    "title": "The Making of a Manager",
    "section": "",
    "text": "Intro: Great Managers Are Made, Not Born\nManagers share a common purpose: helping a group of people achieve a comon goal.\n\n\n1. What is Management?\nYour job, as a manager, is to get better outcomes from a group of people working together.\nHalf of what my manager looked at was my team’s results. The other half was based on the strength and satisfaction of my team.\nResearch shows that teams consistently underperform… that’s because problems with coordination and motivation typically chip away at the benefits of collaboration.\nThe multitude of tasks that fill up a manager’s day sort neatly into 3 buckets: purpose, people, and process.\nPurpose: Why do you wake up and choose to do this thing instead of the thousands of other things you could be doing? What would be different about the world if your team were wildly successful?\nPeople: You might have a superbly talented team with a very clear understanding of what the end goal is, but if it’s not apparent how everyone’s supposed to work together or what the team’s values are, then even simple tasks can get enormously complicated.\nIn a team setting, it’s impossible for a group of people to coordinate what needs to get done without spending time on it.\nYour role as a manager is not to do the work yourself, even if you are the best at it, because that will only take you so far. Your role is to improve the purpose, people, and process of your team to get as high a multiplier effect on your collective outcome as you can.\nYou have to enjoy the day-to-day of management and want to do it.\nIf nobody else does it, then it falls to you.\nIn many organizations, your ability to grow in your career will hit a ceiling unless you start managing people. … That said, many organizations today, particularly those that seek to attract highly skilled or creative talent, have paths for advancement that don’t require managing others.\nSo to be a great manager, one must certainly be a leader. A leader, on the other hand, doesn’t have to be a manager.\n\n\n2. Your First Three Months\n– this chapter lays out a couple of different ways people become managers (apprentice, pioneer, new boss, successor) and provides the “what to take advantage of” and the “what to watch out for” for each path –\n\n\n3. Leading a Small Team\nA manager’s job is to get better outcomes from a group of people working together through influencing purpose, people, and process.\nWhat gets in the way of good work? There are only two possibilities. The first is that people don’t know how to do good work. The second is that they know how, but they aren’t motivated.\nNo matter how you slice it, you are your reports’ boss. This means that the responsibility of building a trusting relationship lies more with you than with them.\nA hallmark of a trusting relationship is that people feel they can share their mistakes, challenges, and fears with you.\nTo track team health, some companies explicitly ask the question, “would you work for your manager again?”\nRemember this: managing is caring.\nSometimes the personal blends into the professional, and that’s okay.\nThe ideal one-on-one meeting leaves your report feeling that it was useful for her. If she thinks that the conversation was pleasant but largely unmemorable, then you can do better.\nYour job as a manager isn’t to dole out advice or “save the day”– it’s to empower your report to find the answer herself.\n\nIdentify: What’s top of mind for you right now?\nUnderstand: What’s the worst case scenario you’re worried about?\nSupport: What can I do to make you more successful?\n\n\n\n4. The Art of Feedback\nFor one, feedback doesn’t have to be critical. Praise is often more motivating that criticism.\nSet clear expectations at the beginning.\nGive task specific feedback as frequently as you can.\nShare behavioral feedback thoughtfully and regularly.\nEvery major disappointment is a failure to set expectations.\nThe most common response to the question “How could your manager better support you?” is simply “Give me more feedback”.\nPart of the reason feedback doesn’t stick is that the recipient often views the conversation as a threat, so his adrenaline-fueled fight-or-flight instinct kicks in.\nRecognizing what’s going well is more likely to change behavior than only point out mistakes.\nWhen you do have critical feedback to share, approach it with a sense of curiosity and an honest desire to understand your report’s perspective.\n\n\n5. Managing Yourself\nWhy does imposter syndrome hit managers so hard? The first is that you’re often looked to for answers. The second reason is that you are constantly put in the position of doing things you haven’t done before.\nSo what happened in those years (when I got better)? … The answer is predictably boring. I practiced and I got better.\n\n\n6. Amazing Meetings\nGood meetings:\n\nare a great use of time\ntaught me something new\nleft me with a clearer sense of what I should do next\nhad everyone engaged\neveryone felt welcomed\n\n\n\n7. Hiring Well\n– skimmed this –\n\n\n8. Making Things Happen\n– common sense –\n\n\n9. Leading a Growing Team\nContext switching all day, every day.\nAs teams grow, managers spend less of their day-to-day on the specific craft of their discipline. What matters more is that they can get the best out of a group of people.\n\n\n10. Nurturing Culture\n– common sense –\n\n\nEpilogue: The Journey is 1% Finished\nIt’s probably more than 1% finished, but the point is that there’s always more to learn."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dan Yavorsky",
    "section": "",
    "text": "SVP, Analytics | GBK Collective\nLecturer, Econometrics | UCLA Anderson\nLecturer, Customer Analytics | UCSD Rady\n\n\n\n\n\n\nGBK Collective | 2022 - Present\nBain & Co | 2020 - 2021\nCornerstone Research | 2006 - 2014\n\n\n\n\n\n\nPhD Quant. Marketing | UCLA Anderson 2020\nMBA Management | UCLA Anderson 2014\nCFA Charterholder (inactive) | CFA Institute 2012\nBA Economics & Mathematics | CMC 2006"
  },
  {
    "objectID": "index.html#current",
    "href": "index.html#current",
    "title": "Dan Yavorsky",
    "section": "",
    "text": "SVP, Analytics | GBK Collective\nLecturer, Econometrics | UCLA Anderson\nLecturer, Customer Analytics | UCSD Rady"
  },
  {
    "objectID": "index.html#work-history",
    "href": "index.html#work-history",
    "title": "Dan Yavorsky",
    "section": "",
    "text": "GBK Collective | 2022 - Present\nBain & Co | 2020 - 2021\nCornerstone Research | 2006 - 2014"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Dan Yavorsky",
    "section": "",
    "text": "PhD Quant. Marketing | UCLA Anderson 2020\nMBA Management | UCLA Anderson 2014\nCFA Charterholder (inactive) | CFA Institute 2012\nBA Economics & Mathematics | CMC 2006"
  },
  {
    "objectID": "links/compute.html",
    "href": "links/compute.html",
    "title": "Links",
    "section": "",
    "text": "Pro Git by Scott Chacon and Ben Straub\nHappy Git and GitHub for the UserR by Jenny Bryan\nGit in Practice by Mike McQuaid\n\n\n\n\n\nOpenVim by Henrik Huttunen\nMastering the Vim Language recorded presentation by Chris Toomey\nVim as Your Editor a 6-part playist by ThePrimeagen\nNeoVim Setup Guide by Josean Martinez\n\n\n\n\n\nQuarto Docs\nQuarto Cheatsheet\nQuarto Extensions\nQuarto website examples: Twitter List Linkedin List\n\n\n\n\n\nData Science at the Command Line by Jeroen Janssens\nThe Unix Workbench by Sean Kross\nDev Ops for Data Science by Alex Gold\nStat447: Data Science Programming Methods by Dirk Eddelbuettel\n\n\n\n\n\nPython 4 Data Science by Arthur Turrell and others"
  },
  {
    "objectID": "links/compute.html#non-r-computing-resources",
    "href": "links/compute.html#non-r-computing-resources",
    "title": "Links",
    "section": "",
    "text": "Pro Git by Scott Chacon and Ben Straub\nHappy Git and GitHub for the UserR by Jenny Bryan\nGit in Practice by Mike McQuaid\n\n\n\n\n\nOpenVim by Henrik Huttunen\nMastering the Vim Language recorded presentation by Chris Toomey\nVim as Your Editor a 6-part playist by ThePrimeagen\nNeoVim Setup Guide by Josean Martinez\n\n\n\n\n\nQuarto Docs\nQuarto Cheatsheet\nQuarto Extensions\nQuarto website examples: Twitter List Linkedin List\n\n\n\n\n\nData Science at the Command Line by Jeroen Janssens\nThe Unix Workbench by Sean Kross\nDev Ops for Data Science by Alex Gold\nStat447: Data Science Programming Methods by Dirk Eddelbuettel\n\n\n\n\n\nPython 4 Data Science by Arthur Turrell and others"
  },
  {
    "objectID": "links/r.html",
    "href": "links/r.html",
    "title": "Links",
    "section": "",
    "text": "Big Book of R compiled by Oscar Baruffa\nAdvanced R by Hadley Wickham\nAdvanced R Solutions by Malte Grosser and Henning Bumann\nR Packages by Hadley Wickham and Jenny Bryan\nR for the Rest of Us: A Stats-Free Intro by David Keyes\nMastering Software Development in R by Roger Peng, Sean Kross, and Brooke Anderson\nScaling Up with R and Arrow by Nic Crane, Jonathan Keane, and Neal Richardson\n\n\n\n\n\nggplot2 by Hadley Wickham\nData Visualization: A Practical Introduction by Kieran Healy\nModern Data Visualization with R by Robert Kabacoff\n\n\n\n\n\nR for Data Science by Hadley Wickham and Garrett Grolemund\nModern R with the Tidyverse by Bruno Rodrigues\nTidy Modeling with R by Max Kuhn and Julia Silge\nData Analysis in R by Steve Midway\nIntroduction to Econometrics with R by Christoph Hanck and others\nModern Data Science with R by Benjamin Baumer, Daniel Kaplan, and Nicholas Horton\n\n\n\n\n\nR Markdown: The Definitive Guide by Yihui Xie, JJ Allaire, and Garrett Grolemund\nBuilding reproducible analytical pipelines with R by Bruno Rodrigues\nTelling Stories with Data by Rohan Alexander\n\n\n\n\n\nExploring Complex Survey Data Analysis by Stephanie Zimmer, Rebecca Powell, and Isabella Velasquez\nQuant UX Resources compiled by Carl Pearson"
  },
  {
    "objectID": "links/r.html#r-resources",
    "href": "links/r.html#r-resources",
    "title": "Links",
    "section": "",
    "text": "Big Book of R compiled by Oscar Baruffa\nAdvanced R by Hadley Wickham\nAdvanced R Solutions by Malte Grosser and Henning Bumann\nR Packages by Hadley Wickham and Jenny Bryan\nR for the Rest of Us: A Stats-Free Intro by David Keyes\nMastering Software Development in R by Roger Peng, Sean Kross, and Brooke Anderson\nScaling Up with R and Arrow by Nic Crane, Jonathan Keane, and Neal Richardson\n\n\n\n\n\nggplot2 by Hadley Wickham\nData Visualization: A Practical Introduction by Kieran Healy\nModern Data Visualization with R by Robert Kabacoff\n\n\n\n\n\nR for Data Science by Hadley Wickham and Garrett Grolemund\nModern R with the Tidyverse by Bruno Rodrigues\nTidy Modeling with R by Max Kuhn and Julia Silge\nData Analysis in R by Steve Midway\nIntroduction to Econometrics with R by Christoph Hanck and others\nModern Data Science with R by Benjamin Baumer, Daniel Kaplan, and Nicholas Horton\n\n\n\n\n\nR Markdown: The Definitive Guide by Yihui Xie, JJ Allaire, and Garrett Grolemund\nBuilding reproducible analytical pipelines with R by Bruno Rodrigues\nTelling Stories with Data by Rohan Alexander\n\n\n\n\n\nExploring Complex Survey Data Analysis by Stephanie Zimmer, Rebecca Powell, and Isabella Velasquez\nQuant UX Resources compiled by Carl Pearson"
  },
  {
    "objectID": "links/quotes.html",
    "href": "links/quotes.html",
    "title": "Links",
    "section": "",
    "text": "Nobody tells this to people who are beginners, I wish someone told me.\nAll of us who do creative work, we get into it because we have good taste. But there is this gap. For the first couple years you make stuff, it’s just not that good. It’s trying to be good, it has potential, but it’s not. But your taste, the thing that got you into the game, is still killer. And your taste is why your work disappoints you.\nA lot of people never get past this phase, they quit. Most people I know who do interesting, creative work went through years of this. We know our work doesn’t have this special thing that we want it to have. We all go through this. And if you are just starting out or you are still in this phase, you gotta know its normal and the most important thing you can do is do a lot of work. Put yourself on a deadline so that every week you will finish one story. It is only by going through a volume of work that you will close that gap, and your work will be as good as your ambitions. And I took longer to figure out how to do this than anyone I’ve ever met. It’s gonna take awhile. It’s normal to take awhile. You’ve just gotta fight your way through.\n\n\n\n\n\nIt is not the critic who counts; not the man who points out how the strong man stumbles, or where the doer of deeds could have done them better.\nThe credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, who comes short again and again, because there is no effort without error and shortcoming; but who does actually strive to do the deeds; who knows the great enthusiasms, the great devotions; who spends himself in a worthy cause; who at the best knows in the end the triumph of high achievement, and who at the worst, if he fails, at least fails while daring greatly, so that his place shall never be with those cold and timid souls who neither know victory nor defeat.\n\n\n\n\n\nYou don’t need a reason to help people.\n\n\n\n\n\nMany a graduate student has come to grief when they discover, after a decade of being told they were ‘good at math’ that in fact they have no real mathematical talent and are just very good at following directions.\n\n\n\n\n\nThe key to learning is feedback. And it is nearly impossible to learn anything without it.\n\n\n\n\n\nI am pretty sure that everything is fairly easy. Most of what passes for intelligence is just persistence and a high tolerance for boredom (both of which may technically be the same thing).\n\n\n\n\n\nYet there will always be a problem about getting rid of the hyphen: if it’s not extra-marital sex (with a hyphen), it is perhaps extra marital sex, which is quite a different bunch of coconuts.\n\n\n\n\n\nThat’s what you do when life hands you a chance to be with someone special. You just grab that brownish area by its points and you don’t let go no matter what your mom says."
  },
  {
    "objectID": "links/quotes.html#quotes",
    "href": "links/quotes.html#quotes",
    "title": "Links",
    "section": "",
    "text": "Nobody tells this to people who are beginners, I wish someone told me.\nAll of us who do creative work, we get into it because we have good taste. But there is this gap. For the first couple years you make stuff, it’s just not that good. It’s trying to be good, it has potential, but it’s not. But your taste, the thing that got you into the game, is still killer. And your taste is why your work disappoints you.\nA lot of people never get past this phase, they quit. Most people I know who do interesting, creative work went through years of this. We know our work doesn’t have this special thing that we want it to have. We all go through this. And if you are just starting out or you are still in this phase, you gotta know its normal and the most important thing you can do is do a lot of work. Put yourself on a deadline so that every week you will finish one story. It is only by going through a volume of work that you will close that gap, and your work will be as good as your ambitions. And I took longer to figure out how to do this than anyone I’ve ever met. It’s gonna take awhile. It’s normal to take awhile. You’ve just gotta fight your way through.\n\n\n\n\n\nIt is not the critic who counts; not the man who points out how the strong man stumbles, or where the doer of deeds could have done them better.\nThe credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, who comes short again and again, because there is no effort without error and shortcoming; but who does actually strive to do the deeds; who knows the great enthusiasms, the great devotions; who spends himself in a worthy cause; who at the best knows in the end the triumph of high achievement, and who at the worst, if he fails, at least fails while daring greatly, so that his place shall never be with those cold and timid souls who neither know victory nor defeat.\n\n\n\n\n\nYou don’t need a reason to help people.\n\n\n\n\n\nMany a graduate student has come to grief when they discover, after a decade of being told they were ‘good at math’ that in fact they have no real mathematical talent and are just very good at following directions.\n\n\n\n\n\nThe key to learning is feedback. And it is nearly impossible to learn anything without it.\n\n\n\n\n\nI am pretty sure that everything is fairly easy. Most of what passes for intelligence is just persistence and a high tolerance for boredom (both of which may technically be the same thing).\n\n\n\n\n\nYet there will always be a problem about getting rid of the hyphen: if it’s not extra-marital sex (with a hyphen), it is perhaps extra marital sex, which is quite a different bunch of coconuts.\n\n\n\n\n\nThat’s what you do when life hands you a chance to be with someone special. You just grab that brownish area by its points and you don’t let go no matter what your mom says."
  },
  {
    "objectID": "pubs/index.html",
    "href": "pubs/index.html",
    "title": "Dan Yavorsky",
    "section": "",
    "text": "In-Progress\nDiscrete Choice Model Estimation with R [link]\nDan Yavorsky\nIn Progress (2024)\nOutside Good Uncertainty: Ordinal Dual Response in Choice-Based Conjoint Analysis [link]\nPrachi Bhalerao, Dan Yavorsky, and Geoffery Zheng\nIn Progress (2024)\n\n\n\nCo-Author\nConsumer Search in the U.S. Auto Industry: The Role of Dealership Visits [link]\nDan Yavorsky, Elisabeth Honka, and Keith Chen\nQuantitative Marketing and Economics (2021)\n\n\n\nContributor\nThe Sequential Search Model: A Framework for Empirical Research [link]\nRaluca Ursu, Stephan Seiler, and Elisabeth Honka\nWorking Paper (2024)\nThe Value of Flexible Work: Evidence from Uber Drivers [link]\nKeith Chen, Judy Chevalier, Peter Rossi, and Emily Oehlsen\nJournal of Political Economy (2019)\nggplot2: Elegant Graphics for Data Analysis [link]\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen\nSpringer\nR Packages: Organize, Test, Document, and Share Your Code [link]\nHadley Wickham and Jennifer Bryan\nSpringer\n\n\n\nShort Form\nSawtooth A&I Summit 2023 [link]\nGBK Thought Leadership Blog\nGBK Employee Spotlight [link]\nGBK Thought Leadership Blog"
  }
]